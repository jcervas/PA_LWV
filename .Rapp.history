plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
arrows(4.25, ((data2[3,4]-data2[1,4])/2)+data2[1,4], 4.5, ((data2[3,4]-data2[1,4])/2)+data2[1,1], length = 0.2, col='black')#
text(4.5, ((data2[3,4]-data2[1,4])/2)+data2[1,1], labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
arrows(4.25, legend.placement, 4.5, legend.placement, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
arrows(4.25, legend.placement, 4.5, legend.placement*1.2, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            abline(4, legend.placement, 4.5, legend.placement*1.5)#
arrows(4.5, legend.placement, 4.5, legend.placement*1.5, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            abline(a=c(4, legend.placement), b=(4.5, legend.placement*1.5))#
arrows(4.5, legend.placement, 4.5, legend.placement*1.5, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot(c(-2,3), c(-1,5), type = "n", xlab = "x", ylab = "y", asp = 1)#
## the x- and y-axis, and an integer grid#
abline(h = 0, v = 0, col = "gray60")#
text(1,0, "abline( h = 0 )", col = "gray60", adj = c(0, -.1))#
abline(h = -1:5, v = -2:3, col = "lightgray", lty = 3)
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            lines(c(4, legend.placement), c(4.5, legend.placement*1.5))#
arrows(4.5, legend.placement, 4.5, legend.placement*1.5, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            lines(c(4, legend.placement), c(4.5, legend.placement))#
arrows(4.5, legend.placement, 4.5, legend.placement*1.5, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
xy.coords(1:3, 1:2, recycle = TRUE)
plot(xy.coords(1:3, 1:2, recycle = TRUE))
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)
legend.placement <- .06
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)
lines(xy.coords(c(4, 4.5), c(legend.placement, legend.placement)))
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)
lines(xy.coords(c(4.25, 4.5), c(legend.placement, legend.placement)))
arrows(4.25, legend.placement, 4.5, legend.placement*1.5, length = 0.2, col='black')
arrows(4.5, legend.placement, 4.25, legend.placement*1.5, length = 0.2, col='black')
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            lines(xy.coords(c(4.25, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            lines(xy.coords(c(4.25, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.16, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.81, 4.25), c(data1[1,4], data1[1,4])))#
arrows(3.81, data1[1,4], 4.25, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*3, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.81, 4), c(data1[1,4], data1[1,4])))#
arrows(3.81, data1[1,4], 4.25, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*3, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.81), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 4, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*3, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.81), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off(
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.81), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.91, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.91), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.97), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.95), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
c(contact_main, commwk_main, donate_main, protest_main, social_main, vote_main, votelocal_main)#
c(contact_ylab, commwk_ylab, donate_ylab, protest_ylab, social_ylab, vote_ylab, votelocal_ylab)#
c(contactF, commwkF, donateF, protestF, socialF, voteF, votelocalF)#
c(contactM, commwkM, donateM, protestM, socialM, voteM, votelocalM)#
c(contactF_se, commwkF_se, donateF_se, protestF_se, socialF_se, voteF_se, votelocalF_se)#
c(contactM_se, commwkM_se, donateM_se, protestM_se, socialM_se, voteM_se, votelocalM_se)#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others To Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate To A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm To Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm To Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm To Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm To Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
#
ci.tmp <- CI(contact, contact_se)#
ci2.tmp <- CI(contact2, contact2_se)#
plot.coef.bars(contact, contact2, ci.tmp[1,], ci.tmp[2,], ci2.tmp[1,], ci2.tmp[2,], "F", "M", main=main, xlab=xlab, ylab=ylab)#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==1#
 cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))#
#
coeftest(m, vcov = vcovHC(m, type="HC1"))#
library(lmtest)#
#
library(sandwich)
c(contact_main, commwk_main, donate_main, protest_main, social_main, vote_main, votelocal_main)#
c(contact_ylab, commwk_ylab, donate_ylab, protest_ylab, social_ylab, vote_ylab, votelocal_ylab)#
c(contactF, commwkF, donateF, protestF, socialF, voteF, votelocalF)#
c(contactM, commwkM, donateM, protestM, socialM, voteM, votelocalM)#
c(contactF_se, commwkF_se, donateF_se, protestF_se, socialF_se, voteF_se, votelocalF_se)#
c(contactM_se, commwkM_se, donateM_se, protestM_se, socialM_se, voteM_se, votelocalM_se)#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others To Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate To A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm To Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm To Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm To Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm To Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
#
ci.tmp <- CI(contact, contact_se)#
ci2.tmp <- CI(contact2, contact2_se)#
plot.coef.bars(contact, contact2, ci.tmp[1,], ci.tmp[2,], ci2.tmp[1,], ci2.tmp[2,], "F", "M", main=main, xlab=xlab, ylab=ylab)#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==1#
 cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))
c(contact_main, commwk_main, donate_main, protest_main, social_main, vote_main, votelocal_main)#
c(contact_ylab, commwk_ylab, donate_ylab, protest_ylab, social_ylab, vote_ylab, votelocal_ylab)#
c(contactF, commwkF, donateF, protestF, socialF, voteF, votelocalF)#
c(contactM, commwkM, donateM, protestM, socialM, voteM, votelocalM)#
c(contactF_se, commwkF_se, donateF_se, protestF_se, socialF_se, voteF_se, votelocalF_se)#
c(contactM_se, commwkM_se, donateM_se, protestM_se, socialM_se, voteM_se, votelocalM_se)#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others To Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate To A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm To Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm To Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm To Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm To Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==1#
 cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))
title <- c(contact_main, commwk_main, donate_main, protest_main, social_main, vote_main, votelocal_main)#
ylabel <- c(contact_ylab, commwk_ylab, donate_ylab, protest_ylab, social_ylab, vote_ylab, votelocal_ylab)#
beta1 <- c(contactF, commwkF, donateF, protestF, socialF, voteF, votelocalF)#
beta2 <- c(contactM, commwkM, donateM, protestM, socialM, voteM, votelocalM)#
se1 <- c(contactF_se, commwkF_se, donateF_se, protestF_se, socialF_se, voteF_se, votelocalF_se)#
se2 <- c(contactM_se, commwkM_se, donateM_se, protestM_se, socialM_se, voteM_se, votelocalM_se)#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others To Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate To A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm To Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm To Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm To Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm To Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==1#
 cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))
title <- c(contact_main, commwk_main, donate_main, protest_main, social_main, vote_main, votelocal_main)#
ylabel <- c(contact_ylab, commwk_ylab, donate_ylab, protest_ylab, social_ylab, vote_ylab, votelocal_ylab)#
beta1 <- c(contactF, commwkF, donateF, protestF, socialF, voteF, votelocalF)#
beta2 <- c(contactM, commwkM, donateM, protestM, socialM, voteM, votelocalM)#
se1 <- c(contactF_se, commwkF_se, donateF_se, protestF_se, socialF_se, voteF_se, votelocalF_se)#
se2 <- c(contactM_se, commwkM_se, donateM_se, protestM_se, socialM_se, voteM_se, votelocalM_se)#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others To Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate To A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm To Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm To Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm To Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm To Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
# cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
# citizen <- cmps$citizen==1#
# ethnic_quota <- cmps$ethnic_quota==1#
#  cmps1 <- cmps[citizen & ethnic_quota,]#
#  base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
# summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))
j <- 1
paste("Marginal Effect of Norm to", title[j],"\nOLS Coefficient & 95% Confidence Intervals")
ylab <- ylabel[j]
ylab
main <- paste("Marginal Effect of Norm to", title[j],"\nOLS Coefficient & 95% Confidence Intervals")#
ylab <- ylabel[j]#
xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(beta1[j], beta2[j], beta1[j], beta2[j], "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
beta1[j]
beta2[j]
CI(data1[j], se1[j], level=0.95)
CI(beta1[j], se1[j], level=0.95)
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
main <- paste("Marginal Effect of Norm to", title[j],"\nOLS Coefficient & 95% Confidence Intervals")#
ylab <- ylabel[j]#
xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(beta1[j], beta2[j], se1[j], se2[j], "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
se1[j]
beta1
votelocalF
beta1
beta1 <- c("contactF", "commwkF", "donateF", "protestF", "socialF", "voteF", "votelocalF")
beta1
get(beta1[j])
title <- c("contact_main", "commwk_main", "donate_main", "protest_main", "social_main", "vote_main", "votelocal_main")#
ylabel <- c("contact_ylab", "commwk_ylab", "donate_ylab", "protest_ylab", "social_ylab", "vote_ylab", "votelocal_ylab")#
beta1 <- c("contactF", "commwkF", "donateF", "protestF", "socialF", "voteF", "votelocalF")#
beta2 <- c("contactM", "commwkM", "donateM", "protestM", "socialM", "voteM", "votelocalM")#
se1 <- c("contactF_se", "commwkF_se", "donateF_se", "protestF_se", "socialF_se", "voteF_se", "votelocalF_se")#
se2 <- c("contactM_se", "commwkM_se", "donateM_se", "protestM_se", "socialM_se", "voteM_se", "votelocalM_se")#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
ylab <- get(ylabel[j])#
xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
par(mfrow=c(3,4))
main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")
ylab <- get(ylabel[j])
xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)
plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)
par(mfrow=c(4,2))
plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 10)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*2, length = 0.2, col='black')#
text(4, legend.placement*2, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*1.5, length = 0.2, col='black')#
text(3.81, legend.placement*1.5, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*2, length = 0.2, col='black')#
text(4, legend.placement*2, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, legend.placement*1.5, length = 0.2, col='black')#
text(3.75, legend.placement*1.5, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.5)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*2, length = 0.2, col='black')#
text(4, legend.placement*2, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, legend.placement*1.5, length = 0.2, col='black')#
text(3.75, legend.placement*1.5, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.3)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*2, length = 0.2, col='black')#
text(4, legend.placement*2, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, legend.placement*1.5, length = 0.2, col='black')#
text(3.75, legend.placement*1.5, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*2, length = 0.2, col='black')#
text(4, legend.placement*2, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, legend.placement*1.5, length = 0.2, col='black')#
text(3.75, legend.placement*1.5, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.4, length = 0.2, col='black')#
text(4,0.4, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
text(4, 0.30, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
title <- c("contact_main", "commwk_main", "donate_main", "protest_main", "social_main", "vote_main", "votelocal_main")#
ylabel <- c("contact_ylab", "commwk_ylab", "donate_ylab", "protest_ylab", "social_ylab", "vote_ylab", "votelocal_ylab")#
beta1 <- c("contactF", "commwkF", "donateF", "protestF", "socialF", "voteF", "votelocalF")#
beta2 <- c("contactM", "commwkM", "donateM", "protestM", "socialM", "voteM", "votelocalM")#
se1 <- c("contactF_se", "commwkF_se", "donateF_se", "protestF_se", "socialF_se", "voteF_se", "votelocalF_se")#
se2 <- c("contactM_se", "commwkM_se", "donateM_se", "protestM_se", "socialM_se", "voteM_se", "votelocalM_se")#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm \nTo Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others \nTo Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate \nTo A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm \nTo Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm \nTo Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm \nTo Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm \nTo Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
# cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
# citizen <- cmps$citizen==1#
# ethnic_quota <- cmps$ethnic_quota==1#
#  cmps1 <- cmps[citizen & ethnic_quota,]#
#  base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
# summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,5,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.3))#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.3))#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.3))#
    }#
dev.off()
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))#
    }#
dev.off()
beta1
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))#
    }#
dev.off()
s
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 6, height = 4)
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 7, height = 4)
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
j <- 3
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 7, height = 4)
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)
par(mfrow=c(1,1))
xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian n\American")
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)
par(mfrow=c(1,1))
xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(0, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)#
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)#
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
names <- c("contact", "commwk", "donate", "protest", "social", "vote", "votelocal")
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))#
    }#
dev.off()
dev.off()
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))#
        dev.off()#
    }
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
                        text(xx, data1[2,] - 0.015, labels=cat1)#
                        text(xx2, data2[2,] - 0.015, labels=cat2)
}
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
`CI` <- #
    function(x, se, level=0.95) {#
        if (level==0.95) z <- 1.96#
        if (level==0.90) z <- 1.645#
        ci <- rbind(x, x - (z * se), x + (z * se))#
        rownames(ci) <- c("beta", "lower", "upper")#
        return(ci)#
    }#
round.ceiling <- function(x, d) ceiling(x/d)*d#
round.floor <- function(x, d) floor(x/d)*d#
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
                        text(xx, data1[2,] - 0.02, labels=cat1)#
                        text(xx2, data2[2,] - 0.02, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
`CI` <- #
    function(x, se, level=0.95) {#
        if (level==0.95) z <- 1.96#
        if (level==0.90) z <- 1.645#
        ci <- rbind(x, x - (z * se), x + (z * se))#
        rownames(ci) <- c("beta", "lower", "upper")#
        return(ci)#
    }#
round.ceiling <- function(x, d) ceiling(x/d)*d#
round.floor <- function(x, d) floor(x/d)*d#
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,5.5,3,3))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
                        text(xx, data1[2,] - 0.02, labels=cat1)#
                        text(xx2, data2[2,] - 0.02, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
`CI` <- #
    function(x, se, level=0.95) {#
        if (level==0.95) z <- 1.96#
        if (level==0.90) z <- 1.645#
        ci <- rbind(x, x - (z * se), x + (z * se))#
        rownames(ci) <- c("beta", "lower", "upper")#
        return(ci)#
    }#
round.ceiling <- function(x, d) ceiling(x/d)*d#
round.floor <- function(x, d) floor(x/d)*d#
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,5.5,3,2))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
                        text(xx, data1[2,] - 0.02, labels=cat1)#
                        text(xx2, data2[2,] - 0.02, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
?png
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), width = 8, height = 4, units=in)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), width = 8, height = 4, units="in")#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), width = 8, height = 4, units=in)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/pdf/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }#
#
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), width = 8, height = 4, units=in)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), width = 8, height = 4, units=in)
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), units=in, width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ".png"), units="px", width=800,height=(600))#
        par(mfrow=c(1,1))#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ".png")
for (j in 1:length(title)) {#
    # png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="px", width=800,height=(600))#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units=in, width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png")
png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units=inches, width = 8, height = 4)
png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="in", width = 8, height = 4)
for (j in 1:length(title)) {#
    # png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="px", width=800,height=(600))#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="in", res=500, width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/pdf/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }#
#
for (j in 1:length(title)) {#
    # png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="px", width=800,height=(600))#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="in", res=500, width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +  .), weight= weight, data=cmps1))
coeftest(m, vcov = vcovHC(m, type="HC1"))
library(sandwich)
install.packages('sandwich')
library(sandwich)
coeftest(m, vcov = vcovHC(m, type="HC1"))
install.packages("lmtest")
library(lmtest)
coeftest(m, vcov = vcovHC(m, type="HC1"))
ethnic <- c(1,2,3,4)
length(names)
c("normcontactNM", "norm_commworkNM", "normprotestNM", "normvoteNM", "normvoteNM", "normsummTrad", "normdonateNM")
c("femnormContact", "femnormCommWk", "femnormProt", "femnormVote", "femnormVote", "femnormTrad", "femnormDonate")
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normvoteNM", "normvoteNM", "normsummTrad", "normdonateNM")
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormVote", "femnormVote", "femnormTrad", "femnormDonate")
dv <- c("socialmedia", "contactissue", "comm_work", "protest", "vote", "votelocal")#
#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normvoteNM", "normvoteNM", "normsummTrad", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormVote", "femnormVote", "femnormTrad", "femnormDonate")#
#
out <- list()#
for(i in length(names)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
#
}
i
out
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "socialmedia", "polpartTrad", "donate")
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normsummTrad", "normdonateNM")
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormTrad", "femnormDonate")
normNM
dv
femnorm
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "polpartTrad", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normsummTrad", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormTrad", "femnormDonate")#
#
out <- list()#
for(i in length(names)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
#
}
femnorm[i]
normNM[i]
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormDonate")#
#
out <- list()#
for(i in length(names)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
#
}
normNM[i]
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormDonate")#
#
out <- list()#
for(i in length(names)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
#
}
dv[i]
i
dv
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormDonate")#
#
out <- list()#
for(i in length(dv)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
#
}
dv[i]
normNM[i]
femnorm[i]
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))
cmps1
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormDonate")#
#
out <- list()#
for(j in 1:length(dv)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
for(i in 1:length(ethnic)) {#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
    }#
}
dv[i]
normNM[i]
femnorm[i]
cmps1
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))
title <- c("contact_main", "commwk_main", "donate_main", "protest_main", "social_main", "vote_main", "votelocal_main")#
ylabel <- c("contact_ylab", "commwk_ylab", "donate_ylab", "protest_ylab", "social_ylab", "vote_ylab", "votelocal_ylab")#
beta1 <- c("contactF", "commwkF", "donateF", "protestF", "socialF", "voteF", "votelocalF")#
beta2 <- c("contactM", "commwkM", "donateM", "protestM", "socialM", "voteM", "votelocalM")#
se1 <- c("contactF_se", "commwkF_se", "donateF_se", "protestF_se", "socialF_se", "voteF_se", "votelocalF_se")#
se2 <- c("contactM_se", "commwkM_se", "donateM_se", "protestM_se", "socialM_se", "voteM_se", "votelocalM_se")#
#
names <- c("contact", "commwk", "donate", "protest", "social", "vote", "votelocal")#
#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm \nTo Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working \nto Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others \nTo Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate \nTo A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm \nTo Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm \nTo Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm \nTo Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm \nTo Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormDonate")#
#
out <- list()#
for(j in 1:length(dv)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
for(i in 1:length(ethnic)) {#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
    }#
}
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/pdf/", names[j] ,".pdf"), width = 8, height = 7)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }#
#
for (j in 1:length(title)) {#
    # png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="px", width=800,height=(600))#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="in", res=500, width = 8, height = 7)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
`CI` <- #
    function(x, se, level=0.95) {#
        if (level==0.95) z <- 1.96#
        if (level==0.90) z <- 1.645#
        ci <- rbind(x, x - (z * se), x + (z * se))#
        rownames(ci) <- c("beta", "lower", "upper")#
        return(ci)#
    }#
round.ceiling <- function(x, d) ceiling(x/d)*d#
round.floor <- function(x, d) floor(x/d)*d#
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,5.5,4,2))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
                        text(xx, data1[2,] - 0.02, labels=cat1)#
                        text(xx2, data2[2,] - 0.02, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/pdf/", names[j] ,".pdf"), width = 8, height = 7)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }#
#
for (j in 1:length(title)) {#
    # png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="px", width=800,height=(600))#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="in", res=500, width = 8, height = 7)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
source("R/Tables.R")
###########################################################################################################
# #
# #
# #
# #
# #
# #
# #
# #
# #
#         #
#         #
#         #
#         #
#         #
#         #
### Code to Replicate "Tools for Identifying Partisan Gerrymandering"#
#  . , University of California Irvine#
#  , University of California Irvine#
### Note: #
# #
# #
# #
###########################################################################################################
    rm(list=ls(all=TRUE))   # Remove all objects just to be safe.#
    options(scipen=999)     # Turn off Scientific Notation#
    options(stringsAsFactors = FALSE)#
    doInstall <- F#
setwd("/Users/cervas/Google Drive/Papers/Tools for Identifying a Partisan Gerrymander/PA_LWV")  # Main directory#
source("R/license.R")    #
seed <- 66#
set.seed(seed)#
              # Change to FALSE if you don't want packages installed.#
  projection <- "+proj=lcc +lat_1=40.96666666666667 +lat_2=39.93333333333333 +lat_0=39.33333333333334 +lon_0=-77.75 +x_0=600000 +y_0=0 +ellps=GRS80 +datum=NAD83 +to_meter=0.3048006096012192 +no_defs"#
# https://spatialreference.org/ref/epsg/nad83-pennsylvania-south-ftus/#
  projection <- "+init=epsg:4269"#
# #
  plan_names <- #
    c(#
      "2011 Enacted",#
      "Joint Legislative",#
      "Gov. Wolf",#
      "2018 Court Remedial")#
  plans <- #
    c("enacted2011", #
        "joint", #
        "govwolf",#
        "court")#
# #
# =================================================================#
# -- FUNCTIONS -- -- FUNCTIONS -- -- FUNCTIONS -- -- FUNCTIONS  -- #
# =================================================================#
    source("R/GERRYfunctions.R")#
    source("/Users/cervas/Google Drive/School/UCI/R Functions/seatsvotes.R")#
# =================================================================#
# -- DATA -- -- DATA -- -- DATA -- -- DATA  -- -- DATA  -- -- DATA #
# =================================================================#
    source("R/PA_Congressional_Data.R")#
    pa.redist.dta <- read.csv("./_data/pa_redist_shp.csv")#
    source("R/DataSetup.R") #
# ================================================================= ##
# -- TOOLS FOR IDENTIFYING PARTISAN  GERRYMANDERING -- ANALYSIS -- -##
# ================================================================= #        #
    source("R/Simulations.R")#
    source("R/GIS.R")#
    source("R/Tables.R")#
    source("R/Plots.R")
latex.special.chr <- function(x) {#
  x <- gsub("$\\hat{\\mkern6mu}$", "^", x,fixed=TRUE)#
  x <- gsub("\\textasteriskcentered ", "*", x,fixed=TRUE)#
  x <- gsub("\\textbackslash ", "\\", x,fixed=TRUE)#
  x <- gsub("\\{", "{", x,fixed=TRUE)#
  x <- gsub("\\}", "}", x,fixed=TRUE)#
  x <- gsub("\\$", "$", x,fixed=TRUE)#
}
cat(#
  "\n#
#
    LOADING FUNCTIONS. . . . . . . . . . . . . . . . . . . . . . #
\n")#
`Table` <- function(x, path=NULL, caption="", label="", footnote="", landscape=FALSE, out=NULL)#
  {#
    x <- append(x,#
paste0(#
"\n% =====================================================================#
%  TABLE  TABLE  TABLE  TABLE#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center}\\textbf{INSERT TABLE \\ref{", label, "} ABOUT HERE} \\end{center}#
% \n#
\n% =====================================================================#
%  TABLE  TABLE  TABLE  TABLE#
% ---------------------------------------------------------------------",#
ifelse(landscape==T, "\n\\begin{landscape}", "")),#
 after = 1)#
#
    x <- append(x,#
paste0("\\end{tabular}#
\\tabnotes{", footnote, "}#
\\end{table}",#
ifelse(landscape==T, "\n\\end{landscape}", ""),#
"#
% ---------------------------------------------------------------------#
%  END TABLE  END TABLE  END TABLE #
% ===================================================================== \n \n"),#
after = length(x))#
# cat(paste(x, collapse = "\n"), "\n")#
cat(paste(latex.special.chr(x), collapse = "\n"), "\n", file = paste0("Latex/", path))#
  }#
#
latex.special.chr <- function(x) {#
  x <- gsub("$\\hat{\\mkern6mu}$", "^", x,fixed=TRUE)#
  x <- gsub("\\textasteriskcentered ", "*", x,fixed=TRUE)#
  x <- gsub("\\textbackslash ", "\\", x,fixed=TRUE)#
  x <- gsub("\\{", "{", x,fixed=TRUE)#
  x <- gsub("\\}", "}", x,fixed=TRUE)#
  x <- gsub("\\$", "$", x,fixed=TRUE)#
}#
#
 `Figure` <- function(path=NULL, caption="", label="", footnote="")#
  {#
    x <- paste0("\n#
% =====================================================================#
%  FIGURE  FIGURE  FIGURE  FI#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center} INSERT FIGURE \\ref{", label, "} ABOUT HERE \\end{center}#
% ---------------------------------------------------------------------#
%  END FIGURE  END FIGURE  END FIGURE #
% ===================================================================== \n",#
#
"\n% =====================================================================#
%  FIGURE  FIGURE  FIGURE  FI#
% ---------------------------------------------------------------------#
% " , caption,#
"% #
% #
\\begin{figure}#
    \\begin{center}#
    \\caption{",caption, "}#
    \\label{", label, "}#
    \\includegraphics[width=1\\textwidth]{", path, "}#
    \\end{center}#
    \\tabnotes{", footnote, "}#
\\end{figure}#
% ---------------------------------------------------------------------#
%  END FIGURE  END FIGURE  END FIGURE #
% =====================================================================\n \n"#
)#
  cat(paste(x, collapse = "\n"), "\n")#
  }#
#
`new.list` <-#
function(len) {#
  out <- list(NA)#
  if (len<1) stop ("Can't make a list of length ",len,".")#
  for (ii in 1:len) out[[ii]] <- NA#
  return(out)#
}#
#
`shift` <- # Seats at c vote#
function(x, w=NULL, c=0.5) {#
    if(is.null(w)) w <- rep(1,length(x))#
  tmp <- mean.w(x, w = w) - c#
    tmp.votes <- x - tmp#
    return(seats(tmp.votes))#
}#
`cube` <- function (x)  x^3/(1-3*x+3*x^2)#
#
seats.display <- function(x) {#
  tmp <- numeric()#
    for (i in 1:dim(x)[2]) {#
      rep <- seats(x[,i]) * length(x[,i])#
      dem <- length(x[,i]) - rep#
      tmp <- rbind(tmp, paste0(rep,"R-", dem, "D"))#
    }#
    return(tmp)#
  }#
#
`two_party` <- #
  function(D, R) {#
    replaceNA(as.numeric(D))/(replaceNA(as.numeric(D))+replaceNA(as.numeric(R)))#
    }#
#
`find.winner` <- #
  function(inp) 0*(inp<0.5)+1*(inp>0.5)#
#
`seats` <- #
  function(inp) mean(inp>.5)#
#
`make.weights` <-  #
  function(x, d = 5) {#
    r(x/sum(x,na.rm=T), d = d)}#
#
`f.num` <- function(x, d=2) format(round(x, d=d), nsmall=d)#
#
`percent` <- #
  function(x, d = 1) {#
    paste0(f.num(x * 100, d= d), "%")#
  }#
#
`replaceNA` <- #
  function (x, value=0) {#
   x[is.na(x)] <- value #
   return(x)}#
#
`mean.w` <- # WEIGHTED MEAN#
function (x, weight=NULL, na.rm = FALSE, ...) {#
    if (is.null(weight)) weight <- rep(1, length(x))#
      w <- as.double(weight)#
    if (na.rm) {#
        i <- !is.na(x)#
        w <- w[i]#
        x <- x[i]#
    }#
    sum((x * w)[w != 0])/sum(w)}#
#
inv <- function(x) {#
  (exp(x) / ( 1 + exp(x)))#
}#
#
sv.curve <- function(s,v) {#
  reg <- lm(log(sv(s)) ~ log(sv(v)))#
  vote <- seq(0.01,0.99, by=.01)#
  seatvotes <-  reg$coefficients[2]*log(vote/(1-vote)) + reg$coefficients[1]#
  return(inv(seatvotes))#
}#
#
sv <- function(x) (x / (1 - x))#
#
`seats.print` <- function(x) paste0(sum(find.winner(x)), "R-", length(x)-sum(find.winner(x)), "D")#
#
`agg.precinct` <- function(data, var, id) {#
        cbind.data.frame(#
          REP = aggregate(as.numeric(data[,paste0(var, "R")]), by=list(id=data[,id]), FUN=sum)[,2], #
          DEM = aggregate(as.numeric(data[,paste0(var, "D")]), by=list(id=data[,id]), FUN=sum)[,2]#
        )#
      }#
#
comp.raw <- function(data) {#
  (two_party(as.numeric(data[,"T16PRESR"]), as.numeric(data[,"T16PRESD"])) +#
  two_party(as.numeric(data[,"T16SENR"]), as.numeric(data[,"T16SEND"])) +#
  two_party(as.numeric(data[,"T16ATGR"]), as.numeric(data[,"T16ATGD"])) +#
  two_party(as.numeric(data[,"T16AUDR"]), as.numeric(data[,"T16AUDD"])) +#
  two_party(as.numeric(data[,"T16TREASR"]), as.numeric(data[,"T16TREASD"]))#
  ) / 5#
  }#
#
composite <- function(data, id) {#
  data <- data[order(data[,id]),]#
  cbind.data.frame(#
        CONG = two_party(aggregate(as.numeric(data[,"T16CONGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16CONGD"]), by=list(id=data[,id]), FUN=sum)[,2]),#
        PRES = two_party(aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2]), #
        USSEN = two_party(aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        ATTGEN = two_party(aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        AUDITOR = two_party(aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        TREASURER = two_party(aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        COMPOSITE = composite.sum(data, id)#
        )#
  }#
#
composite.sum <- function(data, id) {#
  two_party(#
            (#
              aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2]#
            ),#
            (#
              aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]#
            )#
          )#
  }#
#
std <- function(x) {#
  (mean(x[!is.na(x)]) - x) / sd(x[!is.na(x)])#
  }#
#
# ci <- function(x)#
#   {#
#     r <- summary(lm(x~1))#
#       m <- coef(r)[1]#
#       se <- coef(r)[2]#
#       return(qt(0.975, df=length(x)-1) * se)#
#   }#
#
`quick.summary` <- function (x) #
  {#
  `qsum` <- function (set) cbind(mean(set[!is.na(set)]),#
                            sd(set[!is.na(set)]),#
                            var(set[!is.na(set)]),#
                            min(set[!is.na(set)]),#
                            max(set[!is.na(set)]),#
                            sum(1*!is.na(set)),#
                            sum(1*is.na(set)))#
    out <- qsum(x)#
  colnames(out) <- c("Mean","SD","Variance","Min","Max","Valid","Missing")#
    return(out)#
  }#
#
`quick.sum` <- #
  function (x) {#
  `qsum` <- #
    function (set) cbind(#
                      mean(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) - ci(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) + ci(set[!is.na(set)]),#
                      sd(set[!is.na(set)]),#
                      min(set[!is.na(set)]),#
                      quantile(set[!is.na(set)], 0.025),#
                      quantile(set[!is.na(set)], 0.975),                            #
                      max(set[!is.na(set)])#
                      )#
#
    out <- qsum(x)#
  colnames(out) <- c("Mean","CI_lower","CI_upper","SD","Min","2.5%","97.5%","Max")#
    return(out)}#
#
  comb <-combn(c(0:9,LETTERS[1:6]),2)#
  opacity <- c(paste0(comb[1,1:120], comb[2,1:120]), paste0("FF"))#
#
`unc` <- function(x) -1 * (x <= .25) + 1 * (x >= .75)#
#
`delete.unc` <-#
function(vs, uncL, uncU) {#
  #replaces uncontested vote values with "missing".#
 f1 <- function (a,b,c) ifelse (is.na(a),NA,#
                                 ifelse(a<b,NA, ifelse(a>c,NA,a)))#
  return(sapply (vs,f1,uncL,uncU)) }#
#
`replace.unc` <-#
function (vs,l,u,lr,ur, na.rm=T) { # na.rm replaces NAs with 0#
  na <- numeric()#
    if(na.rm==T) #
    {#
      na <- 0#
    } else NA#
  f1 <- function (a,b,c,d,e) if (!is.na(a)) {#
    if (a<b) a <- d else if (a>c) a <- e else a#
  } else NA#
  return(sapply (vs,f1,l,u,lr,ur))}#
`default.unc` <-#
function (vs, uncL = 0.25, uncU = 0.75, uncLR = 0.25, uncUR = 0.75) {#
  vs <- replace.unc(vs, uncL, uncU, uncLR, uncUR)#
  return(vs)}#
#
`mean.unc` <- # uncL and uncU to replace outside bounds, otherwise just NAs#
function(vs, uncL = 0.0001, uncU = 0.99999) {#
  vs <- delete.unc(vs, uncL=uncL, uncU=uncU)#
tmp <- vs[!is.na(vs)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.mean <- truncate(rnorm(length(vs[is.na(vs)]), reg, reg_sig))#
  vs[is.na(vs)] <- new.mean#
  return(vs)}#
`impute.weights` <- # for use when turnout in districts is abnormally high or low (w = # of standard deviations from median)#
function(t1, t2, w = 2) {#
  t <- t1+t2#
  uncL <- (median(t, na.rm=T) -  (w * sd(t, na.rm=T)))#
  uncU <- (median(t, na.rm=T) + (w * sd(t, na.rm=T)))#
  t1 <- delete.unc(t, uncL=uncL, uncU=uncU)#
tmp <- t1[!is.na(t1)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.reg <- r(rnorm(length(t1[is.na(t1)]), reg, reg_sig), d=0)#
  t1[is.na(t1)] <- new.reg#
  return(t1)}#
`truncate` <-#
function(r1) {#
  below <- r1<=0#
  above <- r1>=1#
  between <- !(below|above)#
  (below*0.0001)+(r1*between)+0.9999*above#
}#
sim.election <- function(votes= NULL, center=house.2016.votes, incumbency=NULL, yr=2018, sims=1000, sigma=sigma) {#
  if (is.null(sims)) sims <- 1000#
      equal.vote <- mean(votes) - mean(center)#
      sims.year <- new.list(sims)#
    for (k in 1:sims)#
      {#
    sims.year[[k]] <- #
        rnorm(length(votes), votes - equal.vote, #
          sigma)#
      }#
  return(sims.year)#
  }#
#
r <- function(r, d=2) round(r, digits=d)#
# =================================================================#
# -- GERRYMANDER MEASURES -- GERRYMANDER MEASURES -- GERRYMANDER ME#
# =================================================================#
#  DELINATION #
`declination` <- # Warrington, Gregory S. 2018. Quantifying Gerrymandering Using the Vote Distribution. Election Law Journal 17(1): 3957. www.liebertpub.com (Accessed February 22, 2019).#
  function(votes) {#
    abo = votes[votes > 0.5]   # districts won by party A#
    bel = votes[votes <= 0.5]  # districts won by party B#
  # declination is undefined if one party wins all seats.#
    if (length(bel) == 0 | length(abo) == 0) {#
      return(NaN)#
    }#
  # angle for party B#
    theta = atan((1-(2*mean(bel))) / (length(bel) / length(votes)))#
  # angle for party A#
    gamma = atan((2*mean(abo)-1) / (length(abo) / length(votes)))#
  # normalize from radians to values betwen -1 and 1#
  # A little extra precision just in case :)#
    return(-1 * (2.0*(gamma-theta)/3.1415926535))}#
#
`declin2` <- # Simplified, not transformed from: Katz, Jonathan N. et al. 2018. Theoretical Foundations and Empirical Evaluations of Partisan Fairness in District-Based Democracies *. https://gking.harvard.edu/files/psym_2.pdf (Accessed March 16, 2019).#
  function(vs) {#
    abo = vs[vs > 0.5]   # districts won by party A#
    bel = vs[vs <= 0.5]  # districts won by party B#
    (-1 * (mean(abo) - 0.5) / (sum(find.winner(abo))/length(vs))) - ((0.5 - mean(bel)) / (1 - sum(find.winner(abo))/length(vs)))#
  }#
# #
#  MEAN/MEDIAN#
# #
`meanmedian` <- # Best, Robin E., Shawn J. Donahue, Jonathan Krasno, Daniel B. Magleby, et al. 2018. Considering the Prospects for Establishing a Packing Gerrymandering Standard. Election Law Journal: Rules, Politics, and Policy 17(1): 120. http://www.liebertpub.com/doi/10.1089/elj.2016.0392 (Accessed July 24, 2018).#
  function(votes) median(votes) - mean(votes)#
# #
#  EFFICIENCY GAP#
# #
`eff_gap` <- # Stephanopoulos, Nicholas, and Eric Mcghee. 2014. Partisan Gerrymandering and the Efficiency Gap. University of Chicago Law School 82. http://ssrn.com/abstract=2457468.. (Accessed September 10, 2018).#
  function(D, R) {#
    total <- sum(D)+sum(R)#
    dem_wasted <- sum(ifelse(D>R, D[D>R] - R[D>R], D[D<R]))#
    rep_wasted <- sum(ifelse(D<R, R[D<R] - D[D<R], R[D>R]))#
      return((dem_wasted - rep_wasted) / sum(total))}#
#
eg_TP <- function(votes) #
  {#
    y <- cbind.data.frame(votes, 1-votes)#
      return(-1 * eff_gap(y[1], y[2]))#
  }#
# #
#  GERRY DISPLAY #
# #
  gerry <- function(x, toggle=TRUE)#
    {#
    Seats = paste0(" [", seats.print(x), "]")#
    SeatPER = percent(seats(x))#
    Votes = percent(mean(default.unc(x)))#
    Bias = r(seatsvotes(x)$bias)#
    EfficiencyGap = r(eg_TP(x))#
    MeanMedian = r(meanmedian(x))#
    Declination = r(declination(x))#
    a <- rbind.data.frame(#
      Seats, SeatPER, Votes, Bias, EfficiencyGap, MeanMedian, Declination)#
    rownames(a) <- c("Seats","Seat %","Votes","Bias","Efficiency Gap","Mean/Median","Declination")#
    colnames(a) <- "Summary"#
#
    if (toggle!=TRUE) { # TO RETURN UNFORMATTED MEASUREMENTS#
    SeatsT = NA#
    SeatPERT = r(seats(x))#
    VotesT = r(mean(default.unc(x)))#
    BiasT = r(seatsvotes(x)$bias)#
    EfficiencyGapT = r(eg_TP(x))#
    MeanMedianT = r(meanmedian(x))#
    DeclinationT = r(declination(x))#
      return(rbind.data.frame(#
      SeatsT, SeatPERT, VotesT, BiasT, EfficiencyGapT, MeanMedianT, DeclinationT))}#
    return(a)#
    }#
#
p_value <- function(x){#
  l <- quantile(x[!is.na(x)], 0.025)#
  u <- quantile(x[!is.na(x)], 0.975)#
  t <- mean(x)/sd(x)#
    return(2*pt(-abs(t),df=length(x)-1))#
  }#
#
strs <- function(s) {#
  strs <- #
  ifelse(s <= 0.001, "***", #
    ifelse(0.001 < s & s <= 0.01, "**", #
      ifelse(0.01 < s & s <= 0.05, "*", "")))#
  return(paste0("$^{", strs, "}$"))#
}#
#
nintyfive <- function(x, percent=FALSE) {#
  if (percent==TRUE){ return(paste0("(", percent(r(quantile(x[!is.na(x)], 0.025),d=1)), ", ", percent(r(quantile(x[!is.na(x)], 0.975),d=1)), ")")) }#
  return(paste0("\\textit{[", r(quantile(x[!is.na(x)], 0.025)), ", ", r(quantile(x[!is.na(x)], 0.975)), "]}"))#
  }#
#
gtab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
  x.bias <- get(paste0(p, ".bias"))#
  x.eg <- get(paste0(p, ".eg"))#
  x.mm <- get(paste0(p, ".meanmedian"))#
  x.declin <- get(paste0(p, ".declination"))#
    return(c(#
      paste0(r(mean(x.bias)), strs(p_value(x.bias))), #
        nintyfive(x.bias), #
      paste0(r(mean(x.eg)), strs(p_value(x.eg))),#
        nintyfive(x.eg), #
      paste0(r(mean(x.mm)), strs(p_value(x.mm))),#
        nintyfive(x.mm),#
      paste0(r(mean(x.declin)), strs(p_value(x.declin))),#
        nintyfive(x.declin)#
      ))#
    }#
#
ptab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
    return(c(#
      paste0(r(sum(s.tmp*18)/(1000*18)*18), "R-", r(18-sum(s.tmp*18)/(1000*18)*18), "D"),#
      paste0("\\textit{[", r(quantile(s.tmp, 0.025),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.025),d=1)*18, "D, ", r(quantile(s.tmp, 0.975),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.975),d=1)*18, "D]}"),#
      paste0(median(s.tmp)*18, "R-", 18-median(s.tmp)*18, "D"),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) > 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) < 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) == 0.5) / 1000)#
      ))#
} #
#
 #    #
`circle.new` <- #
  function(xorig, yorig, radius, add, ...){#
    x <- seq(-radius, radius, length.out = 1000)#
  # Euclidian distance to the origin#
  y <- sapply(x, function(z) sqrt(radius^2 - z^2))#
  if(add == TRUE){#
    line(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
          type = "l", add=T, ...)#
   } else {#
   plot(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
        type = "l",  #
    xlab="", #
    ylab="", #
    xaxt="n", #
    yaxt="n", #
    bty="n", ...)#
   }#
}#
#
`circle` <- #
function (x, y, radius, nv = 100, border = NULL, col = NA, lty = 1, #
    lwd = 1) {#
    xylim <- par("usr")#
    plotdim <- par("pin")#
    ymult <- getYmult()#
    angle.inc <- 2 * pi/nv#
    angles <- seq(0, 2 * pi - angle.inc, by = angle.inc)#
    if (length(col) < length(radius)) #
        col <- rep(col, length.out = length(radius))#
    for (circle in 1:length(radius)) {#
        xv <- cos(angles) * radius[circle] + x#
        yv <- sin(angles) * radius[circle] * ymult + y#
        polygon(xv, yv, border = border, col = col[circle], lty = lty, #
            lwd = lwd)#
    }#
    invisible(list(x = xv, y = yv))#
}#
# #
# COMPACTNESS MEASURES#
#
POLSBYPOPPER <- function (x) r(mean((4 * 3.1415926535 * x$area) / (x$perimeter^2)), d = 3)#
REOCK <- function (x) r(mean(x$area/(x$smallestcircle^2 * 3.1415926535)), d = 3)#
poly.math <- function (x) {#
    compactness <- NULL#
  if (class(x)=="character") x <- get(x)#
      shapeFile <- x#
      mapObject <- fortify(shapeFile)#
      mapObject$id <- as.character(as.numeric(mapObject$id) + 1)#
      mapObject <- data.frame(mapObject, shapeFile@data[mapObject$id, ])#
      mapObject$piece <- as.character(mapObject$piece)#
      uniqueCDs <- sort(unique(as.numeric(mapObject$id)))#
      for(id in uniqueCDs)#
        {#
          cdShape <- mapObject[mapObject$id == id, ]#
          cdPoly <- SpatialPolygons(list(Polygons(lapply(split(cdShape[, c("long", "lat")], cdShape$piece), Polygon), ID = "b")))#
          owinObject <- try(as(cdPoly, "owin"))#
          compactness[[id]] <-  data.frame(area=area.owin(owinObject), perimeter=perimeter(owinObject), smallestcircle=boundingradius(owinObject))#
        }#
        x <- do.call("rbind", compactness)#
        cat(#
          "\n REOCK:        ", REOCK(x), "\n",#
          "POLSBY-POPPER:", POLSBYPOPPER(x), "\n \n"#
          )#
        return(x)#
  }
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)#
    tab_gerry.tex <- gsub("$\\hat{\\mkern6mu}$", "^", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textasteriskcentered ", "*", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textbackslash ", "\\", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\{", "{", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\}", "}", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\$", "$", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)#
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)
cat(#
  "\n#
#
    LOADING FUNCTIONS. . . . . . . . . . . . . . . . . . . . . . #
\n")#
`Table` <- function(x, path=NULL, caption="", label="", footnote="", landscape=FALSE, out=NULL)#
  {#
    x <- append(x,#
paste0(#
"\n% =====================================================================#
%  TABLE  TABLE  TABLE  TABLE#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center}\\textbf{INSERT TABLE \\ref{", label, "} ABOUT HERE} \\end{center}#
% \n#
\n% =====================================================================#
%  TABLE  TABLE  TABLE  TABLE#
% ---------------------------------------------------------------------",#
ifelse(landscape==T, "\n\\begin{landscape}", "")),#
 after = 1)#
#
    x <- append(x,#
paste0("\\end{tabular}#
\\tabnotes{", footnote, "}#
\\end{table}",#
ifelse(landscape==T, "\n\\end{landscape}", ""),#
"#
% ---------------------------------------------------------------------#
%  END TABLE  END TABLE  END TABLE #
% ===================================================================== \n \n"),#
after = length(x))#
cat(paste(latex.special.chr(x), collapse = "\n"), "\n")#
cat(paste(latex.special.chr(x), collapse = "\n"), "\n", file = paste0("Latex/", path))#
  }#
#
latex.special.chr <- function(x) {#
  x <- gsub("$\\hat{\\mkern6mu}$", "^", x,fixed=TRUE)#
  x <- gsub("\\textasteriskcentered ", "*", x,fixed=TRUE)#
  x <- gsub("\\textbackslash ", "\\", x,fixed=TRUE)#
  x <- gsub("\\{", "{", x,fixed=TRUE)#
  x <- gsub("\\}", "}", x,fixed=TRUE)#
  x <- gsub("\\$", "$", x,fixed=TRUE)#
}#
#
 `Figure` <- function(path=NULL, caption="", label="", footnote="")#
  {#
    x <- paste0("\n#
% =====================================================================#
%  FIGURE  FIGURE  FIGURE  FI#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center} INSERT FIGURE \\ref{", label, "} ABOUT HERE \\end{center}#
% ---------------------------------------------------------------------#
%  END FIGURE  END FIGURE  END FIGURE #
% ===================================================================== \n",#
#
"\n% =====================================================================#
%  FIGURE  FIGURE  FIGURE  FI#
% ---------------------------------------------------------------------#
% " , caption,#
"% #
% #
\\begin{figure}#
    \\begin{center}#
    \\caption{",caption, "}#
    \\label{", label, "}#
    \\includegraphics[width=1\\textwidth]{", path, "}#
    \\end{center}#
    \\tabnotes{", footnote, "}#
\\end{figure}#
% ---------------------------------------------------------------------#
%  END FIGURE  END FIGURE  END FIGURE #
% =====================================================================\n \n"#
)#
  cat(paste(x, collapse = "\n"), "\n")#
  }#
#
`new.list` <-#
function(len) {#
  out <- list(NA)#
  if (len<1) stop ("Can't make a list of length ",len,".")#
  for (ii in 1:len) out[[ii]] <- NA#
  return(out)#
}#
#
`shift` <- # Seats at c vote#
function(x, w=NULL, c=0.5) {#
    if(is.null(w)) w <- rep(1,length(x))#
  tmp <- mean.w(x, w = w) - c#
    tmp.votes <- x - tmp#
    return(seats(tmp.votes))#
}#
`cube` <- function (x)  x^3/(1-3*x+3*x^2)#
#
seats.display <- function(x) {#
  tmp <- numeric()#
    for (i in 1:dim(x)[2]) {#
      rep <- seats(x[,i]) * length(x[,i])#
      dem <- length(x[,i]) - rep#
      tmp <- rbind(tmp, paste0(rep,"R-", dem, "D"))#
    }#
    return(tmp)#
  }#
#
`two_party` <- #
  function(D, R) {#
    replaceNA(as.numeric(D))/(replaceNA(as.numeric(D))+replaceNA(as.numeric(R)))#
    }#
#
`find.winner` <- #
  function(inp) 0*(inp<0.5)+1*(inp>0.5)#
#
`seats` <- #
  function(inp) mean(inp>.5)#
#
`make.weights` <-  #
  function(x, d = 5) {#
    r(x/sum(x,na.rm=T), d = d)}#
#
`f.num` <- function(x, d=2) format(round(x, d=d), nsmall=d)#
#
`percent` <- #
  function(x, d = 1) {#
    paste0(f.num(x * 100, d= d), "%")#
  }#
#
`replaceNA` <- #
  function (x, value=0) {#
   x[is.na(x)] <- value #
   return(x)}#
#
`mean.w` <- # WEIGHTED MEAN#
function (x, weight=NULL, na.rm = FALSE, ...) {#
    if (is.null(weight)) weight <- rep(1, length(x))#
      w <- as.double(weight)#
    if (na.rm) {#
        i <- !is.na(x)#
        w <- w[i]#
        x <- x[i]#
    }#
    sum((x * w)[w != 0])/sum(w)}#
#
inv <- function(x) {#
  (exp(x) / ( 1 + exp(x)))#
}#
#
sv.curve <- function(s,v) {#
  reg <- lm(log(sv(s)) ~ log(sv(v)))#
  vote <- seq(0.01,0.99, by=.01)#
  seatvotes <-  reg$coefficients[2]*log(vote/(1-vote)) + reg$coefficients[1]#
  return(inv(seatvotes))#
}#
#
sv <- function(x) (x / (1 - x))#
#
`seats.print` <- function(x) paste0(sum(find.winner(x)), "R-", length(x)-sum(find.winner(x)), "D")#
#
`agg.precinct` <- function(data, var, id) {#
        cbind.data.frame(#
          REP = aggregate(as.numeric(data[,paste0(var, "R")]), by=list(id=data[,id]), FUN=sum)[,2], #
          DEM = aggregate(as.numeric(data[,paste0(var, "D")]), by=list(id=data[,id]), FUN=sum)[,2]#
        )#
      }#
#
comp.raw <- function(data) {#
  (two_party(as.numeric(data[,"T16PRESR"]), as.numeric(data[,"T16PRESD"])) +#
  two_party(as.numeric(data[,"T16SENR"]), as.numeric(data[,"T16SEND"])) +#
  two_party(as.numeric(data[,"T16ATGR"]), as.numeric(data[,"T16ATGD"])) +#
  two_party(as.numeric(data[,"T16AUDR"]), as.numeric(data[,"T16AUDD"])) +#
  two_party(as.numeric(data[,"T16TREASR"]), as.numeric(data[,"T16TREASD"]))#
  ) / 5#
  }#
#
composite <- function(data, id) {#
  data <- data[order(data[,id]),]#
  cbind.data.frame(#
        CONG = two_party(aggregate(as.numeric(data[,"T16CONGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16CONGD"]), by=list(id=data[,id]), FUN=sum)[,2]),#
        PRES = two_party(aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2]), #
        USSEN = two_party(aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        ATTGEN = two_party(aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        AUDITOR = two_party(aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        TREASURER = two_party(aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        COMPOSITE = composite.sum(data, id)#
        )#
  }#
#
composite.sum <- function(data, id) {#
  two_party(#
            (#
              aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2]#
            ),#
            (#
              aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]#
            )#
          )#
  }#
#
std <- function(x) {#
  (mean(x[!is.na(x)]) - x) / sd(x[!is.na(x)])#
  }#
#
# ci <- function(x)#
#   {#
#     r <- summary(lm(x~1))#
#       m <- coef(r)[1]#
#       se <- coef(r)[2]#
#       return(qt(0.975, df=length(x)-1) * se)#
#   }#
#
`quick.summary` <- function (x) #
  {#
  `qsum` <- function (set) cbind(mean(set[!is.na(set)]),#
                            sd(set[!is.na(set)]),#
                            var(set[!is.na(set)]),#
                            min(set[!is.na(set)]),#
                            max(set[!is.na(set)]),#
                            sum(1*!is.na(set)),#
                            sum(1*is.na(set)))#
    out <- qsum(x)#
  colnames(out) <- c("Mean","SD","Variance","Min","Max","Valid","Missing")#
    return(out)#
  }#
#
`quick.sum` <- #
  function (x) {#
  `qsum` <- #
    function (set) cbind(#
                      mean(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) - ci(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) + ci(set[!is.na(set)]),#
                      sd(set[!is.na(set)]),#
                      min(set[!is.na(set)]),#
                      quantile(set[!is.na(set)], 0.025),#
                      quantile(set[!is.na(set)], 0.975),                            #
                      max(set[!is.na(set)])#
                      )#
#
    out <- qsum(x)#
  colnames(out) <- c("Mean","CI_lower","CI_upper","SD","Min","2.5%","97.5%","Max")#
    return(out)}#
#
  comb <-combn(c(0:9,LETTERS[1:6]),2)#
  opacity <- c(paste0(comb[1,1:120], comb[2,1:120]), paste0("FF"))#
#
`unc` <- function(x) -1 * (x <= .25) + 1 * (x >= .75)#
#
`delete.unc` <-#
function(vs, uncL, uncU) {#
  #replaces uncontested vote values with "missing".#
 f1 <- function (a,b,c) ifelse (is.na(a),NA,#
                                 ifelse(a<b,NA, ifelse(a>c,NA,a)))#
  return(sapply (vs,f1,uncL,uncU)) }#
#
`replace.unc` <-#
function (vs,l,u,lr,ur, na.rm=T) { # na.rm replaces NAs with 0#
  na <- numeric()#
    if(na.rm==T) #
    {#
      na <- 0#
    } else NA#
  f1 <- function (a,b,c,d,e) if (!is.na(a)) {#
    if (a<b) a <- d else if (a>c) a <- e else a#
  } else NA#
  return(sapply (vs,f1,l,u,lr,ur))}#
`default.unc` <-#
function (vs, uncL = 0.25, uncU = 0.75, uncLR = 0.25, uncUR = 0.75) {#
  vs <- replace.unc(vs, uncL, uncU, uncLR, uncUR)#
  return(vs)}#
#
`mean.unc` <- # uncL and uncU to replace outside bounds, otherwise just NAs#
function(vs, uncL = 0.0001, uncU = 0.99999) {#
  vs <- delete.unc(vs, uncL=uncL, uncU=uncU)#
tmp <- vs[!is.na(vs)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.mean <- truncate(rnorm(length(vs[is.na(vs)]), reg, reg_sig))#
  vs[is.na(vs)] <- new.mean#
  return(vs)}#
`impute.weights` <- # for use when turnout in districts is abnormally high or low (w = # of standard deviations from median)#
function(t1, t2, w = 2) {#
  t <- t1+t2#
  uncL <- (median(t, na.rm=T) -  (w * sd(t, na.rm=T)))#
  uncU <- (median(t, na.rm=T) + (w * sd(t, na.rm=T)))#
  t1 <- delete.unc(t, uncL=uncL, uncU=uncU)#
tmp <- t1[!is.na(t1)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.reg <- r(rnorm(length(t1[is.na(t1)]), reg, reg_sig), d=0)#
  t1[is.na(t1)] <- new.reg#
  return(t1)}#
`truncate` <-#
function(r1) {#
  below <- r1<=0#
  above <- r1>=1#
  between <- !(below|above)#
  (below*0.0001)+(r1*between)+0.9999*above#
}#
sim.election <- function(votes= NULL, center=house.2016.votes, incumbency=NULL, yr=2018, sims=1000, sigma=sigma) {#
  if (is.null(sims)) sims <- 1000#
      equal.vote <- mean(votes) - mean(center)#
      sims.year <- new.list(sims)#
    for (k in 1:sims)#
      {#
    sims.year[[k]] <- #
        rnorm(length(votes), votes - equal.vote, #
          sigma)#
      }#
  return(sims.year)#
  }#
#
r <- function(r, d=2) round(r, digits=d)#
# =================================================================#
# -- GERRYMANDER MEASURES -- GERRYMANDER MEASURES -- GERRYMANDER ME#
# =================================================================#
#  DELINATION #
`declination` <- # Warrington, Gregory S. 2018. Quantifying Gerrymandering Using the Vote Distribution. Election Law Journal 17(1): 3957. www.liebertpub.com (Accessed February 22, 2019).#
  function(votes) {#
    abo = votes[votes > 0.5]   # districts won by party A#
    bel = votes[votes <= 0.5]  # districts won by party B#
  # declination is undefined if one party wins all seats.#
    if (length(bel) == 0 | length(abo) == 0) {#
      return(NaN)#
    }#
  # angle for party B#
    theta = atan((1-(2*mean(bel))) / (length(bel) / length(votes)))#
  # angle for party A#
    gamma = atan((2*mean(abo)-1) / (length(abo) / length(votes)))#
  # normalize from radians to values betwen -1 and 1#
  # A little extra precision just in case :)#
    return(-1 * (2.0*(gamma-theta)/3.1415926535))}#
#
`declin2` <- # Simplified, not transformed from: Katz, Jonathan N. et al. 2018. Theoretical Foundations and Empirical Evaluations of Partisan Fairness in District-Based Democracies *. https://gking.harvard.edu/files/psym_2.pdf (Accessed March 16, 2019).#
  function(vs) {#
    abo = vs[vs > 0.5]   # districts won by party A#
    bel = vs[vs <= 0.5]  # districts won by party B#
    (-1 * (mean(abo) - 0.5) / (sum(find.winner(abo))/length(vs))) - ((0.5 - mean(bel)) / (1 - sum(find.winner(abo))/length(vs)))#
  }#
# #
#  MEAN/MEDIAN#
# #
`meanmedian` <- # Best, Robin E., Shawn J. Donahue, Jonathan Krasno, Daniel B. Magleby, et al. 2018. Considering the Prospects for Establishing a Packing Gerrymandering Standard. Election Law Journal: Rules, Politics, and Policy 17(1): 120. http://www.liebertpub.com/doi/10.1089/elj.2016.0392 (Accessed July 24, 2018).#
  function(votes) median(votes) - mean(votes)#
# #
#  EFFICIENCY GAP#
# #
`eff_gap` <- # Stephanopoulos, Nicholas, and Eric Mcghee. 2014. Partisan Gerrymandering and the Efficiency Gap. University of Chicago Law School 82. http://ssrn.com/abstract=2457468.. (Accessed September 10, 2018).#
  function(D, R) {#
    total <- sum(D)+sum(R)#
    dem_wasted <- sum(ifelse(D>R, D[D>R] - R[D>R], D[D<R]))#
    rep_wasted <- sum(ifelse(D<R, R[D<R] - D[D<R], R[D>R]))#
      return((dem_wasted - rep_wasted) / sum(total))}#
#
eg_TP <- function(votes) #
  {#
    y <- cbind.data.frame(votes, 1-votes)#
      return(-1 * eff_gap(y[1], y[2]))#
  }#
# #
#  GERRY DISPLAY #
# #
  gerry <- function(x, toggle=TRUE)#
    {#
    Seats = paste0(" [", seats.print(x), "]")#
    SeatPER = percent(seats(x))#
    Votes = percent(mean(default.unc(x)))#
    Bias = r(seatsvotes(x)$bias)#
    EfficiencyGap = r(eg_TP(x))#
    MeanMedian = r(meanmedian(x))#
    Declination = r(declination(x))#
    a <- rbind.data.frame(#
      Seats, SeatPER, Votes, Bias, EfficiencyGap, MeanMedian, Declination)#
    rownames(a) <- c("Seats","Seat %","Votes","Bias","Efficiency Gap","Mean/Median","Declination")#
    colnames(a) <- "Summary"#
#
    if (toggle!=TRUE) { # TO RETURN UNFORMATTED MEASUREMENTS#
    SeatsT = NA#
    SeatPERT = r(seats(x))#
    VotesT = r(mean(default.unc(x)))#
    BiasT = r(seatsvotes(x)$bias)#
    EfficiencyGapT = r(eg_TP(x))#
    MeanMedianT = r(meanmedian(x))#
    DeclinationT = r(declination(x))#
      return(rbind.data.frame(#
      SeatsT, SeatPERT, VotesT, BiasT, EfficiencyGapT, MeanMedianT, DeclinationT))}#
    return(a)#
    }#
#
p_value <- function(x){#
  l <- quantile(x[!is.na(x)], 0.025)#
  u <- quantile(x[!is.na(x)], 0.975)#
  t <- mean(x)/sd(x)#
    return(2*pt(-abs(t),df=length(x)-1))#
  }#
#
strs <- function(s) {#
  strs <- #
  ifelse(s <= 0.001, "***", #
    ifelse(0.001 < s & s <= 0.01, "**", #
      ifelse(0.01 < s & s <= 0.05, "*", "")))#
  return(paste0("$^{", strs, "}$"))#
}#
#
nintyfive <- function(x, percent=FALSE) {#
  if (percent==TRUE){ return(paste0("(", percent(r(quantile(x[!is.na(x)], 0.025),d=1)), ", ", percent(r(quantile(x[!is.na(x)], 0.975),d=1)), ")")) }#
  return(paste0("\\textit{[", r(quantile(x[!is.na(x)], 0.025)), ", ", r(quantile(x[!is.na(x)], 0.975)), "]}"))#
  }#
#
gtab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
  x.bias <- get(paste0(p, ".bias"))#
  x.eg <- get(paste0(p, ".eg"))#
  x.mm <- get(paste0(p, ".meanmedian"))#
  x.declin <- get(paste0(p, ".declination"))#
    return(c(#
      paste0(r(mean(x.bias)), strs(p_value(x.bias))), #
        nintyfive(x.bias), #
      paste0(r(mean(x.eg)), strs(p_value(x.eg))),#
        nintyfive(x.eg), #
      paste0(r(mean(x.mm)), strs(p_value(x.mm))),#
        nintyfive(x.mm),#
      paste0(r(mean(x.declin)), strs(p_value(x.declin))),#
        nintyfive(x.declin)#
      ))#
    }#
#
ptab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
    return(c(#
      paste0(r(sum(s.tmp*18)/(1000*18)*18), "R-", r(18-sum(s.tmp*18)/(1000*18)*18), "D"),#
      paste0("\\textit{[", r(quantile(s.tmp, 0.025),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.025),d=1)*18, "D, ", r(quantile(s.tmp, 0.975),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.975),d=1)*18, "D]}"),#
      paste0(median(s.tmp)*18, "R-", 18-median(s.tmp)*18, "D"),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) > 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) < 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) == 0.5) / 1000)#
      ))#
} #
#
 #    #
`circle.new` <- #
  function(xorig, yorig, radius, add, ...){#
    x <- seq(-radius, radius, length.out = 1000)#
  # Euclidian distance to the origin#
  y <- sapply(x, function(z) sqrt(radius^2 - z^2))#
  if(add == TRUE){#
    line(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
          type = "l", add=T, ...)#
   } else {#
   plot(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
        type = "l",  #
    xlab="", #
    ylab="", #
    xaxt="n", #
    yaxt="n", #
    bty="n", ...)#
   }#
}#
#
`circle` <- #
function (x, y, radius, nv = 100, border = NULL, col = NA, lty = 1, #
    lwd = 1) {#
    xylim <- par("usr")#
    plotdim <- par("pin")#
    ymult <- getYmult()#
    angle.inc <- 2 * pi/nv#
    angles <- seq(0, 2 * pi - angle.inc, by = angle.inc)#
    if (length(col) < length(radius)) #
        col <- rep(col, length.out = length(radius))#
    for (circle in 1:length(radius)) {#
        xv <- cos(angles) * radius[circle] + x#
        yv <- sin(angles) * radius[circle] * ymult + y#
        polygon(xv, yv, border = border, col = col[circle], lty = lty, #
            lwd = lwd)#
    }#
    invisible(list(x = xv, y = yv))#
}#
# #
# COMPACTNESS MEASURES#
#
POLSBYPOPPER <- function (x) r(mean((4 * 3.1415926535 * x$area) / (x$perimeter^2)), d = 3)#
REOCK <- function (x) r(mean(x$area/(x$smallestcircle^2 * 3.1415926535)), d = 3)#
poly.math <- function (x) {#
    compactness <- NULL#
  if (class(x)=="character") x <- get(x)#
      shapeFile <- x#
      mapObject <- fortify(shapeFile)#
      mapObject$id <- as.character(as.numeric(mapObject$id) + 1)#
      mapObject <- data.frame(mapObject, shapeFile@data[mapObject$id, ])#
      mapObject$piece <- as.character(mapObject$piece)#
      uniqueCDs <- sort(unique(as.numeric(mapObject$id)))#
      for(id in uniqueCDs)#
        {#
          cdShape <- mapObject[mapObject$id == id, ]#
          cdPoly <- SpatialPolygons(list(Polygons(lapply(split(cdShape[, c("long", "lat")], cdShape$piece), Polygon), ID = "b")))#
          owinObject <- try(as(cdPoly, "owin"))#
          compactness[[id]] <-  data.frame(area=area.owin(owinObject), perimeter=perimeter(owinObject), smallestcircle=boundingradius(owinObject))#
        }#
        x <- do.call("rbind", compactness)#
        cat(#
          "\n REOCK:        ", REOCK(x), "\n",#
          "POLSBY-POPPER:", POLSBYPOPPER(x), "\n \n"#
          )#
        return(x)#
  }
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)
ptab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
    return(c(#
      paste0(r(sum(s.tmp*18)/(1000*18)*18, d=1), "R-", r(18-sum(s.tmp*18)/(1000*18)*18, d=1), "D"),#
      paste0("\\textit{[", r(quantile(s.tmp, 0.025),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.025),d=1)*18, "D, ", r(quantile(s.tmp, 0.975),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.975),d=1)*18, "D]}"),#
      paste0(median(s.tmp)*18, "R-", 18-median(s.tmp)*18, "D"),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) > 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) < 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) == 0.5) / 1000)#
      ))#
}
#      #
#      #
#      #
# #
# #
cat(#
    "\n#
#
        CREATING TABLES . . . . . . . . . . . . . . . . . . . . .#
\n")#
sink("/dev/null")#
# races <- c("cong2016", "pres16", "ussen16", "atg16", "aud16", "trea16", "comp2016")#
statewide.contests.2016 <- c("Congress", "Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite")#
# #
# County Spliits#
county_splits <- c(41, 19, 19, 17)#
# COMPACTNESS#
geom_names <- paste0(plans, ".geom")#
#
reock.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        reock.plans <- rbind(reock.plans, REOCK(get(geom_names[i])))#
        }#
polsby.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        polsby.plans <- rbind(polsby.plans, POLSBYPOPPER(get(geom_names[i])))#
        }#
#
tab_plan_sum <- cbind.data.frame(#
    Plan = plan_names,#
    CountySplits = county_splits, #
    Reock = reock.plans, #
    PolsbyPopper = polsby.plans#
    # Pres2016 = c(seats.print(enacted.pres), seats.print(court.pres), seats.print(joint.pres), seats.print(govwolf.pres)),#
    # Comp2016 = c(seats.print(enacted.comp), seats.print(court.comp), seats.print(joint.comp), seats.print(govwolf.comp))#
    )#
tab_plan_sum#
plan_summary.caption = "County Splits and Compactness Scores of the Plans"#
plan_summary.label = "tab:summaries"#
plan_summary.footnote = "County splits include all the pieces in which a county is split, not just the total number of counties that have been split. (The latter number is the one most often reported in both court documents and in the media, but we regard the measure we report as both more precise and more informative.)"#
tab_plan_summary <- #
    stargazer(tab_plan_sum,#
        style = "apsr", #
        header = FALSE,#
        summary = FALSE,#
        model.numbers = FALSE,#
        initial.zero = TRUE,#
        digits = 3,#
        column.sep.width = "0pt",#
        rownames = FALSE,#
        multicolumn = TRUE,#
        label = plan_summary.label,#
        title = plan_summary.caption,#
        covariate.labels = ,#
        notes = plan_summary.footnote #
        )#
tab_plan_summary <- tab_plan_summary[c(-6, -7, -13, -14, -15, -16)]#
# tab_plan_summary <- append(tab_plan_summary, #
#   "  &   &  &  & \\textbf{Projected} & \\textbf{Projected}  \\\\", #
#   after = 5)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  &  \\textbf{County}  &  \\textbf{Polsby}   &   &  \\textbf{2016 using}  &  \\textbf{five state-wide} \\\\",#
#   after = 6)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  &  \\textbf{Presidential Results}  &  \\textbf{elections in 2016} \\\\",#
#   after = 7)#
#
tab_plan_summary <- append(tab_plan_summary,#
    "  &  \\textbf{County}  &  \\textbf{Polsby}   &   \\\\",#
    after = 5)#
tab_plan_summary <- append(tab_plan_summary,#
    "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  \\\\",#
    after = 6)#
# #
# #
# #
# #
# =================================================================#
# Comparing Pennsylvania Congressional Results with State-wide Elections (2016) %#
# =================================================================#
# #
# SUMMARY OF ELECTIONS#
percent(sum(house.rep.2012)/sum(house.2012.turnout))#
percent(sum(house.rep.2014)/sum(house.2014.turnout))#
percent(sum(house.rep.2016)/sum(house.2016.turnout))#
#
congsum.caption = "U.S. House Election Summaries \\\\ \\hspace{2cm}(PA 2012-2016 Enacted Map)"#
congsum.label = "tab:congsum"#
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."#
congsum.tmp <- cbind.data.frame(#
    # PA2004 = gerry(default.unc(house.2004.votes)),#
    # PA2006 = gerry(default.unc(house.2006.votes)),#
    # PA2008 = gerry(default.unc(house.2008.votes)),#
    # PA2010 = gerry(default.unc(house.2010.votes)),#
    PA2012 = gerry(default.unc(house.2012.votes)), #
    PA2014 = gerry(default.unc(house.2014.votes)), #
    PA2016 = gerry(default.unc(house.2016.votes)),#
    # PA2018 = gerry(default.unc(house.2018.votes)),#
    PA2012_2016_AVE = rowMeans(cbind(   #
    gerry(default.unc(house.2012.votes), toggle=F),#
    gerry(default.unc(house.2014.votes), toggle=F),#
    gerry(default.unc(house.2016.votes), toggle=F)))#
  )#
congsum.tmp[2:3,4]  <- percent(congsum.tmp[2:3,4])#
congsum.tmp[4:7,4] <- r(as.numeric(congsum.tmp[4:7,4]))#
colnames(congsum.tmp) <- c(seq(2012,2016,2),"AVE")#
congsum.tmp.tex <- stargazer(congsum.tmp,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    label = congsum.label,#
    title = congsum.caption,#
    notes = congsum.footnote)#
congsum.tmp.tex <- congsum.tmp.tex[c(-6, -16, -17, -18, -19)]#
congsum.tmp.tex[8] <- "Seats &  [13R-5D] &  [13R-5D] &  [13R-5D] & [13R-5D] \\\\ "#
# #
# #
# #
# #
dist_summary <- #
    rbind(#
      cbind.data.frame(#
        District = seq(1,18,1),#
        sapply(composite(pa.redist.dta, "enacted"), percent)),#
        c(#
          "Statewide",#
          sapply(colMeans(composite(pa.redist.dta, "enacted")), percent)#
        )#
    )#
#
district_summary.caption = "2016 District-level Summaries for 2011 Enacted Plan"#
district_summary.label = "tab:districtvotes"#
district_summary.footnote = "Uncontested races and those with only negligible competition will be imputed with 0.25 and 0.75 for the respective winners. All votes are calculated from the Republican perspective of the two-party vote. Composite does NOT include the Congressional elections. The statewide average is the unweighted mean of districts."#
#
district_summary <- stargazer(dist_summary,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    rownames = FALSE,#
    label = district_summary.label,#
    title = district_summary.caption,#
    notes = "REPLACE WITH NOTES" )#
district_summary <- district_summary[c(-6, -28, -29, -30, -31)]#
# #
# #
# #
# #
#
# =================================================================#
# Measures of Gerrymandering for the Eight Considered Plans#
# =================================================================#
gerry.table.gen <- #
    cbind.data.frame(#
        gtab("enacted"),#
        gtab("joint"),#
        gtab("govwolf"),#
        gtab("court")#
        )#
#
    colnames(gerry.table.gen) <- c(plan_names)#
    rownames(gerry.table.gen) <- c(#
        "Partisan Bias", #
        "CI1",#
        "Efficiency Gap",#
        "CI2",#
        "Mean/Median",#
        "CI3",#
        "Declination",#
        "CI4")#
#
gerry.caption = "Measures of Gerrymandering for the Four Considered Plans"#
gerry.label = "tab:gerry"#
gerry.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001. Measures are averages of 1,000 simulations for each map using the 2016 composite. Brackets numbers are the 95\\% range."#
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)#
    tab_gerry.tex <- gsub("$\\hat{\\mkern6mu}$", "^", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textasteriskcentered ", "*", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textbackslash ", "\\", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\{", "{", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\}", "}", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\$", "$", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)#
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]#
# #
# #
#
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )#
    colnames(tab_prop.gen) <- c(plan_names)#
    rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "SDS", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")#
#
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"#
prop.label = "tab:prob"#
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."#
#
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)#
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]#
#
# #
# #
#
# #
# #
#
# =================================================================#
# Information about Twelve States with Constitutional Provisions similar to Pennsylvania#
# =================================================================#
# #
# latex <- function(x) cat(paste(x, collapse="\n"), "\n")#
# states_compare.caption = "Information about Twelve States with Constitutional Provisions similar to Pennsylvania"#
# states_compare.label = "tab:states_compare"#
# states_compare.footnote = "Seats and votes are based on the 2016 five-election projection (to deal with the existence of non-contested congressional districts). Percentages are of the of district level results. This difference is why the percentages reported in columns 6 and 8 are not identical. Data from DailyKos, All about Redistricting, and Ballotpedia web sites.States with fewer than 9 districts do not have efficiency gap or median values reported because of the potential unreliability of those calculations given the small number of districts involved."#
#
# tab_states_compare <- #
#   paste("#
#       \\begin{table}#
#       \\caption{", states_compare.caption, "}#
#       \\label{", states_compare.label, "}#
#       \\begin{tabular}{#
#       >{\\centering\\arraybackslash}#
#       M{0.1\\linewidth}|#
#       M{.025\\linewidth}|#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.3\\linewidth}}#
#       \\textbf{State} & \\textbf{\\# CDs} & \\textbf{Unified Control (2011)} & \\textbf{Unified Control (2016)} & \\textbf{Seats (2016)} & \\textbf{Votes (2016)} & \\textbf{Mean District Vote Share (Dem)} & \\textbf{Median District Vote Share (Dem)} & \\textbf{Efficiency Gap} & \\textbf{Who does districting} \\\\#
#        \\hline#
#       Arizona & 9 & $\\surd$ (R) & $\\surd$ (R) & 44.4\\% & 48.1\\% & 50.4\\% & 49.4\\% & 0.08 & Independent commission \\\\#
#       Arkansas & 4 & $\\surd$ (D) & $\\surd$ (R) & 0\\% & 35.7\\% & 35.5\\% &  &  & state legislature \\\\#
#       Delaware & 1 & $\\surd$ (D) & $\\surd$ (D) & 100\\% & 55.9\\% & 56\\% &  &  & NA \\\\#
#       Illinois & 18 & $\\surd$ (D) &  & 61.1\\% & 59.0\\% & 59.6\\% & 59.8\\% & 0.08 & state legislature \\\\#
#       Indiana & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 39.9\\% & 40.1\\% & 35.9\\% & 0.09 & state legislature \\\\#
#       Kentucky & 6 &  & $\\surd$ (R) & 16.7\\% & 34.3\\% & 33.8\\% &  &  & state legislature \\\\#
#       Oklahoma & 5 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 30.7\\% & 30.7\\% &  &  & state legislature \\\\#
#       Oregon & 5 &  & $\\surd$ (D) & 80\\% & 56.2\\% & 56\\% &  &  & state legislature \\\\#
#       South Dakota & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 34.0\\% & 34\\% &  &  & NA \\\\#
#       Tennessee & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 36.4\\% & 37.4\\% & 31.3\\% & 0.03 & state legislature \\\\#
#       Washington & 10 & $\\surd$ (D) & $\\surd$ (D) & 70\\% & 58.8\\% & 56\\% & 52.3\\% & -0.05 & 5-member independent commission \\\\#
#       Wyoming & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 24.3\\% & 24.3\\% &  &  & NA ")#
# #
# #
#
# TWO-PARTY VOTE SHARES, ACTUAL (CONGRESSIONAL)#
# tab_summary_stats <- stargazer(#
#   congress.PA.2008.2018,#
#   title="Republican Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   style = "apsr", #
#   header = FALSE,#
#   column.sep.width = "-5pt",#
#   multicolumn = TRUE,#
#   label = "tab:twoparty_actuals" )#
# tab_summary_stats <- tab_summary_stats[c(-6, -11)]#
#
#  Table(tab_summary_stats,#
#   caption="Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   label="ab:twoparty_actuals",#
#   footnote="Votes are the two-party unweighted average of PA Congressional Districts by year (uncontested elections imputed).")#
#
# #
# AVERAGE VOTE BY DISTRICT#
means_races <- numeric()#
  base.form <- formula(. ~ 1)#
  cong.form <- formula(default.unc(enacted.elections$CONG) ~ + pahouse.2016$INC)#
for (k in colnames(enacted.elections))#
{#
  r.tmp <- composite(pa.redist.dta, "enacted")[,k]#
    reg_races <- summary(lm(update(base.form, as.formula(default.unc(r.tmp) ~ .))))#
    means_races[k] <- coef(reg_races)[1]#
}#
#
  cat(#
    "2016 Average Vote Share by State-level Contest \n"#
    )#
    print(cbind.data.frame(Race = statewide.contests.2016, Mean = percent(means_races)))#
    cat(paste("", collapse = "\n"), "\n")#
# # Regression of Race on Congressional Results#
    reg_cong_pres <- lm(update(cong.form, as.formula(. ~ . + default.unc(enacted.elections$PRES))))#
    reg_cong_ussen <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$USSEN))))#
    reg_cong_atg <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$ATTGEN))))#
    reg_cong_aud <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$AUDITOR))))#
    reg_cong_trea <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$TREASURER))))#
    reg_cong_comp <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$COMPOSITE))))#
    reg_cong_all <- lm(update(cong.form, #
      as.formula(. ~  . +  #
        default.unc(enacted.elections$PRES) + #
        default.unc(enacted.elections$USSEN) +#
        default.unc(enacted.elections$ATTGEN) +#
        default.unc(enacted.elections$AUDITOR) +#
        default.unc(enacted.elections$TREASURER))#
    ))#
#
congress_predict.caption = "Comparing Pennsylvania Congressional Results with State-wide Elections (2016)"#
congress_predict.label = "tab:tab_congress_predict"#
congress_predict.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001 \\\\ Uncontested (or non-competitive) elections replaced with 0.25 \\& 0.75 vote shares. Regressions are unweighted, ie, all districts are assumed to have identical turnout. This is the usual way political scientist measure aggregate congressional vote \\cite{GelmanKing1994_unifiedAJPS}. \\\\"#
#
tab_congress_predict <- stargazer(#
        reg_cong_pres, #
         reg_cong_ussen, #
         reg_cong_atg, #
         reg_cong_aud, #
         reg_cong_trea, #
         reg_cong_comp,#
         reg_cong_all,#
star.cutoffs = c(0.05,0.01, 0.001),#
style = "apsr", #
header = FALSE,#
model.numbers = FALSE,#
initial.zero = TRUE,#
digits = 2,#
column.sep.width = "0pt",#
multicolumn = TRUE,#
omit.stat = c("ll", "F", "ser"),#
dep.var.labels = "Actual Congressional Results (2016)",#
covariate.labels = c("Incumbency Advantage","Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite"),#
label = congress_predict.label,#
title = congress_predict.caption,#
notes = congress_predict.footnote#
    )#
tab_congress_predict <- tab_congress_predict[c(-6, -26, -28, -29, -30, -31, -32)]#
# #
#
sink()#
Table(tab_plan_summary,#
    path = "Tables/_tab_summaries.tex",#
    caption = plan_summary.caption,#
    label = plan_summary.label,#
    footnote = plan_summary.footnote)#
#
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)#
#
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)#
#
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)#
#
Table(district_summary,#
    path = "Tables/_a_tab_districtvotes.tex",#
    caption = district_summary.caption,#
    label = district_summary.label,#
    footnote = district_summary.footnote#
    )#
#
Table(tab_congress_predict,#
    path = "Tables/a_tab_congress_predict.tex",#
    caption = congress_predict.caption,#
    label = congress_predict.label,#
    footnote = congress_predict.footnote)#
cat("#
#
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#
    ")
#      #
#      #
#      #
# #
# #
cat(#
    "\n#
#
        CREATING TABLES . . . . . . . . . . . . . . . . . . . . .#
\n")#
sink("/dev/null")#
# races <- c("cong2016", "pres16", "ussen16", "atg16", "aud16", "trea16", "comp2016")#
statewide.contests.2016 <- c("Congress", "Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite")#
# #
# County Spliits#
county_splits <- c(41, 19, 19, 17)#
# COMPACTNESS#
geom_names <- paste0(plans, ".geom")#
#
reock.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        reock.plans <- rbind(reock.plans, REOCK(get(geom_names[i])))#
        }#
polsby.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        polsby.plans <- rbind(polsby.plans, POLSBYPOPPER(get(geom_names[i])))#
        }#
#
tab_plan_sum <- cbind.data.frame(#
    Plan = plan_names,#
    CountySplits = county_splits, #
    Reock = reock.plans, #
    PolsbyPopper = polsby.plans#
    # Pres2016 = c(seats.print(enacted.pres), seats.print(court.pres), seats.print(joint.pres), seats.print(govwolf.pres)),#
    # Comp2016 = c(seats.print(enacted.comp), seats.print(court.comp), seats.print(joint.comp), seats.print(govwolf.comp))#
    )#
tab_plan_sum#
plan_summary.caption = "County Splits and Compactness Scores of the Plans"#
plan_summary.label = "tab:summaries"#
plan_summary.footnote = "County splits include all the pieces in which a county is split, not just the total number of counties that have been split. (The latter number is the one most often reported in both court documents and in the media, but we regard the measure we report as both more precise and more informative.)"#
tab_plan_summary <- #
    stargazer(tab_plan_sum,#
        style = "apsr", #
        header = FALSE,#
        summary = FALSE,#
        model.numbers = FALSE,#
        initial.zero = TRUE,#
        digits = 3,#
        column.sep.width = "0pt",#
        rownames = FALSE,#
        multicolumn = TRUE,#
        label = plan_summary.label,#
        title = plan_summary.caption,#
        covariate.labels = ,#
        notes = plan_summary.footnote #
        )#
tab_plan_summary <- tab_plan_summary[c(-6, -7, -13, -14, -15, -16)]#
# tab_plan_summary <- append(tab_plan_summary, #
#   "  &   &  &  & \\textbf{Projected} & \\textbf{Projected}  \\\\", #
#   after = 5)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  &  \\textbf{County}  &  \\textbf{Polsby}   &   &  \\textbf{2016 using}  &  \\textbf{five state-wide} \\\\",#
#   after = 6)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  &  \\textbf{Presidential Results}  &  \\textbf{elections in 2016} \\\\",#
#   after = 7)#
#
tab_plan_summary <- append(tab_plan_summary,#
    "  &  \\textbf{County}  &  \\textbf{Polsby}   &   \\\\",#
    after = 5)#
tab_plan_summary <- append(tab_plan_summary,#
    "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  \\\\",#
    after = 6)#
# #
# #
# #
# #
# =================================================================#
# Comparing Pennsylvania Congressional Results with State-wide Elections (2016) %#
# =================================================================#
# #
# SUMMARY OF ELECTIONS#
percent(sum(house.rep.2012)/sum(house.2012.turnout))#
percent(sum(house.rep.2014)/sum(house.2014.turnout))#
percent(sum(house.rep.2016)/sum(house.2016.turnout))#
#
congsum.caption = "U.S. House Election Summaries \\\\ \\hspace{2cm}(PA 2012-2016 Enacted Map)"#
congsum.label = "tab:congsum"#
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."#
congsum.tmp <- cbind.data.frame(#
    # PA2004 = gerry(default.unc(house.2004.votes)),#
    # PA2006 = gerry(default.unc(house.2006.votes)),#
    # PA2008 = gerry(default.unc(house.2008.votes)),#
    # PA2010 = gerry(default.unc(house.2010.votes)),#
    PA2012 = gerry(default.unc(house.2012.votes)), #
    PA2014 = gerry(default.unc(house.2014.votes)), #
    PA2016 = gerry(default.unc(house.2016.votes)),#
    # PA2018 = gerry(default.unc(house.2018.votes)),#
    PA2012_2016_AVE = rowMeans(cbind(   #
    gerry(default.unc(house.2012.votes), toggle=F),#
    gerry(default.unc(house.2014.votes), toggle=F),#
    gerry(default.unc(house.2016.votes), toggle=F)))#
  )#
congsum.tmp[2:3,4]  <- percent(congsum.tmp[2:3,4])#
congsum.tmp[4:7,4] <- r(as.numeric(congsum.tmp[4:7,4]))#
colnames(congsum.tmp) <- c(seq(2012,2016,2),"AVE")#
congsum.tmp.tex <- stargazer(congsum.tmp,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    label = congsum.label,#
    title = congsum.caption,#
    notes = congsum.footnote)#
congsum.tmp.tex <- congsum.tmp.tex[c(-6, -16, -17, -18, -19)]#
congsum.tmp.tex[8] <- "Seats &  [13R-5D] &  [13R-5D] &  [13R-5D] & [13R-5D] \\\\ "#
# #
# #
# #
# #
dist_summary <- #
    rbind(#
      cbind.data.frame(#
        District = seq(1,18,1),#
        sapply(composite(pa.redist.dta, "enacted"), percent)),#
        c(#
          "Statewide",#
          sapply(colMeans(composite(pa.redist.dta, "enacted")), percent)#
        )#
    )#
#
district_summary.caption = "2016 District-level Summaries for 2011 Enacted Plan"#
district_summary.label = "tab:districtvotes"#
district_summary.footnote = "Uncontested races and those with only negligible competition will be imputed with 0.25 and 0.75 for the respective winners. All votes are calculated from the Republican perspective of the two-party vote. Composite does NOT include the Congressional elections. The statewide average is the unweighted mean of districts."#
#
district_summary <- stargazer(dist_summary,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    rownames = FALSE,#
    label = district_summary.label,#
    title = district_summary.caption,#
    notes = "REPLACE WITH NOTES" )#
district_summary <- district_summary[c(-6, -28, -29, -30, -31)]#
# #
# #
# #
# #
#
# =================================================================#
# Measures of Gerrymandering for the Eight Considered Plans#
# =================================================================#
gerry.table.gen <- #
    cbind.data.frame(#
        gtab("enacted"),#
        gtab("joint"),#
        gtab("govwolf"),#
        gtab("court")#
        )#
#
    colnames(gerry.table.gen) <- c(plan_names)#
    rownames(gerry.table.gen) <- c(#
        "Partisan Bias", #
        "CI1",#
        "Efficiency Gap",#
        "CI2",#
        "Mean/Median",#
        "CI3",#
        "Declination",#
        "CI4")#
#
gerry.caption = "Measures of Gerrymandering for the Four Considered Plans"#
gerry.label = "tab:gerry"#
gerry.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001. Measures are averages of 1,000 simulations for each map using the 2016 composite. Brackets numbers are the 95\\% range."#
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)#
    tab_gerry.tex <- gsub("$\\hat{\\mkern6mu}$", "^", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textasteriskcentered ", "*", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textbackslash ", "\\", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\{", "{", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\}", "}", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\$", "$", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)#
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]#
# #
# #
#
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )#
    colnames(tab_prop.gen) <- c(plan_names)#
    rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "SDS", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")#
#
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"#
prop.label = "tab:prob"#
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."#
#
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)#
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]#
#
# #
# #
#
# #
# #
#
# =================================================================#
# Information about Twelve States with Constitutional Provisions similar to Pennsylvania#
# =================================================================#
# #
# latex <- function(x) cat(paste(x, collapse="\n"), "\n")#
# states_compare.caption = "Information about Twelve States with Constitutional Provisions similar to Pennsylvania"#
# states_compare.label = "tab:states_compare"#
# states_compare.footnote = "Seats and votes are based on the 2016 five-election projection (to deal with the existence of non-contested congressional districts). Percentages are of the of district level results. This difference is why the percentages reported in columns 6 and 8 are not identical. Data from DailyKos, All about Redistricting, and Ballotpedia web sites.States with fewer than 9 districts do not have efficiency gap or median values reported because of the potential unreliability of those calculations given the small number of districts involved."#
#
# tab_states_compare <- #
#   paste("#
#       \\begin{table}#
#       \\caption{", states_compare.caption, "}#
#       \\label{", states_compare.label, "}#
#       \\begin{tabular}{#
#       >{\\centering\\arraybackslash}#
#       M{0.1\\linewidth}|#
#       M{.025\\linewidth}|#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.3\\linewidth}}#
#       \\textbf{State} & \\textbf{\\# CDs} & \\textbf{Unified Control (2011)} & \\textbf{Unified Control (2016)} & \\textbf{Seats (2016)} & \\textbf{Votes (2016)} & \\textbf{Mean District Vote Share (Dem)} & \\textbf{Median District Vote Share (Dem)} & \\textbf{Efficiency Gap} & \\textbf{Who does districting} \\\\#
#        \\hline#
#       Arizona & 9 & $\\surd$ (R) & $\\surd$ (R) & 44.4\\% & 48.1\\% & 50.4\\% & 49.4\\% & 0.08 & Independent commission \\\\#
#       Arkansas & 4 & $\\surd$ (D) & $\\surd$ (R) & 0\\% & 35.7\\% & 35.5\\% &  &  & state legislature \\\\#
#       Delaware & 1 & $\\surd$ (D) & $\\surd$ (D) & 100\\% & 55.9\\% & 56\\% &  &  & NA \\\\#
#       Illinois & 18 & $\\surd$ (D) &  & 61.1\\% & 59.0\\% & 59.6\\% & 59.8\\% & 0.08 & state legislature \\\\#
#       Indiana & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 39.9\\% & 40.1\\% & 35.9\\% & 0.09 & state legislature \\\\#
#       Kentucky & 6 &  & $\\surd$ (R) & 16.7\\% & 34.3\\% & 33.8\\% &  &  & state legislature \\\\#
#       Oklahoma & 5 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 30.7\\% & 30.7\\% &  &  & state legislature \\\\#
#       Oregon & 5 &  & $\\surd$ (D) & 80\\% & 56.2\\% & 56\\% &  &  & state legislature \\\\#
#       South Dakota & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 34.0\\% & 34\\% &  &  & NA \\\\#
#       Tennessee & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 36.4\\% & 37.4\\% & 31.3\\% & 0.03 & state legislature \\\\#
#       Washington & 10 & $\\surd$ (D) & $\\surd$ (D) & 70\\% & 58.8\\% & 56\\% & 52.3\\% & -0.05 & 5-member independent commission \\\\#
#       Wyoming & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 24.3\\% & 24.3\\% &  &  & NA ")#
# #
# #
#
# TWO-PARTY VOTE SHARES, ACTUAL (CONGRESSIONAL)#
# tab_summary_stats <- stargazer(#
#   congress.PA.2008.2018,#
#   title="Republican Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   style = "apsr", #
#   header = FALSE,#
#   column.sep.width = "-5pt",#
#   multicolumn = TRUE,#
#   label = "tab:twoparty_actuals" )#
# tab_summary_stats <- tab_summary_stats[c(-6, -11)]#
#
#  Table(tab_summary_stats,#
#   caption="Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   label="ab:twoparty_actuals",#
#   footnote="Votes are the two-party unweighted average of PA Congressional Districts by year (uncontested elections imputed).")#
#
# #
# AVERAGE VOTE BY DISTRICT#
means_races <- numeric()#
  base.form <- formula(. ~ 1)#
  cong.form <- formula(default.unc(enacted.elections$CONG) ~ + pahouse.2016$INC)#
for (k in colnames(enacted.elections))#
{#
  r.tmp <- composite(pa.redist.dta, "enacted")[,k]#
    reg_races <- summary(lm(update(base.form, as.formula(default.unc(r.tmp) ~ .))))#
    means_races[k] <- coef(reg_races)[1]#
}#
#
  cat(#
    "2016 Average Vote Share by State-level Contest \n"#
    )#
    print(cbind.data.frame(Race = statewide.contests.2016, Mean = percent(means_races)))#
    cat(paste("", collapse = "\n"), "\n")#
# # Regression of Race on Congressional Results#
    reg_cong_pres <- lm(update(cong.form, as.formula(. ~ . + default.unc(enacted.elections$PRES))))#
    reg_cong_ussen <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$USSEN))))#
    reg_cong_atg <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$ATTGEN))))#
    reg_cong_aud <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$AUDITOR))))#
    reg_cong_trea <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$TREASURER))))#
    reg_cong_comp <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$COMPOSITE))))#
    reg_cong_all <- lm(update(cong.form, #
      as.formula(. ~  . +  #
        default.unc(enacted.elections$PRES) + #
        default.unc(enacted.elections$USSEN) +#
        default.unc(enacted.elections$ATTGEN) +#
        default.unc(enacted.elections$AUDITOR) +#
        default.unc(enacted.elections$TREASURER))#
    ))#
#
congress_predict.caption = "Comparing Pennsylvania Congressional Results with State-wide Elections (2016)"#
congress_predict.label = "tab:tab_congress_predict"#
congress_predict.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001 \\\\ Uncontested (or non-competitive) elections replaced with 0.25 \\& 0.75 vote shares. Regressions are unweighted, ie, all districts are assumed to have identical turnout. This is the usual way political scientist measure aggregate congressional vote \\cite{GelmanKing1994_unifiedAJPS}. \\\\"#
#
tab_congress_predict <- stargazer(#
        reg_cong_pres, #
         reg_cong_ussen, #
         reg_cong_atg, #
         reg_cong_aud, #
         reg_cong_trea, #
         reg_cong_comp,#
         reg_cong_all,#
star.cutoffs = c(0.05,0.01, 0.001),#
style = "apsr", #
header = FALSE,#
model.numbers = FALSE,#
initial.zero = TRUE,#
digits = 2,#
column.sep.width = "0pt",#
multicolumn = TRUE,#
omit.stat = c("ll", "F", "ser"),#
dep.var.labels = "Actual Congressional Results (2016)",#
covariate.labels = c("Incumbency Advantage","Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite"),#
label = congress_predict.label,#
title = congress_predict.caption,#
notes = congress_predict.footnote#
    )#
tab_congress_predict <- tab_congress_predict[c(-6, -26, -28, -29, -30, -31, -32)]#
# #
#
sink()#
Table(tab_plan_summary,#
    path = "Tables/_tab_summaries.tex",#
    caption = plan_summary.caption,#
    label = plan_summary.label,#
    footnote = plan_summary.footnote)#
#
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)#
#
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)#
#
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)#
#
Table(district_summary,#
    path = "Tables/_a_tab_districtvotes.tex",#
    caption = district_summary.caption,#
    label = district_summary.label,#
    footnote = district_summary.footnote#
    )#
#
Table(tab_congress_predict,#
    path = "Tables/a_tab_congress_predict.tex",#
    caption = congress_predict.caption,#
    label = congress_predict.label,#
    footnote = congress_predict.footnote)#
cat("#
#
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#
    ")
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)
# tab_gerry.tex <- gsub("$\\hat{\\mkern6mu}$", "^", tab_gerry.tex,fixed=TRUE)
# tab_gerry.tex <- gsub("\\textasteriskcentered ", "*", tab_gerry.tex,fixed=TRUE)
# tab_gerry.tex <- gsub("\\textbackslash ", "\\", tab_gerry.tex,fixed=TRUE)
# tab_gerry.tex <- gsub("\\{", "{", tab_gerry.tex,fixed=TRUE)
#    tab_gerry.tex <- gsub("\\}", "}", tab_gerry.tex,fixed=TRUE)
#    tab_gerry.tex <- gsub("\\$", "$", tab_gerry.tex,fixed=TRUE)
tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )
colnames(tab_prop.gen) <- c(plan_names)
rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "CI1", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"
prop.label = "tab:prob"
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)
tab_prop <- gsub("CI[1-9]", "", tab_prop)
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)
tab_prop
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )#
    colnames(tab_prop.gen) <- c(plan_names)#
    rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "CI1", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")#
#
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"#
prop.label = "tab:prob"#
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."#
#
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)#
tab_prop.tex <- gsub("CI[1-9]", "", tab_prop.tex)#
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)
ptab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
    return(c(#
      paste0(r(sum(s.tmp*18)/(1000*18)*18, d=1), "R-", r(18-sum(s.tmp*18)/(1000*18)*18, d=1), "D"),#
      paste0("{\\small\\textit{[", r(quantile(s.tmp, 0.025),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.025),d=1)*18, "D, ", r(quantile(s.tmp, 0.975),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.975),d=1)*18, "D]}}"),#
      paste0(median(s.tmp)*18, "R-", 18-median(s.tmp)*18, "D"),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) > 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) < 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) == 0.5) / 1000)#
      ))#
}
#      #
#      #
#      #
# #
# #
cat(#
    "\n#
#
        CREATING TABLES . . . . . . . . . . . . . . . . . . . . .#
\n")#
sink("/dev/null")#
# races <- c("cong2016", "pres16", "ussen16", "atg16", "aud16", "trea16", "comp2016")#
statewide.contests.2016 <- c("Congress", "Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite")#
# #
# County Spliits#
county_splits <- c(41, 19, 19, 17)#
# COMPACTNESS#
geom_names <- paste0(plans, ".geom")#
#
reock.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        reock.plans <- rbind(reock.plans, REOCK(get(geom_names[i])))#
        }#
polsby.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        polsby.plans <- rbind(polsby.plans, POLSBYPOPPER(get(geom_names[i])))#
        }#
#
tab_plan_sum <- cbind.data.frame(#
    Plan = plan_names,#
    CountySplits = county_splits, #
    Reock = reock.plans, #
    PolsbyPopper = polsby.plans#
    # Pres2016 = c(seats.print(enacted.pres), seats.print(court.pres), seats.print(joint.pres), seats.print(govwolf.pres)),#
    # Comp2016 = c(seats.print(enacted.comp), seats.print(court.comp), seats.print(joint.comp), seats.print(govwolf.comp))#
    )#
tab_plan_sum#
plan_summary.caption = "County Splits and Compactness Scores of the Plans"#
plan_summary.label = "tab:summaries"#
plan_summary.footnote = "County splits include all the pieces in which a county is split, not just the total number of counties that have been split. (The latter number is the one most often reported in both court documents and in the media, but we regard the measure we report as both more precise and more informative.)"#
tab_plan_summary <- #
    stargazer(tab_plan_sum,#
        style = "apsr", #
        header = FALSE,#
        summary = FALSE,#
        model.numbers = FALSE,#
        initial.zero = TRUE,#
        digits = 3,#
        column.sep.width = "0pt",#
        rownames = FALSE,#
        multicolumn = TRUE,#
        label = plan_summary.label,#
        title = plan_summary.caption,#
        covariate.labels = ,#
        notes = plan_summary.footnote #
        )#
tab_plan_summary <- tab_plan_summary[c(-6, -7, -13, -14, -15, -16)]#
# tab_plan_summary <- append(tab_plan_summary, #
#   "  &   &  &  & \\textbf{Projected} & \\textbf{Projected}  \\\\", #
#   after = 5)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  &  \\textbf{County}  &  \\textbf{Polsby}   &   &  \\textbf{2016 using}  &  \\textbf{five state-wide} \\\\",#
#   after = 6)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  &  \\textbf{Presidential Results}  &  \\textbf{elections in 2016} \\\\",#
#   after = 7)#
#
tab_plan_summary <- append(tab_plan_summary,#
    "  &  \\textbf{County}  &  \\textbf{Polsby}   &   \\\\",#
    after = 5)#
tab_plan_summary <- append(tab_plan_summary,#
    "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  \\\\",#
    after = 6)#
# #
# #
# #
# #
# =================================================================#
# Comparing Pennsylvania Congressional Results with State-wide Elections (2016) %#
# =================================================================#
# #
# SUMMARY OF ELECTIONS#
percent(sum(house.rep.2012)/sum(house.2012.turnout))#
percent(sum(house.rep.2014)/sum(house.2014.turnout))#
percent(sum(house.rep.2016)/sum(house.2016.turnout))#
#
congsum.caption = "U.S. House Election Summaries \\\\ \\hspace{2cm}(PA 2012-2016 Enacted Map)"#
congsum.label = "tab:congsum"#
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."#
congsum.tmp <- cbind.data.frame(#
    # PA2004 = gerry(default.unc(house.2004.votes)),#
    # PA2006 = gerry(default.unc(house.2006.votes)),#
    # PA2008 = gerry(default.unc(house.2008.votes)),#
    # PA2010 = gerry(default.unc(house.2010.votes)),#
    PA2012 = gerry(default.unc(house.2012.votes)), #
    PA2014 = gerry(default.unc(house.2014.votes)), #
    PA2016 = gerry(default.unc(house.2016.votes)),#
    # PA2018 = gerry(default.unc(house.2018.votes)),#
    PA2012_2016_AVE = rowMeans(cbind(   #
    gerry(default.unc(house.2012.votes), toggle=F),#
    gerry(default.unc(house.2014.votes), toggle=F),#
    gerry(default.unc(house.2016.votes), toggle=F)))#
  )#
congsum.tmp[2:3,4]  <- percent(congsum.tmp[2:3,4])#
congsum.tmp[4:7,4] <- r(as.numeric(congsum.tmp[4:7,4]))#
colnames(congsum.tmp) <- c(seq(2012,2016,2),"AVE")#
congsum.tmp.tex <- stargazer(congsum.tmp,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    label = congsum.label,#
    title = congsum.caption,#
    notes = congsum.footnote)#
congsum.tmp.tex <- congsum.tmp.tex[c(-6, -16, -17, -18, -19)]#
congsum.tmp.tex[8] <- "Seats &  [13R-5D] &  [13R-5D] &  [13R-5D] & [13R-5D] \\\\ "#
# #
# #
# #
# #
dist_summary <- #
    rbind(#
      cbind.data.frame(#
        District = seq(1,18,1),#
        sapply(composite(pa.redist.dta, "enacted"), percent)),#
        c(#
          "Statewide",#
          sapply(colMeans(composite(pa.redist.dta, "enacted")), percent)#
        )#
    )#
#
district_summary.caption = "2016 District-level Summaries for 2011 Enacted Plan"#
district_summary.label = "tab:districtvotes"#
district_summary.footnote = "Uncontested races and those with only negligible competition will be imputed with 0.25 and 0.75 for the respective winners. All votes are calculated from the Republican perspective of the two-party vote. Composite does NOT include the Congressional elections. The statewide average is the unweighted mean of districts."#
#
district_summary <- stargazer(dist_summary,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    rownames = FALSE,#
    label = district_summary.label,#
    title = district_summary.caption,#
    notes = "REPLACE WITH NOTES" )#
district_summary <- district_summary[c(-6, -28, -29, -30, -31)]#
# #
# #
# #
# #
#
# =================================================================#
# Measures of Gerrymandering for the Eight Considered Plans#
# =================================================================#
gerry.table.gen <- #
    cbind.data.frame(#
        gtab("enacted"),#
        gtab("joint"),#
        gtab("govwolf"),#
        gtab("court")#
        )#
#
    colnames(gerry.table.gen) <- c(plan_names)#
    rownames(gerry.table.gen) <- c(#
        "Partisan Bias", #
        "CI1",#
        "Efficiency Gap",#
        "CI2",#
        "Mean/Median",#
        "CI3",#
        "Declination",#
        "CI4")#
#
gerry.caption = "Measures of Gerrymandering for the Four Considered Plans"#
gerry.label = "tab:gerry"#
gerry.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001. Measures are averages of 1,000 simulations for each map using the 2016 composite. Brackets numbers are the 95\\% range."#
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)#
    tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)#
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]#
# #
# #
#
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )#
    colnames(tab_prop.gen) <- c(plan_names)#
    rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "CI1", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")#
#
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"#
prop.label = "tab:prob"#
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."#
#
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)#
tab_prop.tex <- gsub("CI[1-9]", "", tab_prop.tex)#
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]#
#
# #
# #
#
# #
# #
#
# =================================================================#
# Information about Twelve States with Constitutional Provisions similar to Pennsylvania#
# =================================================================#
# #
# latex <- function(x) cat(paste(x, collapse="\n"), "\n")#
# states_compare.caption = "Information about Twelve States with Constitutional Provisions similar to Pennsylvania"#
# states_compare.label = "tab:states_compare"#
# states_compare.footnote = "Seats and votes are based on the 2016 five-election projection (to deal with the existence of non-contested congressional districts). Percentages are of the of district level results. This difference is why the percentages reported in columns 6 and 8 are not identical. Data from DailyKos, All about Redistricting, and Ballotpedia web sites.States with fewer than 9 districts do not have efficiency gap or median values reported because of the potential unreliability of those calculations given the small number of districts involved."#
#
# tab_states_compare <- #
#   paste("#
#       \\begin{table}#
#       \\caption{", states_compare.caption, "}#
#       \\label{", states_compare.label, "}#
#       \\begin{tabular}{#
#       >{\\centering\\arraybackslash}#
#       M{0.1\\linewidth}|#
#       M{.025\\linewidth}|#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.3\\linewidth}}#
#       \\textbf{State} & \\textbf{\\# CDs} & \\textbf{Unified Control (2011)} & \\textbf{Unified Control (2016)} & \\textbf{Seats (2016)} & \\textbf{Votes (2016)} & \\textbf{Mean District Vote Share (Dem)} & \\textbf{Median District Vote Share (Dem)} & \\textbf{Efficiency Gap} & \\textbf{Who does districting} \\\\#
#        \\hline#
#       Arizona & 9 & $\\surd$ (R) & $\\surd$ (R) & 44.4\\% & 48.1\\% & 50.4\\% & 49.4\\% & 0.08 & Independent commission \\\\#
#       Arkansas & 4 & $\\surd$ (D) & $\\surd$ (R) & 0\\% & 35.7\\% & 35.5\\% &  &  & state legislature \\\\#
#       Delaware & 1 & $\\surd$ (D) & $\\surd$ (D) & 100\\% & 55.9\\% & 56\\% &  &  & NA \\\\#
#       Illinois & 18 & $\\surd$ (D) &  & 61.1\\% & 59.0\\% & 59.6\\% & 59.8\\% & 0.08 & state legislature \\\\#
#       Indiana & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 39.9\\% & 40.1\\% & 35.9\\% & 0.09 & state legislature \\\\#
#       Kentucky & 6 &  & $\\surd$ (R) & 16.7\\% & 34.3\\% & 33.8\\% &  &  & state legislature \\\\#
#       Oklahoma & 5 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 30.7\\% & 30.7\\% &  &  & state legislature \\\\#
#       Oregon & 5 &  & $\\surd$ (D) & 80\\% & 56.2\\% & 56\\% &  &  & state legislature \\\\#
#       South Dakota & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 34.0\\% & 34\\% &  &  & NA \\\\#
#       Tennessee & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 36.4\\% & 37.4\\% & 31.3\\% & 0.03 & state legislature \\\\#
#       Washington & 10 & $\\surd$ (D) & $\\surd$ (D) & 70\\% & 58.8\\% & 56\\% & 52.3\\% & -0.05 & 5-member independent commission \\\\#
#       Wyoming & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 24.3\\% & 24.3\\% &  &  & NA ")#
# #
# #
#
# TWO-PARTY VOTE SHARES, ACTUAL (CONGRESSIONAL)#
# tab_summary_stats <- stargazer(#
#   congress.PA.2008.2018,#
#   title="Republican Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   style = "apsr", #
#   header = FALSE,#
#   column.sep.width = "-5pt",#
#   multicolumn = TRUE,#
#   label = "tab:twoparty_actuals" )#
# tab_summary_stats <- tab_summary_stats[c(-6, -11)]#
#
#  Table(tab_summary_stats,#
#   caption="Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   label="ab:twoparty_actuals",#
#   footnote="Votes are the two-party unweighted average of PA Congressional Districts by year (uncontested elections imputed).")#
#
# #
# AVERAGE VOTE BY DISTRICT#
means_races <- numeric()#
  base.form <- formula(. ~ 1)#
  cong.form <- formula(default.unc(enacted.elections$CONG) ~ + pahouse.2016$INC)#
for (k in colnames(enacted.elections))#
{#
  r.tmp <- composite(pa.redist.dta, "enacted")[,k]#
    reg_races <- summary(lm(update(base.form, as.formula(default.unc(r.tmp) ~ .))))#
    means_races[k] <- coef(reg_races)[1]#
}#
#
  cat(#
    "2016 Average Vote Share by State-level Contest \n"#
    )#
    print(cbind.data.frame(Race = statewide.contests.2016, Mean = percent(means_races)))#
    cat(paste("", collapse = "\n"), "\n")#
# # Regression of Race on Congressional Results#
    reg_cong_pres <- lm(update(cong.form, as.formula(. ~ . + default.unc(enacted.elections$PRES))))#
    reg_cong_ussen <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$USSEN))))#
    reg_cong_atg <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$ATTGEN))))#
    reg_cong_aud <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$AUDITOR))))#
    reg_cong_trea <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$TREASURER))))#
    reg_cong_comp <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$COMPOSITE))))#
    reg_cong_all <- lm(update(cong.form, #
      as.formula(. ~  . +  #
        default.unc(enacted.elections$PRES) + #
        default.unc(enacted.elections$USSEN) +#
        default.unc(enacted.elections$ATTGEN) +#
        default.unc(enacted.elections$AUDITOR) +#
        default.unc(enacted.elections$TREASURER))#
    ))#
#
congress_predict.caption = "Comparing Pennsylvania Congressional Results with State-wide Elections (2016)"#
congress_predict.label = "tab:tab_congress_predict"#
congress_predict.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001 \\\\ Uncontested (or non-competitive) elections replaced with 0.25 \\& 0.75 vote shares. Regressions are unweighted, ie, all districts are assumed to have identical turnout. This is the usual way political scientist measure aggregate congressional vote \\cite{GelmanKing1994_unifiedAJPS}. \\\\"#
#
tab_congress_predict <- stargazer(#
        reg_cong_pres, #
         reg_cong_ussen, #
         reg_cong_atg, #
         reg_cong_aud, #
         reg_cong_trea, #
         reg_cong_comp,#
         reg_cong_all,#
star.cutoffs = c(0.05,0.01, 0.001),#
style = "apsr", #
header = FALSE,#
model.numbers = FALSE,#
initial.zero = TRUE,#
digits = 2,#
column.sep.width = "0pt",#
multicolumn = TRUE,#
omit.stat = c("ll", "F", "ser"),#
dep.var.labels = "Actual Congressional Results (2016)",#
covariate.labels = c("Incumbency Advantage","Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite"),#
label = congress_predict.label,#
title = congress_predict.caption,#
notes = congress_predict.footnote#
    )#
tab_congress_predict <- tab_congress_predict[c(-6, -26, -28, -29, -30, -31, -32)]#
# #
#
sink()
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)
Table(tab_plan_summary,#
    path = "Tables/_tab_summaries.tex",#
    caption = plan_summary.caption,#
    label = plan_summary.label,#
    footnote = plan_summary.footnote)#
#
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)#
#
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)#
#
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)#
#
Table(district_summary,#
    path = "Tables/_a_tab_districtvotes.tex",#
    caption = district_summary.caption,#
    label = district_summary.label,#
    footnote = district_summary.footnote#
    )#
#
Table(tab_congress_predict,#
    path = "Tables/a_tab_congress_predict.tex",#
    caption = congress_predict.caption,#
    label = congress_predict.label,#
    footnote = congress_predict.footnote)
cat(#
  "\n#
#
    LOADING FUNCTIONS. . . . . . . . . . . . . . . . . . . . . . #
\n")#
`Table` <- function(x, path=NULL, caption="", label="", footnote="", landscape=FALSE, out=NULL)#
  {#
    x <- append(x,#
paste0(#
"\n% =====================================================================#
%  TABLE  TABLE  TABLE  TABLE#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center}\\textbf{INSERT TABLE \\ref{", label, "} ABOUT HERE} \\end{center}#
% \n#
\n% =====================================================================#
%  TABLE  TABLE  TABLE  TABLE#
% ---------------------------------------------------------------------",#
ifelse(landscape==T, "\n\\begin{landscape}", "")),#
 after = 1)#
#
    x <- append(x,#
paste0("\\end{tabular}#
\\tabnotes{", footnote, "}#
\\end{table}",#
ifelse(landscape==T, "\n\\end{landscape}", ""),#
"#
% ---------------------------------------------------------------------#
%  END TABLE  END TABLE  END TABLE #
% ===================================================================== \n \n"),#
after = length(x))#
cat(paste(latex.special.chr(x), collapse = "\n"), "\n")#
cat(paste(latex.special.chr(x), collapse = "\n"), "\n", file = paste0("Latex/", path))#
  }#
#
latex.special.chr <- function(x) {#
  x <- gsub("$\\hat{\\mkern6mu}$", "^", x,fixed=TRUE)#
  x <- gsub("\\textasteriskcentered ", "*", x,fixed=TRUE)#
  x <- gsub("\\textbackslash ", "\\", x,fixed=TRUE)#
  x <- gsub("\\{", "{", x,fixed=TRUE)#
  x <- gsub("\\}", "}", x,fixed=TRUE)#
  x <- gsub("\\$", "$", x,fixed=TRUE)#
}#
#
 `Figure` <- function(path=NULL, caption="", label="", footnote="")#
  {#
    x <- paste0("\n#
% =====================================================================#
%  FIGURE  FIGURE  FIGURE  FI#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center} INSERT FIGURE \\ref{", label, "} ABOUT HERE \\end{center}#
% ---------------------------------------------------------------------#
%  END FIGURE  END FIGURE  END FIGURE #
% ===================================================================== \n",#
#
"\n% =====================================================================#
%  FIGURE  FIGURE  FIGURE  FI#
% ---------------------------------------------------------------------#
% " , caption,#
"% #
% #
\\begin{figure}#
    \\begin{center}#
    \\caption{",caption, "}#
    \\label{", label, "}#
    \\includegraphics[width=1\\textwidth]{", path, "}#
    \\end{center}#
    \\tabnotes{", footnote, "}#
\\end{figure}#
% ---------------------------------------------------------------------#
%  END FIGURE  END FIGURE  END FIGURE #
% =====================================================================\n \n"#
)#
  cat(paste(x, collapse = "\n"), "\n")#
  }#
#
`new.list` <-#
function(len) {#
  out <- list(NA)#
  if (len<1) stop ("Can't make a list of length ",len,".")#
  for (ii in 1:len) out[[ii]] <- NA#
  return(out)#
}#
#
`shift` <- # Seats at c vote#
function(x, w=NULL, c=0.5) {#
    if(is.null(w)) w <- rep(1,length(x))#
  tmp <- mean.w(x, w = w) - c#
    tmp.votes <- x - tmp#
    return(seats(tmp.votes))#
}#
`cube` <- function (x)  x^3/(1-3*x+3*x^2)#
#
seats.display <- function(x) {#
  tmp <- numeric()#
    for (i in 1:dim(x)[2]) {#
      rep <- seats(x[,i]) * length(x[,i])#
      dem <- length(x[,i]) - rep#
      tmp <- rbind(tmp, paste0(rep,"R-", dem, "D"))#
    }#
    return(tmp)#
  }#
#
`two_party` <- #
  function(D, R) {#
    replaceNA(as.numeric(D))/(replaceNA(as.numeric(D))+replaceNA(as.numeric(R)))#
    }#
#
`find.winner` <- #
  function(inp) 0*(inp<0.5)+1*(inp>0.5)#
#
`seats` <- #
  function(inp) mean(inp>.5)#
#
`make.weights` <-  #
  function(x, d = 5) {#
    r(x/sum(x,na.rm=T), d = d)}#
#
`f.num` <- function(x, d=2) format(round(x, d=d), nsmall=d)#
#
`percent` <- #
  function(x, d = 1) {#
    paste0(f.num(x * 100, d= d), "%")#
  }#
#
`replaceNA` <- #
  function (x, value=0) {#
   x[is.na(x)] <- value #
   return(x)}#
#
`mean.w` <- # WEIGHTED MEAN#
function (x, weight=NULL, na.rm = FALSE, ...) {#
    if (is.null(weight)) weight <- rep(1, length(x))#
      w <- as.double(weight)#
    if (na.rm) {#
        i <- !is.na(x)#
        w <- w[i]#
        x <- x[i]#
    }#
    sum((x * w)[w != 0])/sum(w)}#
#
inv <- function(x) {#
  (exp(x) / ( 1 + exp(x)))#
}#
#
sv.curve <- function(s,v) {#
  reg <- lm(log(sv(s)) ~ log(sv(v)))#
  vote <- seq(0.01,0.99, by=.01)#
  seatvotes <-  reg$coefficients[2]*log(vote/(1-vote)) + reg$coefficients[1]#
  return(inv(seatvotes))#
}#
#
sv <- function(x) (x / (1 - x))#
#
`seats.print` <- function(x) paste0(sum(find.winner(x)), "R-", length(x)-sum(find.winner(x)), "D")#
#
`agg.precinct` <- function(data, var, id) {#
        cbind.data.frame(#
          REP = aggregate(as.numeric(data[,paste0(var, "R")]), by=list(id=data[,id]), FUN=sum)[,2], #
          DEM = aggregate(as.numeric(data[,paste0(var, "D")]), by=list(id=data[,id]), FUN=sum)[,2]#
        )#
      }#
#
comp.raw <- function(data) {#
  (two_party(as.numeric(data[,"T16PRESR"]), as.numeric(data[,"T16PRESD"])) +#
  two_party(as.numeric(data[,"T16SENR"]), as.numeric(data[,"T16SEND"])) +#
  two_party(as.numeric(data[,"T16ATGR"]), as.numeric(data[,"T16ATGD"])) +#
  two_party(as.numeric(data[,"T16AUDR"]), as.numeric(data[,"T16AUDD"])) +#
  two_party(as.numeric(data[,"T16TREASR"]), as.numeric(data[,"T16TREASD"]))#
  ) / 5#
  }#
#
composite <- function(data, id) {#
  data <- data[order(data[,id]),]#
  cbind.data.frame(#
        CONG = two_party(aggregate(as.numeric(data[,"T16CONGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16CONGD"]), by=list(id=data[,id]), FUN=sum)[,2]),#
        PRES = two_party(aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2]), #
        USSEN = two_party(aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        ATTGEN = two_party(aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        AUDITOR = two_party(aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        TREASURER = two_party(aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        COMPOSITE = composite.sum(data, id)#
        )#
  }#
#
composite.sum <- function(data, id) {#
  two_party(#
            (#
              aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2]#
            ),#
            (#
              aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]#
            )#
          )#
  }#
#
std <- function(x) {#
  (mean(x[!is.na(x)]) - x) / sd(x[!is.na(x)])#
  }#
#
# ci <- function(x)#
#   {#
#     r <- summary(lm(x~1))#
#       m <- coef(r)[1]#
#       se <- coef(r)[2]#
#       return(qt(0.975, df=length(x)-1) * se)#
#   }#
#
`quick.summary` <- function (x) #
  {#
  `qsum` <- function (set) cbind(mean(set[!is.na(set)]),#
                            sd(set[!is.na(set)]),#
                            var(set[!is.na(set)]),#
                            min(set[!is.na(set)]),#
                            max(set[!is.na(set)]),#
                            sum(1*!is.na(set)),#
                            sum(1*is.na(set)))#
    out <- qsum(x)#
  colnames(out) <- c("Mean","SD","Variance","Min","Max","Valid","Missing")#
    return(out)#
  }#
#
`quick.sum` <- #
  function (x) {#
  `qsum` <- #
    function (set) cbind(#
                      mean(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) - ci(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) + ci(set[!is.na(set)]),#
                      sd(set[!is.na(set)]),#
                      min(set[!is.na(set)]),#
                      quantile(set[!is.na(set)], 0.025),#
                      quantile(set[!is.na(set)], 0.975),                            #
                      max(set[!is.na(set)])#
                      )#
#
    out <- qsum(x)#
  colnames(out) <- c("Mean","CI_lower","CI_upper","SD","Min","2.5%","97.5%","Max")#
    return(out)}#
#
  comb <-combn(c(0:9,LETTERS[1:6]),2)#
  opacity <- c(paste0(comb[1,1:120], comb[2,1:120]), paste0("FF"))#
#
`unc` <- function(x) -1 * (x <= .25) + 1 * (x >= .75)#
#
`delete.unc` <-#
function(vs, uncL, uncU) {#
  #replaces uncontested vote values with "missing".#
 f1 <- function (a,b,c) ifelse (is.na(a),NA,#
                                 ifelse(a<b,NA, ifelse(a>c,NA,a)))#
  return(sapply (vs,f1,uncL,uncU)) }#
#
`replace.unc` <-#
function (vs,l,u,lr,ur, na.rm=T) { # na.rm replaces NAs with 0#
  na <- numeric()#
    if(na.rm==T) #
    {#
      na <- 0#
    } else NA#
  f1 <- function (a,b,c,d,e) if (!is.na(a)) {#
    if (a<b) a <- d else if (a>c) a <- e else a#
  } else NA#
  return(sapply (vs,f1,l,u,lr,ur))}#
`default.unc` <-#
function (vs, uncL = 0.25, uncU = 0.75, uncLR = 0.25, uncUR = 0.75) {#
  vs <- replace.unc(vs, uncL, uncU, uncLR, uncUR)#
  return(vs)}#
#
`mean.unc` <- # uncL and uncU to replace outside bounds, otherwise just NAs#
function(vs, uncL = 0.0001, uncU = 0.99999) {#
  vs <- delete.unc(vs, uncL=uncL, uncU=uncU)#
tmp <- vs[!is.na(vs)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.mean <- truncate(rnorm(length(vs[is.na(vs)]), reg, reg_sig))#
  vs[is.na(vs)] <- new.mean#
  return(vs)}#
`impute.weights` <- # for use when turnout in districts is abnormally high or low (w = # of standard deviations from median)#
function(t1, t2, w = 2) {#
  t <- t1+t2#
  uncL <- (median(t, na.rm=T) -  (w * sd(t, na.rm=T)))#
  uncU <- (median(t, na.rm=T) + (w * sd(t, na.rm=T)))#
  t1 <- delete.unc(t, uncL=uncL, uncU=uncU)#
tmp <- t1[!is.na(t1)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.reg <- r(rnorm(length(t1[is.na(t1)]), reg, reg_sig), d=0)#
  t1[is.na(t1)] <- new.reg#
  return(t1)}#
`truncate` <-#
function(r1) {#
  below <- r1<=0#
  above <- r1>=1#
  between <- !(below|above)#
  (below*0.0001)+(r1*between)+0.9999*above#
}#
sim.election <- function(votes= NULL, center=house.2016.votes, incumbency=NULL, yr=2018, sims=1000, sigma=sigma) {#
  if (is.null(sims)) sims <- 1000#
      equal.vote <- mean(votes) - mean(center)#
      sims.year <- new.list(sims)#
    for (k in 1:sims)#
      {#
    sims.year[[k]] <- #
        rnorm(length(votes), votes - equal.vote, #
          sigma)#
      }#
  return(sims.year)#
  }#
#
r <- function(r, d=2) round(r, digits=d)#
# =================================================================#
# -- GERRYMANDER MEASURES -- GERRYMANDER MEASURES -- GERRYMANDER ME#
# =================================================================#
#  DELINATION #
`declination` <- # Warrington, Gregory S. 2018. Quantifying Gerrymandering Using the Vote Distribution. Election Law Journal 17(1): 3957. www.liebertpub.com (Accessed February 22, 2019).#
  function(votes) {#
    abo = votes[votes > 0.5]   # districts won by party A#
    bel = votes[votes <= 0.5]  # districts won by party B#
  # declination is undefined if one party wins all seats.#
    if (length(bel) == 0 | length(abo) == 0) {#
      return(NaN)#
    }#
  # angle for party B#
    theta = atan((1-(2*mean(bel))) / (length(bel) / length(votes)))#
  # angle for party A#
    gamma = atan((2*mean(abo)-1) / (length(abo) / length(votes)))#
  # normalize from radians to values betwen -1 and 1#
  # A little extra precision just in case :)#
    return(-1 * (2.0*(gamma-theta)/3.1415926535))}#
#
`declin2` <- # Simplified, not transformed from: Katz, Jonathan N. et al. 2018. Theoretical Foundations and Empirical Evaluations of Partisan Fairness in District-Based Democracies *. https://gking.harvard.edu/files/psym_2.pdf (Accessed March 16, 2019).#
  function(vs) {#
    abo = vs[vs > 0.5]   # districts won by party A#
    bel = vs[vs <= 0.5]  # districts won by party B#
    (-1 * (mean(abo) - 0.5) / (sum(find.winner(abo))/length(vs))) - ((0.5 - mean(bel)) / (1 - sum(find.winner(abo))/length(vs)))#
  }#
# #
#  MEAN/MEDIAN#
# #
`meanmedian` <- # Best, Robin E., Shawn J. Donahue, Jonathan Krasno, Daniel B. Magleby, et al. 2018. Considering the Prospects for Establishing a Packing Gerrymandering Standard. Election Law Journal: Rules, Politics, and Policy 17(1): 120. http://www.liebertpub.com/doi/10.1089/elj.2016.0392 (Accessed July 24, 2018).#
  function(votes) median(votes) - mean(votes)#
# #
#  EFFICIENCY GAP#
# #
`eff_gap` <- # Stephanopoulos, Nicholas, and Eric Mcghee. 2014. Partisan Gerrymandering and the Efficiency Gap. University of Chicago Law School 82. http://ssrn.com/abstract=2457468.. (Accessed September 10, 2018).#
  function(D, R) {#
    total <- sum(D)+sum(R)#
    dem_wasted <- sum(ifelse(D>R, D[D>R] - R[D>R], D[D<R]))#
    rep_wasted <- sum(ifelse(D<R, R[D<R] - D[D<R], R[D>R]))#
      return((dem_wasted - rep_wasted) / sum(total))}#
#
eg_TP <- function(votes) #
  {#
    y <- cbind.data.frame(votes, 1-votes)#
      return(-1 * eff_gap(y[1], y[2]))#
  }#
# #
#  GERRY DISPLAY #
# #
  gerry <- function(x, toggle=TRUE)#
    {#
    Seats = paste0(" [", seats.print(x), "]")#
    SeatPER = percent(seats(x))#
    Votes = percent(mean(default.unc(x)))#
    Bias = r(seatsvotes(x)$bias)#
    EfficiencyGap = r(eg_TP(x))#
    MeanMedian = r(meanmedian(x))#
    Declination = r(declination(x))#
    a <- rbind.data.frame(#
      Seats, SeatPER, Votes, Bias, EfficiencyGap, MeanMedian, Declination)#
    rownames(a) <- c("Seats","Seat %","Votes","Bias","Efficiency Gap","Mean/Median","Declination")#
    colnames(a) <- "Summary"#
#
    if (toggle!=TRUE) { # TO RETURN UNFORMATTED MEASUREMENTS#
    SeatsT = NA#
    SeatPERT = r(seats(x))#
    VotesT = r(mean(default.unc(x)))#
    BiasT = r(seatsvotes(x)$bias)#
    EfficiencyGapT = r(eg_TP(x))#
    MeanMedianT = r(meanmedian(x))#
    DeclinationT = r(declination(x))#
      return(rbind.data.frame(#
      SeatsT, SeatPERT, VotesT, BiasT, EfficiencyGapT, MeanMedianT, DeclinationT))}#
    return(a)#
    }#
#
p_value <- function(x){#
  l <- quantile(x[!is.na(x)], 0.025)#
  u <- quantile(x[!is.na(x)], 0.975)#
  t <- mean(x)/sd(x)#
    return(2*pt(-abs(t),df=length(x)-1))#
  }#
#
strs <- function(s) {#
  strs <- #
  ifelse(s <= 0.001, "***", #
    ifelse(0.001 < s & s <= 0.01, "**", #
      ifelse(0.01 < s & s <= 0.05, "*", "")))#
  return(paste0("$^{", strs, "}$"))#
}#
#
nintyfive <- function(x, percent=FALSE) {#
  if (percent==TRUE){ return(paste0("(", percent(r(quantile(x[!is.na(x)], 0.025),d=1)), ", ", percent(r(quantile(x[!is.na(x)], 0.975),d=1)), ")")) }#
  return(paste0("{\\small\\textit{[", r(quantile(x[!is.na(x)], 0.025)), ", ", r(quantile(x[!is.na(x)], 0.975)), "]}}"))#
  }#
#
gtab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
  x.bias <- get(paste0(p, ".bias"))#
  x.eg <- get(paste0(p, ".eg"))#
  x.mm <- get(paste0(p, ".meanmedian"))#
  x.declin <- get(paste0(p, ".declination"))#
    return(c(#
      paste0(r(mean(x.bias)), strs(p_value(x.bias))), #
        nintyfive(x.bias), #
      paste0(r(mean(x.eg)), strs(p_value(x.eg))),#
        nintyfive(x.eg), #
      paste0(r(mean(x.mm)), strs(p_value(x.mm))),#
        nintyfive(x.mm),#
      paste0(r(mean(x.declin)), strs(p_value(x.declin))),#
        nintyfive(x.declin)#
      ))#
    }#
#
ptab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
    return(c(#
      paste0(r(sum(s.tmp*18)/(1000*18)*18, d=1), "R-", r(18-sum(s.tmp*18)/(1000*18)*18, d=1), "D"),#
      paste0("{\\small\\textit{[", r(quantile(s.tmp, 0.025),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.025),d=1)*18, "D, ", r(quantile(s.tmp, 0.975),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.975),d=1)*18, "D]}}"),#
      paste0(median(s.tmp)*18, "R-", 18-median(s.tmp)*18, "D"),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) > 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) < 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) == 0.5) / 1000)#
      ))#
} #
#
 #    #
`circle.new` <- #
  function(xorig, yorig, radius, add, ...){#
    x <- seq(-radius, radius, length.out = 1000)#
  # Euclidian distance to the origin#
  y <- sapply(x, function(z) sqrt(radius^2 - z^2))#
  if(add == TRUE){#
    line(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
          type = "l", add=T, ...)#
   } else {#
   plot(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
        type = "l",  #
    xlab="", #
    ylab="", #
    xaxt="n", #
    yaxt="n", #
    bty="n", ...)#
   }#
}#
#
`circle` <- #
function (x, y, radius, nv = 100, border = NULL, col = NA, lty = 1, #
    lwd = 1) {#
    xylim <- par("usr")#
    plotdim <- par("pin")#
    ymult <- getYmult()#
    angle.inc <- 2 * pi/nv#
    angles <- seq(0, 2 * pi - angle.inc, by = angle.inc)#
    if (length(col) < length(radius)) #
        col <- rep(col, length.out = length(radius))#
    for (circle in 1:length(radius)) {#
        xv <- cos(angles) * radius[circle] + x#
        yv <- sin(angles) * radius[circle] * ymult + y#
        polygon(xv, yv, border = border, col = col[circle], lty = lty, #
            lwd = lwd)#
    }#
    invisible(list(x = xv, y = yv))#
}#
# #
# COMPACTNESS MEASURES#
#
POLSBYPOPPER <- function (x) r(mean((4 * 3.1415926535 * x$area) / (x$perimeter^2)), d = 3)#
REOCK <- function (x) r(mean(x$area/(x$smallestcircle^2 * 3.1415926535)), d = 3)#
poly.math <- function (x) {#
    compactness <- NULL#
  if (class(x)=="character") x <- get(x)#
      shapeFile <- x#
      mapObject <- fortify(shapeFile)#
      mapObject$id <- as.character(as.numeric(mapObject$id) + 1)#
      mapObject <- data.frame(mapObject, shapeFile@data[mapObject$id, ])#
      mapObject$piece <- as.character(mapObject$piece)#
      uniqueCDs <- sort(unique(as.numeric(mapObject$id)))#
      for(id in uniqueCDs)#
        {#
          cdShape <- mapObject[mapObject$id == id, ]#
          cdPoly <- SpatialPolygons(list(Polygons(lapply(split(cdShape[, c("long", "lat")], cdShape$piece), Polygon), ID = "b")))#
          owinObject <- try(as(cdPoly, "owin"))#
          compactness[[id]] <-  data.frame(area=area.owin(owinObject), perimeter=perimeter(owinObject), smallestcircle=boundingradius(owinObject))#
        }#
        x <- do.call("rbind", compactness)#
        cat(#
          "\n REOCK:        ", REOCK(x), "\n",#
          "POLSBY-POPPER:", POLSBYPOPPER(x), "\n \n"#
          )#
        return(x)#
  }
#      #
#      #
#      #
# #
# #
cat(#
    "\n#
#
        CREATING TABLES . . . . . . . . . . . . . . . . . . . . .#
\n")#
sink("/dev/null")#
# races <- c("cong2016", "pres16", "ussen16", "atg16", "aud16", "trea16", "comp2016")#
statewide.contests.2016 <- c("Congress", "Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite")#
# #
# County Spliits#
county_splits <- c(41, 19, 19, 17)#
# COMPACTNESS#
geom_names <- paste0(plans, ".geom")#
#
reock.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        reock.plans <- rbind(reock.plans, REOCK(get(geom_names[i])))#
        }#
polsby.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        polsby.plans <- rbind(polsby.plans, POLSBYPOPPER(get(geom_names[i])))#
        }#
#
tab_plan_sum <- cbind.data.frame(#
    Plan = plan_names,#
    CountySplits = county_splits, #
    Reock = reock.plans, #
    PolsbyPopper = polsby.plans#
    # Pres2016 = c(seats.print(enacted.pres), seats.print(court.pres), seats.print(joint.pres), seats.print(govwolf.pres)),#
    # Comp2016 = c(seats.print(enacted.comp), seats.print(court.comp), seats.print(joint.comp), seats.print(govwolf.comp))#
    )#
tab_plan_sum#
plan_summary.caption = "County Splits and Compactness Scores of the Plans"#
plan_summary.label = "tab:summaries"#
plan_summary.footnote = "County splits include all the pieces in which a county is split, not just the total number of counties that have been split. (The latter number is the one most often reported in both court documents and in the media, but we regard the measure we report as both more precise and more informative.)"#
tab_plan_summary <- #
    stargazer(tab_plan_sum,#
        style = "apsr", #
        header = FALSE,#
        summary = FALSE,#
        model.numbers = FALSE,#
        initial.zero = TRUE,#
        digits = 3,#
        column.sep.width = "0pt",#
        rownames = FALSE,#
        multicolumn = TRUE,#
        label = plan_summary.label,#
        title = plan_summary.caption,#
        covariate.labels = ,#
        notes = plan_summary.footnote #
        )#
tab_plan_summary <- tab_plan_summary[c(-6, -7, -13, -14, -15, -16)]#
# tab_plan_summary <- append(tab_plan_summary, #
#   "  &   &  &  & \\textbf{Projected} & \\textbf{Projected}  \\\\", #
#   after = 5)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  &  \\textbf{County}  &  \\textbf{Polsby}   &   &  \\textbf{2016 using}  &  \\textbf{five state-wide} \\\\",#
#   after = 6)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  &  \\textbf{Presidential Results}  &  \\textbf{elections in 2016} \\\\",#
#   after = 7)#
#
tab_plan_summary <- append(tab_plan_summary,#
    "  &  \\textbf{County}  &  \\textbf{Polsby}   &   \\\\",#
    after = 5)#
tab_plan_summary <- append(tab_plan_summary,#
    "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  \\\\",#
    after = 6)#
# #
# #
# #
# #
# =================================================================#
# Comparing Pennsylvania Congressional Results with State-wide Elections (2016) %#
# =================================================================#
# #
# SUMMARY OF ELECTIONS#
percent(sum(house.rep.2012)/sum(house.2012.turnout))#
percent(sum(house.rep.2014)/sum(house.2014.turnout))#
percent(sum(house.rep.2016)/sum(house.2016.turnout))#
#
congsum.caption = "U.S. House Election Summaries \\\\ \\hspace{2cm}(PA 2012-2016 Enacted Map)"#
congsum.label = "tab:congsum"#
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."#
congsum.tmp <- cbind.data.frame(#
    # PA2004 = gerry(default.unc(house.2004.votes)),#
    # PA2006 = gerry(default.unc(house.2006.votes)),#
    # PA2008 = gerry(default.unc(house.2008.votes)),#
    # PA2010 = gerry(default.unc(house.2010.votes)),#
    PA2012 = gerry(default.unc(house.2012.votes)), #
    PA2014 = gerry(default.unc(house.2014.votes)), #
    PA2016 = gerry(default.unc(house.2016.votes)),#
    # PA2018 = gerry(default.unc(house.2018.votes)),#
    PA2012_2016_AVE = rowMeans(cbind(   #
    gerry(default.unc(house.2012.votes), toggle=F),#
    gerry(default.unc(house.2014.votes), toggle=F),#
    gerry(default.unc(house.2016.votes), toggle=F)))#
  )#
congsum.tmp[2:3,4]  <- percent(congsum.tmp[2:3,4])#
congsum.tmp[4:7,4] <- r(as.numeric(congsum.tmp[4:7,4]))#
colnames(congsum.tmp) <- c(seq(2012,2016,2),"AVE")#
congsum.tmp.tex <- stargazer(congsum.tmp,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    label = congsum.label,#
    title = congsum.caption,#
    notes = congsum.footnote)#
congsum.tmp.tex <- congsum.tmp.tex[c(-6, -16, -17, -18, -19)]#
congsum.tmp.tex[8] <- "Seats &  [13R-5D] &  [13R-5D] &  [13R-5D] & [13R-5D] \\\\ "#
# #
# #
# #
# #
dist_summary <- #
    rbind(#
      cbind.data.frame(#
        District = seq(1,18,1),#
        sapply(composite(pa.redist.dta, "enacted"), percent)),#
        c(#
          "Statewide",#
          sapply(colMeans(composite(pa.redist.dta, "enacted")), percent)#
        )#
    )#
#
district_summary.caption = "2016 District-level Summaries for 2011 Enacted Plan"#
district_summary.label = "tab:districtvotes"#
district_summary.footnote = "Uncontested races and those with only negligible competition will be imputed with 0.25 and 0.75 for the respective winners. All votes are calculated from the Republican perspective of the two-party vote. Composite does NOT include the Congressional elections. The statewide average is the unweighted mean of districts."#
#
district_summary <- stargazer(dist_summary,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    rownames = FALSE,#
    label = district_summary.label,#
    title = district_summary.caption,#
    notes = "REPLACE WITH NOTES" )#
district_summary <- district_summary[c(-6, -28, -29, -30, -31)]#
# #
# #
# #
# #
#
# =================================================================#
# Measures of Gerrymandering for the Eight Considered Plans#
# =================================================================#
gerry.table.gen <- #
    cbind.data.frame(#
        gtab("enacted"),#
        gtab("joint"),#
        gtab("govwolf"),#
        gtab("court")#
        )#
#
    colnames(gerry.table.gen) <- c(plan_names)#
    rownames(gerry.table.gen) <- c(#
        "Partisan Bias", #
        "CI1",#
        "Efficiency Gap",#
        "CI2",#
        "Mean/Median",#
        "CI3",#
        "Declination",#
        "CI4")#
#
gerry.caption = "Measures of Gerrymandering for the Four Considered Plans"#
gerry.label = "tab:gerry"#
gerry.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001. Measures are averages of 1,000 simulations for each map using the 2016 composite. Brackets numbers are the 95\\% range."#
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)#
    tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)#
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]#
# #
# #
#
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )#
    colnames(tab_prop.gen) <- c(plan_names)#
    rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "CI1", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")#
#
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"#
prop.label = "tab:prob"#
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."#
#
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)#
tab_prop.tex <- gsub("CI[1-9]", "", tab_prop.tex)#
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]#
#
# #
# #
#
# #
# #
#
# =================================================================#
# Information about Twelve States with Constitutional Provisions similar to Pennsylvania#
# =================================================================#
# #
# latex <- function(x) cat(paste(x, collapse="\n"), "\n")#
# states_compare.caption = "Information about Twelve States with Constitutional Provisions similar to Pennsylvania"#
# states_compare.label = "tab:states_compare"#
# states_compare.footnote = "Seats and votes are based on the 2016 five-election projection (to deal with the existence of non-contested congressional districts). Percentages are of the of district level results. This difference is why the percentages reported in columns 6 and 8 are not identical. Data from DailyKos, All about Redistricting, and Ballotpedia web sites.States with fewer than 9 districts do not have efficiency gap or median values reported because of the potential unreliability of those calculations given the small number of districts involved."#
#
# tab_states_compare <- #
#   paste("#
#       \\begin{table}#
#       \\caption{", states_compare.caption, "}#
#       \\label{", states_compare.label, "}#
#       \\begin{tabular}{#
#       >{\\centering\\arraybackslash}#
#       M{0.1\\linewidth}|#
#       M{.025\\linewidth}|#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.3\\linewidth}}#
#       \\textbf{State} & \\textbf{\\# CDs} & \\textbf{Unified Control (2011)} & \\textbf{Unified Control (2016)} & \\textbf{Seats (2016)} & \\textbf{Votes (2016)} & \\textbf{Mean District Vote Share (Dem)} & \\textbf{Median District Vote Share (Dem)} & \\textbf{Efficiency Gap} & \\textbf{Who does districting} \\\\#
#        \\hline#
#       Arizona & 9 & $\\surd$ (R) & $\\surd$ (R) & 44.4\\% & 48.1\\% & 50.4\\% & 49.4\\% & 0.08 & Independent commission \\\\#
#       Arkansas & 4 & $\\surd$ (D) & $\\surd$ (R) & 0\\% & 35.7\\% & 35.5\\% &  &  & state legislature \\\\#
#       Delaware & 1 & $\\surd$ (D) & $\\surd$ (D) & 100\\% & 55.9\\% & 56\\% &  &  & NA \\\\#
#       Illinois & 18 & $\\surd$ (D) &  & 61.1\\% & 59.0\\% & 59.6\\% & 59.8\\% & 0.08 & state legislature \\\\#
#       Indiana & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 39.9\\% & 40.1\\% & 35.9\\% & 0.09 & state legislature \\\\#
#       Kentucky & 6 &  & $\\surd$ (R) & 16.7\\% & 34.3\\% & 33.8\\% &  &  & state legislature \\\\#
#       Oklahoma & 5 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 30.7\\% & 30.7\\% &  &  & state legislature \\\\#
#       Oregon & 5 &  & $\\surd$ (D) & 80\\% & 56.2\\% & 56\\% &  &  & state legislature \\\\#
#       South Dakota & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 34.0\\% & 34\\% &  &  & NA \\\\#
#       Tennessee & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 36.4\\% & 37.4\\% & 31.3\\% & 0.03 & state legislature \\\\#
#       Washington & 10 & $\\surd$ (D) & $\\surd$ (D) & 70\\% & 58.8\\% & 56\\% & 52.3\\% & -0.05 & 5-member independent commission \\\\#
#       Wyoming & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 24.3\\% & 24.3\\% &  &  & NA ")#
# #
# #
#
# TWO-PARTY VOTE SHARES, ACTUAL (CONGRESSIONAL)#
# tab_summary_stats <- stargazer(#
#   congress.PA.2008.2018,#
#   title="Republican Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   style = "apsr", #
#   header = FALSE,#
#   column.sep.width = "-5pt",#
#   multicolumn = TRUE,#
#   label = "tab:twoparty_actuals" )#
# tab_summary_stats <- tab_summary_stats[c(-6, -11)]#
#
#  Table(tab_summary_stats,#
#   caption="Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   label="ab:twoparty_actuals",#
#   footnote="Votes are the two-party unweighted average of PA Congressional Districts by year (uncontested elections imputed).")#
#
# #
# AVERAGE VOTE BY DISTRICT#
means_races <- numeric()#
  base.form <- formula(. ~ 1)#
  cong.form <- formula(default.unc(enacted.elections$CONG) ~ + pahouse.2016$INC)#
for (k in colnames(enacted.elections))#
{#
  r.tmp <- composite(pa.redist.dta, "enacted")[,k]#
    reg_races <- summary(lm(update(base.form, as.formula(default.unc(r.tmp) ~ .))))#
    means_races[k] <- coef(reg_races)[1]#
}#
#
  cat(#
    "2016 Average Vote Share by State-level Contest \n"#
    )#
    print(cbind.data.frame(Race = statewide.contests.2016, Mean = percent(means_races)))#
    cat(paste("", collapse = "\n"), "\n")#
# # Regression of Race on Congressional Results#
    reg_cong_pres <- lm(update(cong.form, as.formula(. ~ . + default.unc(enacted.elections$PRES))))#
    reg_cong_ussen <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$USSEN))))#
    reg_cong_atg <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$ATTGEN))))#
    reg_cong_aud <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$AUDITOR))))#
    reg_cong_trea <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$TREASURER))))#
    reg_cong_comp <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$COMPOSITE))))#
    reg_cong_all <- lm(update(cong.form, #
      as.formula(. ~  . +  #
        default.unc(enacted.elections$PRES) + #
        default.unc(enacted.elections$USSEN) +#
        default.unc(enacted.elections$ATTGEN) +#
        default.unc(enacted.elections$AUDITOR) +#
        default.unc(enacted.elections$TREASURER))#
    ))#
#
congress_predict.caption = "Comparing Pennsylvania Congressional Results with State-wide Elections (2016)"#
congress_predict.label = "tab:tab_congress_predict"#
congress_predict.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001 \\\\ Uncontested (or non-competitive) elections replaced with 0.25 \\& 0.75 vote shares. Regressions are unweighted, ie, all districts are assumed to have identical turnout. This is the usual way political scientist measure aggregate congressional vote \\cite{GelmanKing1994_unifiedAJPS}. \\\\"#
#
tab_congress_predict <- stargazer(#
        reg_cong_pres, #
         reg_cong_ussen, #
         reg_cong_atg, #
         reg_cong_aud, #
         reg_cong_trea, #
         reg_cong_comp,#
         reg_cong_all,#
star.cutoffs = c(0.05,0.01, 0.001),#
style = "apsr", #
header = FALSE,#
model.numbers = FALSE,#
initial.zero = TRUE,#
digits = 2,#
column.sep.width = "0pt",#
multicolumn = TRUE,#
omit.stat = c("ll", "F", "ser"),#
dep.var.labels = "Actual Congressional Results (2016)",#
covariate.labels = c("Incumbency Advantage","Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite"),#
label = congress_predict.label,#
title = congress_predict.caption,#
notes = congress_predict.footnote#
    )#
tab_congress_predict <- tab_congress_predict[c(-6, -26, -28, -29, -30, -31, -32)]#
# #
#
sink()#
Table(tab_plan_summary,#
    path = "Tables/_tab_summaries.tex",#
    caption = plan_summary.caption,#
    label = plan_summary.label,#
    footnote = plan_summary.footnote)#
#
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)#
#
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)#
#
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)#
#
Table(district_summary,#
    path = "Tables/_a_tab_districtvotes.tex",#
    caption = district_summary.caption,#
    label = district_summary.label,#
    footnote = district_summary.footnote#
    )#
#
Table(tab_congress_predict,#
    path = "Tables/a_tab_congress_predict.tex",#
    caption = congress_predict.caption,#
    label = congress_predict.label,#
    footnote = congress_predict.footnote)#
cat("#
#
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#
    ")
congsum.caption = "U.S. House Election Summaries \\\\ {\\Large\\hspace{4cm}(PA 2012-2016 Enacted Map)}"
congsum.label = "tab:congsum"
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."
congsum.tmp <- cbind.data.frame(#
    # PA2004 = gerry(default.unc(house.2004.votes)),#
    # PA2006 = gerry(default.unc(house.2006.votes)),#
    # PA2008 = gerry(default.unc(house.2008.votes)),#
    # PA2010 = gerry(default.unc(house.2010.votes)),#
    PA2012 = gerry(default.unc(house.2012.votes)), #
    PA2014 = gerry(default.unc(house.2014.votes)), #
    PA2016 = gerry(default.unc(house.2016.votes)),#
    # PA2018 = gerry(default.unc(house.2018.votes)),#
    PA2012_2016_AVE = rowMeans(cbind(   #
    gerry(default.unc(house.2012.votes), toggle=F),#
    gerry(default.unc(house.2014.votes), toggle=F),#
    gerry(default.unc(house.2016.votes), toggle=F)))#
  )
congsum.tmp[2:3,4]  <- percent(congsum.tmp[2:3,4])
congsum.tmp[4:7,4] <- r(as.numeric(congsum.tmp[4:7,4]))
colnames(congsum.tmp) <- c(seq(2012,2016,2),"AVE")
congsum.tmp.tex <- stargazer(congsum.tmp,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    label = congsum.label,#
    title = congsum.caption,#
    notes = congsum.footnote)
congsum.tmp.tex <- congsum.tmp.tex[c(-6, -16, -17, -18, -19)]
congsum.tmp.tex[8] <- "Seats &  [13R-5D] &  [13R-5D] &  [13R-5D] & [13R-5D] \\\\ "
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)
congsum.caption = "U.S. House Election Summaries \\\\ {\\large\\hspace{4cm}(PA 2012-2016 Enacted Map)}"
congsum.label = "tab:congsum"
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)
