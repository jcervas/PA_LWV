plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
arrows(4.25, ((data2[3,4]-data2[1,4])/2)+data2[1,4], 4.5, ((data2[3,4]-data2[1,4])/2)+data2[1,1], length = 0.2, col='black')#
text(4.5, ((data2[3,4]-data2[1,4])/2)+data2[1,1], labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
arrows(4.25, legend.placement, 4.5, legend.placement, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
arrows(4.25, legend.placement, 4.5, legend.placement*1.2, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            abline(4, legend.placement, 4.5, legend.placement*1.5)#
arrows(4.5, legend.placement, 4.5, legend.placement*1.5, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            abline(a=c(4, legend.placement), b=(4.5, legend.placement*1.5))#
arrows(4.5, legend.placement, 4.5, legend.placement*1.5, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot(c(-2,3), c(-1,5), type = "n", xlab = "x", ylab = "y", asp = 1)#
## the x- and y-axis, and an integer grid#
abline(h = 0, v = 0, col = "gray60")#
text(1,0, "abline( h = 0 )", col = "gray60", adj = c(0, -.1))#
abline(h = -1:5, v = -2:3, col = "lightgray", lty = 3)
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            lines(c(4, legend.placement), c(4.5, legend.placement*1.5))#
arrows(4.5, legend.placement, 4.5, legend.placement*1.5, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            lines(c(4, legend.placement), c(4.5, legend.placement))#
arrows(4.5, legend.placement, 4.5, legend.placement*1.5, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
xy.coords(1:3, 1:2, recycle = TRUE)
plot(xy.coords(1:3, 1:2, recycle = TRUE))
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)
legend.placement <- .06
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)
lines(xy.coords(c(4, 4.5), c(legend.placement, legend.placement)))
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)
lines(xy.coords(c(4.25, 4.5), c(legend.placement, legend.placement)))
arrows(4.25, legend.placement, 4.5, legend.placement*1.5, length = 0.2, col='black')
arrows(4.5, legend.placement, 4.25, legend.placement*1.5, length = 0.2, col='black')
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            lines(xy.coords(c(4.25, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
            lines(xy.coords(c(4.25, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.16, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.5)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.81, 4.25), c(data1[1,4], data1[1,4])))#
arrows(3.81, data1[1,4], 4.25, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*3, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.81, 4), c(data1[1,4], data1[1,4])))#
arrows(3.81, data1[1,4], 4.25, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*3, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.81), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 4, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*3, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.81), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off(
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.81), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.91, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.91), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.97), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.95), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(contactF, contactM, contactF_se, contactM_se, "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
c(contact_main, commwk_main, donate_main, protest_main, social_main, vote_main, votelocal_main)#
c(contact_ylab, commwk_ylab, donate_ylab, protest_ylab, social_ylab, vote_ylab, votelocal_ylab)#
c(contactF, commwkF, donateF, protestF, socialF, voteF, votelocalF)#
c(contactM, commwkM, donateM, protestM, socialM, voteM, votelocalM)#
c(contactF_se, commwkF_se, donateF_se, protestF_se, socialF_se, voteF_se, votelocalF_se)#
c(contactM_se, commwkM_se, donateM_se, protestM_se, socialM_se, voteM_se, votelocalM_se)#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others To Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate To A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm To Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm To Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm To Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm To Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
#
ci.tmp <- CI(contact, contact_se)#
ci2.tmp <- CI(contact2, contact2_se)#
plot.coef.bars(contact, contact2, ci.tmp[1,], ci.tmp[2,], ci2.tmp[1,], ci2.tmp[2,], "F", "M", main=main, xlab=xlab, ylab=ylab)#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==1#
 cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))#
#
coeftest(m, vcov = vcovHC(m, type="HC1"))#
library(lmtest)#
#
library(sandwich)
c(contact_main, commwk_main, donate_main, protest_main, social_main, vote_main, votelocal_main)#
c(contact_ylab, commwk_ylab, donate_ylab, protest_ylab, social_ylab, vote_ylab, votelocal_ylab)#
c(contactF, commwkF, donateF, protestF, socialF, voteF, votelocalF)#
c(contactM, commwkM, donateM, protestM, socialM, voteM, votelocalM)#
c(contactF_se, commwkF_se, donateF_se, protestF_se, socialF_se, voteF_se, votelocalF_se)#
c(contactM_se, commwkM_se, donateM_se, protestM_se, socialM_se, voteM_se, votelocalM_se)#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others To Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate To A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm To Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm To Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm To Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm To Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
#
ci.tmp <- CI(contact, contact_se)#
ci2.tmp <- CI(contact2, contact2_se)#
plot.coef.bars(contact, contact2, ci.tmp[1,], ci.tmp[2,], ci2.tmp[1,], ci2.tmp[2,], "F", "M", main=main, xlab=xlab, ylab=ylab)#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==1#
 cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))
c(contact_main, commwk_main, donate_main, protest_main, social_main, vote_main, votelocal_main)#
c(contact_ylab, commwk_ylab, donate_ylab, protest_ylab, social_ylab, vote_ylab, votelocal_ylab)#
c(contactF, commwkF, donateF, protestF, socialF, voteF, votelocalF)#
c(contactM, commwkM, donateM, protestM, socialM, voteM, votelocalM)#
c(contactF_se, commwkF_se, donateF_se, protestF_se, socialF_se, voteF_se, votelocalF_se)#
c(contactM_se, commwkM_se, donateM_se, protestM_se, socialM_se, voteM_se, votelocalM_se)#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others To Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate To A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm To Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm To Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm To Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm To Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==1#
 cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))
title <- c(contact_main, commwk_main, donate_main, protest_main, social_main, vote_main, votelocal_main)#
ylabel <- c(contact_ylab, commwk_ylab, donate_ylab, protest_ylab, social_ylab, vote_ylab, votelocal_ylab)#
beta1 <- c(contactF, commwkF, donateF, protestF, socialF, voteF, votelocalF)#
beta2 <- c(contactM, commwkM, donateM, protestM, socialM, voteM, votelocalM)#
se1 <- c(contactF_se, commwkF_se, donateF_se, protestF_se, socialF_se, voteF_se, votelocalF_se)#
se2 <- c(contactM_se, commwkM_se, donateM_se, protestM_se, socialM_se, voteM_se, votelocalM_se)#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others To Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate To A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm To Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm To Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm To Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm To Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==1#
 cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))
title <- c(contact_main, commwk_main, donate_main, protest_main, social_main, vote_main, votelocal_main)#
ylabel <- c(contact_ylab, commwk_ylab, donate_ylab, protest_ylab, social_ylab, vote_ylab, votelocal_ylab)#
beta1 <- c(contactF, commwkF, donateF, protestF, socialF, voteF, votelocalF)#
beta2 <- c(contactM, commwkM, donateM, protestM, socialM, voteM, votelocalM)#
se1 <- c(contactF_se, commwkF_se, donateF_se, protestF_se, socialF_se, voteF_se, votelocalF_se)#
se2 <- c(contactM_se, commwkM_se, donateM_se, protestM_se, socialM_se, voteM_se, votelocalM_se)#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others To Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate To A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm To Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm To Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm To Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm To Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
# cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
# citizen <- cmps$citizen==1#
# ethnic_quota <- cmps$ethnic_quota==1#
#  cmps1 <- cmps[citizen & ethnic_quota,]#
#  base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
# summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))
j <- 1
paste("Marginal Effect of Norm to", title[j],"\nOLS Coefficient & 95% Confidence Intervals")
ylab <- ylabel[j]
ylab
main <- paste("Marginal Effect of Norm to", title[j],"\nOLS Coefficient & 95% Confidence Intervals")#
ylab <- ylabel[j]#
xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(beta1[j], beta2[j], beta1[j], beta2[j], "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
beta1[j]
beta2[j]
CI(data1[j], se1[j], level=0.95)
CI(beta1[j], se1[j], level=0.95)
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
main <- paste("Marginal Effect of Norm to", title[j],"\nOLS Coefficient & 95% Confidence Intervals")#
ylab <- ylabel[j]#
xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(beta1[j], beta2[j], se1[j], se2[j], "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
se1[j]
beta1
votelocalF
beta1
beta1 <- c("contactF", "commwkF", "donateF", "protestF", "socialF", "voteF", "votelocalF")
beta1
get(beta1[j])
title <- c("contact_main", "commwk_main", "donate_main", "protest_main", "social_main", "vote_main", "votelocal_main")#
ylabel <- c("contact_ylab", "commwk_ylab", "donate_ylab", "protest_ylab", "social_ylab", "vote_ylab", "votelocal_ylab")#
beta1 <- c("contactF", "commwkF", "donateF", "protestF", "socialF", "voteF", "votelocalF")#
beta2 <- c("contactM", "commwkM", "donateM", "protestM", "socialM", "voteM", "votelocalM")#
se1 <- c("contactF_se", "commwkF_se", "donateF_se", "protestF_se", "socialF_se", "voteF_se", "votelocalF_se")#
se2 <- c("contactM_se", "commwkM_se", "donateM_se", "protestM_se", "socialM_se", "voteM_se", "votelocalM_se")#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm To Contact On Issues"
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*3, length = 0.2, col='black')#
text(4, legend.placement*3, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*2, length = 0.2, col='black')#
text(3.81, legend.placement*2, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
ylab <- get(ylabel[j])#
xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)#
plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
# dev.off()
par(mfrow=c(3,4))
main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")
ylab <- get(ylabel[j])
xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")
# pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 5)
plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)
par(mfrow=c(4,2))
plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 10)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*2, length = 0.2, col='black')#
text(4, legend.placement*2, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.75, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.75, data1[1,4], 3.81, legend.placement*1.5, length = 0.2, col='black')#
text(3.81, legend.placement*1.5, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*2, length = 0.2, col='black')#
text(4, legend.placement*2, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, legend.placement*1.5, length = 0.2, col='black')#
text(3.75, legend.placement*1.5, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.5)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*2, length = 0.2, col='black')#
text(4, legend.placement*2, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, legend.placement*1.5, length = 0.2, col='black')#
text(3.75, legend.placement*1.5, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.3)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*2, length = 0.2, col='black')#
text(4, legend.placement*2, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, legend.placement*1.5, length = 0.2, col='black')#
text(3.75, legend.placement*1.5, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, legend.placement*2, length = 0.2, col='black')#
text(4, legend.placement*2, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, legend.placement*1.5, length = 0.2, col='black')#
text(3.75, legend.placement*1.5, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.4, length = 0.2, col='black')#
text(4,0.4, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
text(4, 0.30, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,4,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
title <- c("contact_main", "commwk_main", "donate_main", "protest_main", "social_main", "vote_main", "votelocal_main")#
ylabel <- c("contact_ylab", "commwk_ylab", "donate_ylab", "protest_ylab", "social_ylab", "vote_ylab", "votelocal_ylab")#
beta1 <- c("contactF", "commwkF", "donateF", "protestF", "socialF", "voteF", "votelocalF")#
beta2 <- c("contactM", "commwkM", "donateM", "protestM", "socialM", "voteM", "votelocalM")#
se1 <- c("contactF_se", "commwkF_se", "donateF_se", "protestF_se", "socialF_se", "voteF_se", "votelocalF_se")#
se2 <- c("contactM_se", "commwkM_se", "donateM_se", "protestM_se", "socialM_se", "voteM_se", "votelocalM_se")#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm \nTo Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working to Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others \nTo Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate \nTo A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm \nTo Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm \nTo Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm \nTo Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm \nTo Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
# cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
# citizen <- cmps$citizen==1#
# ethnic_quota <- cmps$ethnic_quota==1#
#  cmps1 <- cmps[citizen & ethnic_quota,]#
#  base.form <- formula(. ~ age + age65 + faminc + educ + homeowner + unemployed + pctnonUS + ptystn + orgactive + interest + linkfate + recruitact_ldr)#
# summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +.), weight= weight, data=cmps1))
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,5,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                # ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
                ylim <- c(-0.05, 0.4)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), "F", "M", main=main, xlab=xlab, ylab=ylab)#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
    lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.3))#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
    lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
# text(x=as.Date(c("01/01/2010"), "%m/%d/%Y"), y=50, labels="Sum of the Very Favorable and Mostly Favorable categories", cex=0.65)#
                # legend("topright", legend="95% Confidence", text.col="#000000", bty="n", cex=1, text.font=2, adj = c(-0.02,0.0))#
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.3))#
    }#
dev.off()
plot.coef.bars <- #
    function (betasM, betasF, seM, seF, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betasF, seF, level=0.95)#
            data2 <- CI(betasM, seM, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.3))#
    }#
dev.off()
}#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
    plot.coef.bars(get(beta1[j]), get(beta2[j]), get(se1[j]), get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))#
    }#
dev.off()
beta1
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(1, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 16, height = 15)#
par(mfrow=c(4,2))#
for (j in 1:length(title)) {#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx White", "Latinx", "African American", "Asian American")#
#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))#
    }#
dev.off()
s
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 6, height = 4)
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 7, height = 4)
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
j <- 3
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 7, height = 4)
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)
par(mfrow=c(1,1))
xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian n\American")
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)
par(mfrow=c(1,1))
xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(0, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)#
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
#
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)#
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            text(xx, data1[2,] - 0.01, labels=cat1)#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
            text(xx2, data2[2,] - 0.01, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }
pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/contact.pdf"), width = 8, height = 4)
par(mfrow=c(1,1))
plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))
dev.off()
names <- c("contact", "commwk", "donate", "protest", "social", "vote", "votelocal")
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))#
    }#
dev.off()
dev.off()
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=c(-0.05, 0.25))#
        dev.off()#
    }
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
                        text(xx, data1[2,] - 0.015, labels=cat1)#
                        text(xx2, data2[2,] - 0.015, labels=cat2)
}
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
`CI` <- #
    function(x, se, level=0.95) {#
        if (level==0.95) z <- 1.96#
        if (level==0.90) z <- 1.645#
        ci <- rbind(x, x - (z * se), x + (z * se))#
        rownames(ci) <- c("beta", "lower", "upper")#
        return(ci)#
    }#
round.ceiling <- function(x, d) ceiling(x/d)*d#
round.floor <- function(x, d) floor(x/d)*d#
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,6,3,3))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
                        text(xx, data1[2,] - 0.02, labels=cat1)#
                        text(xx2, data2[2,] - 0.02, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
`CI` <- #
    function(x, se, level=0.95) {#
        if (level==0.95) z <- 1.96#
        if (level==0.90) z <- 1.645#
        ci <- rbind(x, x - (z * se), x + (z * se))#
        rownames(ci) <- c("beta", "lower", "upper")#
        return(ci)#
    }#
round.ceiling <- function(x, d) ceiling(x/d)*d#
round.floor <- function(x, d) floor(x/d)*d#
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,5.5,3,3))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
                        text(xx, data1[2,] - 0.02, labels=cat1)#
                        text(xx2, data2[2,] - 0.02, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
`CI` <- #
    function(x, se, level=0.95) {#
        if (level==0.95) z <- 1.96#
        if (level==0.90) z <- 1.645#
        ci <- rbind(x, x - (z * se), x + (z * se))#
        rownames(ci) <- c("beta", "lower", "upper")#
        return(ci)#
    }#
round.ceiling <- function(x, d) ceiling(x/d)*d#
round.floor <- function(x, d) floor(x/d)*d#
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,5.5,3,2))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
                        text(xx, data1[2,] - 0.02, labels=cat1)#
                        text(xx2, data2[2,] - 0.02, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
?png
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), width = 8, height = 4, units=in)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), width = 8, height = 4, units="in")#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), width = 8, height = 4, units=in)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/pdf/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }#
#
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), width = 8, height = 4, units=in)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), width = 8, height = 4, units=in)
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ,".png"), units=in, width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
for (j in 1:length(title)) {#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ".png"), units="px", width=800,height=(600))#
        par(mfrow=c(1,1))#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j] ".png")
for (j in 1:length(title)) {#
    # png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="px", width=800,height=(600))#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units=in, width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png")
png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units=inches, width = 8, height = 4)
png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="in", width = 8, height = 4)
for (j in 1:length(title)) {#
    # png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="px", width=800,height=(600))#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="in", res=500, width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/pdf/", names[j] ,".pdf"), width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }#
#
for (j in 1:length(title)) {#
    # png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="px", width=800,height=(600))#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="in", res=500, width = 8, height = 4)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
summary(m <- lm(update(base.form, contactissue ~ female + normcontactNM + femnormContact +  .), weight= weight, data=cmps1))
coeftest(m, vcov = vcovHC(m, type="HC1"))
library(sandwich)
install.packages('sandwich')
library(sandwich)
coeftest(m, vcov = vcovHC(m, type="HC1"))
install.packages("lmtest")
library(lmtest)
coeftest(m, vcov = vcovHC(m, type="HC1"))
ethnic <- c(1,2,3,4)
length(names)
c("normcontactNM", "norm_commworkNM", "normprotestNM", "normvoteNM", "normvoteNM", "normsummTrad", "normdonateNM")
c("femnormContact", "femnormCommWk", "femnormProt", "femnormVote", "femnormVote", "femnormTrad", "femnormDonate")
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normvoteNM", "normvoteNM", "normsummTrad", "normdonateNM")
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormVote", "femnormVote", "femnormTrad", "femnormDonate")
dv <- c("socialmedia", "contactissue", "comm_work", "protest", "vote", "votelocal")#
#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normvoteNM", "normvoteNM", "normsummTrad", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormVote", "femnormVote", "femnormTrad", "femnormDonate")#
#
out <- list()#
for(i in length(names)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
#
}
i
out
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "socialmedia", "polpartTrad", "donate")
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normsummTrad", "normdonateNM")
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormTrad", "femnormDonate")
normNM
dv
femnorm
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "polpartTrad", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normsummTrad", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormTrad", "femnormDonate")#
#
out <- list()#
for(i in length(names)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
#
}
femnorm[i]
normNM[i]
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormDonate")#
#
out <- list()#
for(i in length(names)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
#
}
normNM[i]
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormDonate")#
#
out <- list()#
for(i in length(names)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
#
}
dv[i]
i
dv
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormDonate")#
#
out <- list()#
for(i in length(dv)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
#
}
dv[i]
normNM[i]
femnorm[i]
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))
cmps1
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormDonate")#
#
out <- list()#
for(j in 1:length(dv)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
for(i in 1:length(ethnic)) {#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
    }#
}
dv[i]
normNM[i]
femnorm[i]
cmps1
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))
title <- c("contact_main", "commwk_main", "donate_main", "protest_main", "social_main", "vote_main", "votelocal_main")#
ylabel <- c("contact_ylab", "commwk_ylab", "donate_ylab", "protest_ylab", "social_ylab", "vote_ylab", "votelocal_ylab")#
beta1 <- c("contactF", "commwkF", "donateF", "protestF", "socialF", "voteF", "votelocalF")#
beta2 <- c("contactM", "commwkM", "donateM", "protestM", "socialM", "voteM", "votelocalM")#
se1 <- c("contactF_se", "commwkF_se", "donateF_se", "protestF_se", "socialF_se", "voteF_se", "votelocalF_se")#
se2 <- c("contactM_se", "commwkM_se", "donateM_se", "protestM_se", "socialM_se", "voteM_se", "votelocalM_se")#
#
names <- c("contact", "commwk", "donate", "protest", "social", "vote", "votelocal")#
#
contact_main <- "Contact on Contacting on an Issue,"#
contact_ylab <- "OLS Coefficient On Norm \nTo Contact On Issues"#
#
contactF <- #
c(0.0860655,#
0.0697297,#
0.0478951,#
0.0454305)#
#
contactF_se <- #
c(0.019,#
0.013,#
0.01,#
0.017)#
#
contactM <- #
c(0.1488321,#
0.0566743,#
0.0497024,#
0.0659856)#
#
contactM_se <- #
c(0.029,#
0.016,#
0.014,#
0.022)#
commwk_main <- "Work on Community Problem on Working \nto Solve a Community Problem,"#
commwk_ylab <- "OLS Coefficient On Norm To Work With Others \nTo Solve Community Problem"#
#
commwkF <- #
c(0.0755397,#
0.0645092,#
0.0431188,#
0.0594256)#
#
commwkF_se <- #
c(0.017,#
0.013,#
0.009,#
0.018)#
#
commwkM <- #
c(0.1290913,#
0.078696,#
0.0828597,#
0.0851571)#
#
commwkM_se <- #
c(0.025,#
0.016,#
0.016,#
0.019)#
donate_main <- "Donate on Donating to a Campaign,"#
donate_ylab <- "OLS Coefficient On Norm To Donate \nTo A Campaign Or Cause"#
#
donateF <- #
c(0.0937791,#
0.0607834,#
0.0614953,#
0.0701727)#
#
donateF_se <- #
c(0.019,#
0.012,#
0.009,#
0.016)#
#
donateM <- #
c(0.1583963,#
0.0870401,#
0.0892198,#
0.0830664)#
#
donateM_se <- #
c(0.024,#
0.018,#
0.017,#
0.018)#
protest_main <- "Protest on Attending a Protest,"#
protest_ylab <- "OLS Coefficient On Norm \nTo Attend Protest"#
#
protestF <- #
c(0.0587314,#
0.073317,#
0.0531946,#
0.0648408)#
#
protestF_se <- #
c(0.013,#
0.013,#
0.008,#
0.013)#
#
protestM <- #
c(0.1069584,#
0.0574463,#
0.0333203,#
0.0430373)#
#
protestM_se <- #
c(0.021,#
0.016,#
0.015,#
0.013)#
social_main <- "Use Social Media for Politics on Posting Politics on Social Media,"#
social_ylab <- "OLS Coefficient On Norm \nTo Use Social Media For Politics"#
#
socialF <- #
c(0.1587346,#
0.1203141,#
0.1219445,#
0.1289472)#
#
socialF_se <- #
c(0.022,#
0.014,#
0.012,#
0.02)#
#
socialM <- #
c(0.1462155,#
0.0897798,#
0.1250902,#
0.1150495)#
#
socialM_se <- #
c(0.029,#
0.02,#
0.017,#
0.02)#
vote_main <- "Vote on Voting in 2016,"#
vote_ylab <- "OLS Coefficient On Norm \nTo Vote"#
#
voteF <- #
c(0.1444461,#
0.1028096,#
0.1239695,#
0.1057274)#
#
voteF_se <- #
c(0.024,#
0.014,#
0.013,#
0.019)#
#
voteM <- #
c(0.1092774,#
0.0920095,#
0.11765,#
0.1502012)#
#
voteM_se <- #
c(0.026,#
0.018,#
0.017,#
0.022)#
votelocal_main <- "Vote on Voting in State and Local Elections,"#
votelocal_ylab <- "OLS Coefficient On Norm \nTo Vote"#
#
votelocalF <- #
c(0.3150406,#
0.131653,#
0.1585753,#
0.2737248)#
#
votelocalF_se <- #
c(0.067,#
0.043,#
0.034,#
0.047)#
#
votelocalM <- #
c(0.2192945,#
0.1622558,#
0.2397637,#
0.2111982)#
#
votelocalM_se <- #
c(0.081,#
0.057,#
0.041,#
0.073)#
dv <- c("contactissue", "comm_work", "protest", "socialmedia", "vote", "votelocal", "donate")#
normNM <- c("normcontactNM", "norm_commworkNM", "normprotestNM", "normsocialmediaNM", "normvoteNM", "normvoteNM", "normdonateNM")#
femnorm <- c("femnormContact", "femnormCommWk", "femnormProt", "femnormSocial", "femnormVote", "femnormVote", "femnormDonate")#
#
out <- list()#
for(j in 1:length(dv)) {#
#
ethnic <- c(1,2,3,4)#
#
cmps <- read.dta("/Users/cervas/Google Drive/Data/CMPS/CMPSmarch27_19.dta", convert.factors = F)#
for(i in 1:length(ethnic)) {#
citizen <- cmps$citizen==1#
ethnic_quota <- cmps$ethnic_quota==ethnic[i]#
cmps1 <- cmps[citizen & ethnic_quota,]#
 base.form <- #
    formula(. ~ #
        age + #
        age65 + #
        faminc + #
        educ + #
        homeowner + #
        unemployed + #
        pctnonUS + #
        ptystn + #
        orgactive + #
        interest + #
        linkfate + #
        recruitact_ldr)#
#
summary(m <- lm(update(base.form, dv[i] ~ female + normNM[i] + femnorm[i] +  .), weight= weight, data=cmps1))#
out[[i]] <- coeftest(m, vcov = vcovHC(m, type="HC1"))#
    }#
}
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/pdf/", names[j] ,".pdf"), width = 8, height = 7)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }#
#
for (j in 1:length(title)) {#
    # png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="px", width=800,height=(600))#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="in", res=500, width = 8, height = 7)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
`CI` <- #
    function(x, se, level=0.95) {#
        if (level==0.95) z <- 1.96#
        if (level==0.90) z <- 1.645#
        ci <- rbind(x, x - (z * se), x + (z * se))#
        rownames(ci) <- c("beta", "lower", "upper")#
        return(ci)#
    }#
round.ceiling <- function(x, d) ceiling(x/d)*d#
round.floor <- function(x, d) floor(x/d)*d#
plot.coef.bars <- #
    function (betas1, betas2, se1, se2, cat1=NULL, cat2=NULL, main=NULL, labels=NULL, xlab=NULL, ylab=NULL, ylim=NULL) {#
            data1 <- CI(betas1, se1, level=0.95)#
            data2 <- CI(betas2, se2, level=0.95)#
            legend.placement <- ((data2[3,4]-data2[1,4])/2)+data2[1,4]#
            xx <- seq(1,length(xlab),1)#
            xx2 <- xx + 0.15#
            yy <- data1[1,]#
            yy2 <- data2[1,]#
#
                values <- c(data1, data2)#
                if(is.null(ylim)) ylim <- c(min(round.floor(values, 0.05))-0.05, max(round.ceiling(values, 0.05))+0.05)#
        par(mar=c(3,5.5,4,2))#
            plot(x = xx, y = yy, xlim = c(0.5, length(xlab)+0.5), ylim = ylim, main = main, type = "n", yaxt = "n", xaxt = "n", xlab = "", ylab = "", bty = "n")#
            axis(side=2, las=2, labels= seq(ylim[1], ylim[2],0.05), at=seq(ylim[1], ylim[2],0.05), cex.axis=1, font=2)#
            axis(side=1, at = seq(1.12,4.12,1), labels = xlab, cex.axis=1, lwd=0, lwd.ticks=0, font=2)#
            mtext(side=2, line= 3, ylab, cex=1)#
            abline(h=seq(ylim[1], ylim[2], 0.05), lty=2, lwd=0.5, col="gray80")#
            abline(h=0, lwd=1.5, lty=2, col="gray20")#
            arrows(y0=data1[2,], x0=xx, y1=data1[3,], x1=xx, code=3, angle = 90, length=0.1, lwd=1.5)#
            points(xx, data1[1,], cex=1.5, pch=19)#
            # points(xx, yy, cex=3, pch=ifelse(donate_F_low>0, ifelse(donate_F_high<0, 1, 19), 1))#
            # points(xx, yy, pch=ifelse(donate_low<0, ifelse(donate_high>0, 1, 19), 1))#
#
            arrows(y0=data2[2,], x0=xx2, y1=data2[3,], x1=xx2, code=3, angle=90, length=0.1, lty=1, col="gray40", lwd=1.5)#
            points(xx2, data2[1,], cex=1.5, col="gray40", pch=19)#
            # points(xx2, yy2, cex=3, col="gray40", pch=ifelse(donate_M_low>0, ifelse(donate_M_high<0, 1, 19), 1))#
                        text(xx, data1[2,] - 0.02, labels=cat1)#
                        text(xx2, data2[2,] - 0.02, labels=cat2)#
#
#   lines(xy.coords(c(4.18, 4.5), c(legend.placement, legend.placement)))#
# arrows(4.5, legend.placement, 4.25, 0.39, length = 0.2, col='black')#
# text(4, 0.39, labels="95% Confidence Interval", col='black', pos=3, cex=0.85)#
#
#   lines(xy.coords(c(3.7, 3.94), c(data1[1,4], data1[1,4])))#
# arrows(3.7, data1[1,4], 3.75, 0.35, length = 0.2, col='black')#
# text(3.75, 0.35, labels="Point Estimate", col='black', pos=3, cex=0.85)#
        }#
for (j in 1:length(title)) {#
    pdf(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/pdf/", names[j] ,".pdf"), width = 8, height = 7)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }#
#
for (j in 1:length(title)) {#
    # png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="px", width=800,height=(600))#
    png(paste0("/Users/cervas/Google Drive/Projects/Uhlaner/March2019_MPSA/Figures/png/", names[j], ".png"), units="in", res=500, width = 8, height = 7)#
        par(mfrow=c(1,1))#
#
    main <- paste("Marginal Effect of Norm to", get(title[j]),"\nOLS Coefficient & 95% Confidence Intervals")#
    ylab <- get(ylabel[j])#
    xlab <- c("Non-Latinx \nWhite", "Latinx", "African \nAmerican", "Asian \nAmerican")#
    ylim <- c(-0.05, 0.25)#
    if(j==7) ylim <- c(-0.05,0.5)#
    plot.coef.bars(betas1=get(beta1[j]), betas2=get(beta2[j]), se1=get(se1[j]), se2=get(se2[j]), cat1="F", cat2="M", main=main, xlab=xlab, ylab=ylab, ylim=ylim)#
        dev.off()#
    }
source("R/Tables.R")
###########################################################################################################
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ──╔╦═══╦═╗─╔╦═══╦════╦╗─╔╦═══╦═╗─╔╗#
# ──║║╔═╗║║╚╗║║╔═╗║╔╗╔╗║║─║║╔═╗║║╚╗║║#
# ──║║║─║║╔╗╚╝║║─║╠╝║║╚╣╚═╝║║─║║╔╗╚╝║#
# ╔╗║║║─║║║╚╗║║╚═╝║─║║─║╔═╗║╚═╝║║╚╗║║#
# ║╚╝║╚═╝║║─║║║╔═╗║─║║─║║─║║╔═╗║║─║║║#
# ╚══╩═══╩╝─╚═╩╝─╚╝─╚╝─╚╝─╚╩╝─╚╩╝─╚═╝#
#         ╔═══╦═══╦═══╦╗──╔╦═══╦═══╗#
#         ║╔═╗║╔══╣╔═╗║╚╗╔╝║╔═╗║╔═╗║#
#         ║║─╚╣╚══╣╚═╝╠╗║║╔╣║─║║╚══╗#
#         ║║─╔╣╔══╣╔╗╔╝║╚╝║║╚═╝╠══╗║#
#         ║╚═╝║╚══╣║║╚╗╚╗╔╝║╔═╗║╚═╝║#
#         ╚═══╩═══╩╝╚═╝─╚╝─╚╝─╚╩═══╝#
### Code to Replicate "Tools for Identifying Partisan Gerrymandering"#
# 🅙🅞🅝🅐🅣🅗🅐🅝 🅡. 🅒🅔🅡🅥🅐🅢, University of California Irvine#
# 🅑🅔🅡🅝🅐🅡🅓 🅖🅡🅞🅕🅜🅐🅝, University of California Irvine#
### Note: #
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
###########################################################################################################
    rm(list=ls(all=TRUE))   # Remove all objects just to be safe.#
    options(scipen=999)     # Turn off Scientific Notation#
    options(stringsAsFactors = FALSE)#
    doInstall <- F#
setwd("/Users/cervas/Google Drive/Papers/Tools for Identifying a Partisan Gerrymander/PA_LWV")  # Main directory#
source("R/license.R")    #
seed <- 66#
set.seed(seed)#
              # Change to FALSE if you don't want packages installed.#
  projection <- "+proj=lcc +lat_1=40.96666666666667 +lat_2=39.93333333333333 +lat_0=39.33333333333334 +lon_0=-77.75 +x_0=600000 +y_0=0 +ellps=GRS80 +datum=NAD83 +to_meter=0.3048006096012192 +no_defs"#
# https://spatialreference.org/ref/epsg/nad83-pennsylvania-south-ftus/#
  projection <- "+init=epsg:4269"#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
  plan_names <- #
    c(#
      "2011 Enacted",#
      "Joint Legislative",#
      "Gov. Wolf",#
      "2018 Court Remedial")#
  plans <- #
    c("enacted2011", #
        "joint", #
        "govwolf",#
        "court")#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# =================================================================#
# -- FUNCTIONS -- -- FUNCTIONS -- -- FUNCTIONS -- -- FUNCTIONS  -- #
# =================================================================#
    source("R/GERRYfunctions.R")#
    source("/Users/cervas/Google Drive/School/UCI/R Functions/seatsvotes.R")#
# =================================================================#
# -- DATA -- -- DATA -- -- DATA -- -- DATA  -- -- DATA  -- -- DATA #
# =================================================================#
    source("R/PA_Congressional_Data.R")#
    pa.redist.dta <- read.csv("./_data/pa_redist_shp.csv")#
    source("R/DataSetup.R") #
# ================================================================= ##
# -- TOOLS FOR IDENTIFYING PARTISAN  GERRYMANDERING -- ANALYSIS -- -##
# ================================================================= #        #
    source("R/Simulations.R")#
    source("R/GIS.R")#
    source("R/Tables.R")#
    source("R/Plots.R")
latex.special.chr <- function(x) {#
  x <- gsub("$\\hat{\\mkern6mu}$", "^", x,fixed=TRUE)#
  x <- gsub("\\textasteriskcentered ", "*", x,fixed=TRUE)#
  x <- gsub("\\textbackslash ", "\\", x,fixed=TRUE)#
  x <- gsub("\\{", "{", x,fixed=TRUE)#
  x <- gsub("\\}", "}", x,fixed=TRUE)#
  x <- gsub("\\$", "$", x,fixed=TRUE)#
}
cat(#
  "\n#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
    LOADING FUNCTIONS. . . . . . . . . . . . . . . . . . . . . . #
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n")#
`Table` <- function(x, path=NULL, caption="", label="", footnote="", landscape=FALSE, out=NULL)#
  {#
    x <- append(x,#
paste0(#
"\n% =====================================================================#
% ▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center}\\textbf{INSERT TABLE \\ref{", label, "} ABOUT HERE} \\end{center}#
% •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n#
\n% =====================================================================#
% ▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟#
% ---------------------------------------------------------------------",#
ifelse(landscape==T, "\n\\begin{landscape}", "")),#
 after = 1)#
#
    x <- append(x,#
paste0("\\end{tabular}#
\\tabnotes{", footnote, "}#
\\end{table}",#
ifelse(landscape==T, "\n\\end{landscape}", ""),#
"#
% ---------------------------------------------------------------------#
% ▀▄▀▄▀▄ E͎N͎D͎ T͎A͎B͎L͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ T͎A͎B͎L͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ T͎A͎B͎L͎E͎ ▄▀▄▀▄▀#
% ===================================================================== \n \n"),#
after = length(x))#
# cat(paste(x, collapse = "\n"), "\n")#
cat(paste(latex.special.chr(x), collapse = "\n"), "\n", file = paste0("Latex/", path))#
  }#
#
latex.special.chr <- function(x) {#
  x <- gsub("$\\hat{\\mkern6mu}$", "^", x,fixed=TRUE)#
  x <- gsub("\\textasteriskcentered ", "*", x,fixed=TRUE)#
  x <- gsub("\\textbackslash ", "\\", x,fixed=TRUE)#
  x <- gsub("\\{", "{", x,fixed=TRUE)#
  x <- gsub("\\}", "}", x,fixed=TRUE)#
  x <- gsub("\\$", "$", x,fixed=TRUE)#
}#
#
 `Figure` <- function(path=NULL, caption="", label="", footnote="")#
  {#
    x <- paste0("\n#
% =====================================================================#
% ▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center} INSERT FIGURE \\ref{", label, "} ABOUT HERE \\end{center}#
% ---------------------------------------------------------------------#
% ▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄#
% ===================================================================== \n",#
#
"\n% =====================================================================#
% ▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟#
% ---------------------------------------------------------------------#
% " , caption,#
"% •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
% #
\\begin{figure}#
    \\begin{center}#
    \\caption{",caption, "}#
    \\label{", label, "}#
    \\includegraphics[width=1\\textwidth]{", path, "}#
    \\end{center}#
    \\tabnotes{", footnote, "}#
\\end{figure}#
% ---------------------------------------------------------------------#
% ▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄#
% =====================================================================\n \n"#
)#
  cat(paste(x, collapse = "\n"), "\n")#
  }#
#
`new.list` <-#
function(len) {#
  out <- list(NA)#
  if (len<1) stop ("Can't make a list of length ",len,".")#
  for (ii in 1:len) out[[ii]] <- NA#
  return(out)#
}#
#
`shift` <- # Seats at c vote#
function(x, w=NULL, c=0.5) {#
    if(is.null(w)) w <- rep(1,length(x))#
  tmp <- mean.w(x, w = w) - c#
    tmp.votes <- x - tmp#
    return(seats(tmp.votes))#
}#
`cube` <- function (x)  x^3/(1-3*x+3*x^2)#
#
seats.display <- function(x) {#
  tmp <- numeric()#
    for (i in 1:dim(x)[2]) {#
      rep <- seats(x[,i]) * length(x[,i])#
      dem <- length(x[,i]) - rep#
      tmp <- rbind(tmp, paste0(rep,"R-", dem, "D"))#
    }#
    return(tmp)#
  }#
#
`two_party` <- #
  function(D, R) {#
    replaceNA(as.numeric(D))/(replaceNA(as.numeric(D))+replaceNA(as.numeric(R)))#
    }#
#
`find.winner` <- #
  function(inp) 0*(inp<0.5)+1*(inp>0.5)#
#
`seats` <- #
  function(inp) mean(inp>.5)#
#
`make.weights` <-  #
  function(x, d = 5) {#
    r(x/sum(x,na.rm=T), d = d)}#
#
`f.num` <- function(x, d=2) format(round(x, d=d), nsmall=d)#
#
`percent` <- #
  function(x, d = 1) {#
    paste0(f.num(x * 100, d= d), "%")#
  }#
#
`replaceNA` <- #
  function (x, value=0) {#
   x[is.na(x)] <- value #
   return(x)}#
#
`mean.w` <- # WEIGHTED MEAN#
function (x, weight=NULL, na.rm = FALSE, ...) {#
    if (is.null(weight)) weight <- rep(1, length(x))#
      w <- as.double(weight)#
    if (na.rm) {#
        i <- !is.na(x)#
        w <- w[i]#
        x <- x[i]#
    }#
    sum((x * w)[w != 0])/sum(w)}#
#
inv <- function(x) {#
  (exp(x) / ( 1 + exp(x)))#
}#
#
sv.curve <- function(s,v) {#
  reg <- lm(log(sv(s)) ~ log(sv(v)))#
  vote <- seq(0.01,0.99, by=.01)#
  seatvotes <-  reg$coefficients[2]*log(vote/(1-vote)) + reg$coefficients[1]#
  return(inv(seatvotes))#
}#
#
sv <- function(x) (x / (1 - x))#
#
`seats.print` <- function(x) paste0(sum(find.winner(x)), "R-", length(x)-sum(find.winner(x)), "D")#
#
`agg.precinct` <- function(data, var, id) {#
        cbind.data.frame(#
          REP = aggregate(as.numeric(data[,paste0(var, "R")]), by=list(id=data[,id]), FUN=sum)[,2], #
          DEM = aggregate(as.numeric(data[,paste0(var, "D")]), by=list(id=data[,id]), FUN=sum)[,2]#
        )#
      }#
#
comp.raw <- function(data) {#
  (two_party(as.numeric(data[,"T16PRESR"]), as.numeric(data[,"T16PRESD"])) +#
  two_party(as.numeric(data[,"T16SENR"]), as.numeric(data[,"T16SEND"])) +#
  two_party(as.numeric(data[,"T16ATGR"]), as.numeric(data[,"T16ATGD"])) +#
  two_party(as.numeric(data[,"T16AUDR"]), as.numeric(data[,"T16AUDD"])) +#
  two_party(as.numeric(data[,"T16TREASR"]), as.numeric(data[,"T16TREASD"]))#
  ) / 5#
  }#
#
composite <- function(data, id) {#
  data <- data[order(data[,id]),]#
  cbind.data.frame(#
        CONG = two_party(aggregate(as.numeric(data[,"T16CONGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16CONGD"]), by=list(id=data[,id]), FUN=sum)[,2]),#
        PRES = two_party(aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2]), #
        USSEN = two_party(aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        ATTGEN = two_party(aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        AUDITOR = two_party(aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        TREASURER = two_party(aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        COMPOSITE = composite.sum(data, id)#
        )#
  }#
#
composite.sum <- function(data, id) {#
  two_party(#
            (#
              aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2]#
            ),#
            (#
              aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]#
            )#
          )#
  }#
#
std <- function(x) {#
  (mean(x[!is.na(x)]) - x) / sd(x[!is.na(x)])#
  }#
#
# ci <- function(x)#
#   {#
#     r <- summary(lm(x~1))#
#       m <- coef(r)[1]#
#       se <- coef(r)[2]#
#       return(qt(0.975, df=length(x)-1) * se)#
#   }#
#
`quick.summary` <- function (x) #
  {#
  `qsum` <- function (set) cbind(mean(set[!is.na(set)]),#
                            sd(set[!is.na(set)]),#
                            var(set[!is.na(set)]),#
                            min(set[!is.na(set)]),#
                            max(set[!is.na(set)]),#
                            sum(1*!is.na(set)),#
                            sum(1*is.na(set)))#
    out <- qsum(x)#
  colnames(out) <- c("Mean","SD","Variance","Min","Max","Valid","Missing")#
    return(out)#
  }#
#
`quick.sum` <- #
  function (x) {#
  `qsum` <- #
    function (set) cbind(#
                      mean(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) - ci(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) + ci(set[!is.na(set)]),#
                      sd(set[!is.na(set)]),#
                      min(set[!is.na(set)]),#
                      quantile(set[!is.na(set)], 0.025),#
                      quantile(set[!is.na(set)], 0.975),                            #
                      max(set[!is.na(set)])#
                      )#
#
    out <- qsum(x)#
  colnames(out) <- c("Mean","CI_lower","CI_upper","SD","Min","2.5%","97.5%","Max")#
    return(out)}#
#
  comb <-combn(c(0:9,LETTERS[1:6]),2)#
  opacity <- c(paste0(comb[1,1:120], comb[2,1:120]), paste0("FF"))#
#
`unc` <- function(x) -1 * (x <= .25) + 1 * (x >= .75)#
#
`delete.unc` <-#
function(vs, uncL, uncU) {#
  #replaces uncontested vote values with "missing".#
 f1 <- function (a,b,c) ifelse (is.na(a),NA,#
                                 ifelse(a<b,NA, ifelse(a>c,NA,a)))#
  return(sapply (vs,f1,uncL,uncU)) }#
#
`replace.unc` <-#
function (vs,l,u,lr,ur, na.rm=T) { # na.rm replaces NAs with 0#
  na <- numeric()#
    if(na.rm==T) #
    {#
      na <- 0#
    } else NA#
  f1 <- function (a,b,c,d,e) if (!is.na(a)) {#
    if (a<b) a <- d else if (a>c) a <- e else a#
  } else NA#
  return(sapply (vs,f1,l,u,lr,ur))}#
`default.unc` <-#
function (vs, uncL = 0.25, uncU = 0.75, uncLR = 0.25, uncUR = 0.75) {#
  vs <- replace.unc(vs, uncL, uncU, uncLR, uncUR)#
  return(vs)}#
#
`mean.unc` <- # uncL and uncU to replace outside bounds, otherwise just NAs#
function(vs, uncL = 0.0001, uncU = 0.99999) {#
  vs <- delete.unc(vs, uncL=uncL, uncU=uncU)#
tmp <- vs[!is.na(vs)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.mean <- truncate(rnorm(length(vs[is.na(vs)]), reg, reg_sig))#
  vs[is.na(vs)] <- new.mean#
  return(vs)}#
`impute.weights` <- # for use when turnout in districts is abnormally high or low (w = # of standard deviations from median)#
function(t1, t2, w = 2) {#
  t <- t1+t2#
  uncL <- (median(t, na.rm=T) -  (w * sd(t, na.rm=T)))#
  uncU <- (median(t, na.rm=T) + (w * sd(t, na.rm=T)))#
  t1 <- delete.unc(t, uncL=uncL, uncU=uncU)#
tmp <- t1[!is.na(t1)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.reg <- r(rnorm(length(t1[is.na(t1)]), reg, reg_sig), d=0)#
  t1[is.na(t1)] <- new.reg#
  return(t1)}#
`truncate` <-#
function(r1) {#
  below <- r1<=0#
  above <- r1>=1#
  between <- !(below|above)#
  (below*0.0001)+(r1*between)+0.9999*above#
}#
sim.election <- function(votes= NULL, center=house.2016.votes, incumbency=NULL, yr=2018, sims=1000, sigma=sigma) {#
  if (is.null(sims)) sims <- 1000#
      equal.vote <- mean(votes) - mean(center)#
      sims.year <- new.list(sims)#
    for (k in 1:sims)#
      {#
    sims.year[[k]] <- #
        rnorm(length(votes), votes - equal.vote, #
          sigma)#
      }#
  return(sims.year)#
  }#
#
r <- function(r, d=2) round(r, digits=d)#
# =================================================================#
# -- GERRYMANDER MEASURES -- GERRYMANDER MEASURES -- GERRYMANDER ME#
# =================================================================#
# ••• DELINATION ••••••••••••••••••••••••••••••••••••••••••••••••••#
`declination` <- # Warrington, Gregory S. 2018. “Quantifying Gerrymandering Using the Vote Distribution.” Election Law Journal 17(1): 39–57. www.liebertpub.com (Accessed February 22, 2019).#
  function(votes) {#
    abo = votes[votes > 0.5]   # districts won by party A#
    bel = votes[votes <= 0.5]  # districts won by party B#
  # declination is undefined if one party wins all seats.#
    if (length(bel) == 0 | length(abo) == 0) {#
      return(NaN)#
    }#
  # angle for party B#
    theta = atan((1-(2*mean(bel))) / (length(bel) / length(votes)))#
  # angle for party A#
    gamma = atan((2*mean(abo)-1) / (length(abo) / length(votes)))#
  # normalize from radians to values betwen -1 and 1#
  # A little extra precision just in case :)#
    return(-1 * (2.0*(gamma-theta)/3.1415926535))}#
#
`declin2` <- # Simplified, not transformed from: Katz, Jonathan N. et al. 2018. Theoretical Foundations and Empirical Evaluations of Partisan Fairness in District-Based Democracies *. https://gking.harvard.edu/files/psym_2.pdf (Accessed March 16, 2019).#
  function(vs) {#
    abo = vs[vs > 0.5]   # districts won by party A#
    bel = vs[vs <= 0.5]  # districts won by party B#
    (-1 * (mean(abo) - 0.5) / (sum(find.winner(abo))/length(vs))) - ((0.5 - mean(bel)) / (1 - sum(find.winner(abo))/length(vs)))#
  }#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••• MEAN/MEDIAN••••••••••••••••••••••••••••••••••••••••••••••••••#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
`meanmedian` <- # Best, Robin E., Shawn J. Donahue, Jonathan Krasno, Daniel B. Magleby, et al. 2018. “Considering the Prospects for Establishing a Packing Gerrymandering Standard.” Election Law Journal: Rules, Politics, and Policy 17(1): 1–20. http://www.liebertpub.com/doi/10.1089/elj.2016.0392 (Accessed July 24, 2018).#
  function(votes) median(votes) - mean(votes)#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# •••• EFFICIENCY GAP••••••••••••••••••••••••••••••••••••••••••••••#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
`eff_gap` <- # Stephanopoulos, Nicholas, and Eric Mcghee. 2014. “Partisan Gerrymandering and the Efficiency Gap.” University of Chicago Law School 82. http://ssrn.com/abstract=2457468.. (Accessed September 10, 2018).#
  function(D, R) {#
    total <- sum(D)+sum(R)#
    dem_wasted <- sum(ifelse(D>R, D[D>R] - R[D>R], D[D<R]))#
    rep_wasted <- sum(ifelse(D<R, R[D<R] - D[D<R], R[D>R]))#
      return((dem_wasted - rep_wasted) / sum(total))}#
#
eg_TP <- function(votes) #
  {#
    y <- cbind.data.frame(votes, 1-votes)#
      return(-1 * eff_gap(y[1], y[2]))#
  }#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# •••• GERRY DISPLAY ••••••••••••••••••••••••••••••••••••••••••••••#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
  gerry <- function(x, toggle=TRUE)#
    {#
    Seats = paste0(" [", seats.print(x), "]")#
    SeatPER = percent(seats(x))#
    Votes = percent(mean(default.unc(x)))#
    Bias = r(seatsvotes(x)$bias)#
    EfficiencyGap = r(eg_TP(x))#
    MeanMedian = r(meanmedian(x))#
    Declination = r(declination(x))#
    a <- rbind.data.frame(#
      Seats, SeatPER, Votes, Bias, EfficiencyGap, MeanMedian, Declination)#
    rownames(a) <- c("Seats","Seat %","Votes","Bias","Efficiency Gap","Mean/Median","Declination")#
    colnames(a) <- "Summary"#
#
    if (toggle!=TRUE) { # TO RETURN UNFORMATTED MEASUREMENTS#
    SeatsT = NA#
    SeatPERT = r(seats(x))#
    VotesT = r(mean(default.unc(x)))#
    BiasT = r(seatsvotes(x)$bias)#
    EfficiencyGapT = r(eg_TP(x))#
    MeanMedianT = r(meanmedian(x))#
    DeclinationT = r(declination(x))#
      return(rbind.data.frame(#
      SeatsT, SeatPERT, VotesT, BiasT, EfficiencyGapT, MeanMedianT, DeclinationT))}#
    return(a)#
    }#
#
p_value <- function(x){#
  l <- quantile(x[!is.na(x)], 0.025)#
  u <- quantile(x[!is.na(x)], 0.975)#
  t <- mean(x)/sd(x)#
    return(2*pt(-abs(t),df=length(x)-1))#
  }#
#
strs <- function(s) {#
  strs <- #
  ifelse(s <= 0.001, "***", #
    ifelse(0.001 < s & s <= 0.01, "**", #
      ifelse(0.01 < s & s <= 0.05, "*", "")))#
  return(paste0("$^{", strs, "}$"))#
}#
#
nintyfive <- function(x, percent=FALSE) {#
  if (percent==TRUE){ return(paste0("(", percent(r(quantile(x[!is.na(x)], 0.025),d=1)), ", ", percent(r(quantile(x[!is.na(x)], 0.975),d=1)), ")")) }#
  return(paste0("\\textit{[", r(quantile(x[!is.na(x)], 0.025)), ", ", r(quantile(x[!is.na(x)], 0.975)), "]}"))#
  }#
#
gtab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
  x.bias <- get(paste0(p, ".bias"))#
  x.eg <- get(paste0(p, ".eg"))#
  x.mm <- get(paste0(p, ".meanmedian"))#
  x.declin <- get(paste0(p, ".declination"))#
    return(c(#
      paste0(r(mean(x.bias)), strs(p_value(x.bias))), #
        nintyfive(x.bias), #
      paste0(r(mean(x.eg)), strs(p_value(x.eg))),#
        nintyfive(x.eg), #
      paste0(r(mean(x.mm)), strs(p_value(x.mm))),#
        nintyfive(x.mm),#
      paste0(r(mean(x.declin)), strs(p_value(x.declin))),#
        nintyfive(x.declin)#
      ))#
    }#
#
ptab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
    return(c(#
      paste0(r(sum(s.tmp*18)/(1000*18)*18), "R-", r(18-sum(s.tmp*18)/(1000*18)*18), "D"),#
      paste0("\\textit{[", r(quantile(s.tmp, 0.025),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.025),d=1)*18, "D, ", r(quantile(s.tmp, 0.975),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.975),d=1)*18, "D]}"),#
      paste0(median(s.tmp)*18, "R-", 18-median(s.tmp)*18, "D"),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) > 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) < 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) == 0.5) / 1000)#
      ))#
} #
#
 # •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••   #
`circle.new` <- #
  function(xorig, yorig, radius, add, ...){#
    x <- seq(-radius, radius, length.out = 1000)#
  # Euclidian distance to the origin#
  y <- sapply(x, function(z) sqrt(radius^2 - z^2))#
  if(add == TRUE){#
    line(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
          type = "l", add=T, ...)#
   } else {#
   plot(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
        type = "l",  #
    xlab="", #
    ylab="", #
    xaxt="n", #
    yaxt="n", #
    bty="n", ...)#
   }#
}#
#
`circle` <- #
function (x, y, radius, nv = 100, border = NULL, col = NA, lty = 1, #
    lwd = 1) {#
    xylim <- par("usr")#
    plotdim <- par("pin")#
    ymult <- getYmult()#
    angle.inc <- 2 * pi/nv#
    angles <- seq(0, 2 * pi - angle.inc, by = angle.inc)#
    if (length(col) < length(radius)) #
        col <- rep(col, length.out = length(radius))#
    for (circle in 1:length(radius)) {#
        xv <- cos(angles) * radius[circle] + x#
        yv <- sin(angles) * radius[circle] * ymult + y#
        polygon(xv, yv, border = border, col = col[circle], lty = lty, #
            lwd = lwd)#
    }#
    invisible(list(x = xv, y = yv))#
}#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# COMPACTNESS MEASURES#
#
POLSBYPOPPER <- function (x) r(mean((4 * 3.1415926535 * x$area) / (x$perimeter^2)), d = 3)#
REOCK <- function (x) r(mean(x$area/(x$smallestcircle^2 * 3.1415926535)), d = 3)#
poly.math <- function (x) {#
    compactness <- NULL#
  if (class(x)=="character") x <- get(x)#
      shapeFile <- x#
      mapObject <- fortify(shapeFile)#
      mapObject$id <- as.character(as.numeric(mapObject$id) + 1)#
      mapObject <- data.frame(mapObject, shapeFile@data[mapObject$id, ])#
      mapObject$piece <- as.character(mapObject$piece)#
      uniqueCDs <- sort(unique(as.numeric(mapObject$id)))#
      for(id in uniqueCDs)#
        {#
          cdShape <- mapObject[mapObject$id == id, ]#
          cdPoly <- SpatialPolygons(list(Polygons(lapply(split(cdShape[, c("long", "lat")], cdShape$piece), Polygon), ID = "b")))#
          owinObject <- try(as(cdPoly, "owin"))#
          compactness[[id]] <-  data.frame(area=area.owin(owinObject), perimeter=perimeter(owinObject), smallestcircle=boundingradius(owinObject))#
        }#
        x <- do.call("rbind", compactness)#
        cat(#
          "\n REOCK:        ", REOCK(x), "\n",#
          "POLSBY-POPPER:", POLSBYPOPPER(x), "\n \n"#
          )#
        return(x)#
  }
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)#
    tab_gerry.tex <- gsub("$\\hat{\\mkern6mu}$", "^", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textasteriskcentered ", "*", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textbackslash ", "\\", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\{", "{", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\}", "}", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\$", "$", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)#
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)
cat(#
  "\n#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
    LOADING FUNCTIONS. . . . . . . . . . . . . . . . . . . . . . #
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n")#
`Table` <- function(x, path=NULL, caption="", label="", footnote="", landscape=FALSE, out=NULL)#
  {#
    x <- append(x,#
paste0(#
"\n% =====================================================================#
% ▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center}\\textbf{INSERT TABLE \\ref{", label, "} ABOUT HERE} \\end{center}#
% •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n#
\n% =====================================================================#
% ▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟#
% ---------------------------------------------------------------------",#
ifelse(landscape==T, "\n\\begin{landscape}", "")),#
 after = 1)#
#
    x <- append(x,#
paste0("\\end{tabular}#
\\tabnotes{", footnote, "}#
\\end{table}",#
ifelse(landscape==T, "\n\\end{landscape}", ""),#
"#
% ---------------------------------------------------------------------#
% ▀▄▀▄▀▄ E͎N͎D͎ T͎A͎B͎L͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ T͎A͎B͎L͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ T͎A͎B͎L͎E͎ ▄▀▄▀▄▀#
% ===================================================================== \n \n"),#
after = length(x))#
cat(paste(latex.special.chr(x), collapse = "\n"), "\n")#
cat(paste(latex.special.chr(x), collapse = "\n"), "\n", file = paste0("Latex/", path))#
  }#
#
latex.special.chr <- function(x) {#
  x <- gsub("$\\hat{\\mkern6mu}$", "^", x,fixed=TRUE)#
  x <- gsub("\\textasteriskcentered ", "*", x,fixed=TRUE)#
  x <- gsub("\\textbackslash ", "\\", x,fixed=TRUE)#
  x <- gsub("\\{", "{", x,fixed=TRUE)#
  x <- gsub("\\}", "}", x,fixed=TRUE)#
  x <- gsub("\\$", "$", x,fixed=TRUE)#
}#
#
 `Figure` <- function(path=NULL, caption="", label="", footnote="")#
  {#
    x <- paste0("\n#
% =====================================================================#
% ▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center} INSERT FIGURE \\ref{", label, "} ABOUT HERE \\end{center}#
% ---------------------------------------------------------------------#
% ▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄#
% ===================================================================== \n",#
#
"\n% =====================================================================#
% ▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟#
% ---------------------------------------------------------------------#
% " , caption,#
"% •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
% #
\\begin{figure}#
    \\begin{center}#
    \\caption{",caption, "}#
    \\label{", label, "}#
    \\includegraphics[width=1\\textwidth]{", path, "}#
    \\end{center}#
    \\tabnotes{", footnote, "}#
\\end{figure}#
% ---------------------------------------------------------------------#
% ▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄#
% =====================================================================\n \n"#
)#
  cat(paste(x, collapse = "\n"), "\n")#
  }#
#
`new.list` <-#
function(len) {#
  out <- list(NA)#
  if (len<1) stop ("Can't make a list of length ",len,".")#
  for (ii in 1:len) out[[ii]] <- NA#
  return(out)#
}#
#
`shift` <- # Seats at c vote#
function(x, w=NULL, c=0.5) {#
    if(is.null(w)) w <- rep(1,length(x))#
  tmp <- mean.w(x, w = w) - c#
    tmp.votes <- x - tmp#
    return(seats(tmp.votes))#
}#
`cube` <- function (x)  x^3/(1-3*x+3*x^2)#
#
seats.display <- function(x) {#
  tmp <- numeric()#
    for (i in 1:dim(x)[2]) {#
      rep <- seats(x[,i]) * length(x[,i])#
      dem <- length(x[,i]) - rep#
      tmp <- rbind(tmp, paste0(rep,"R-", dem, "D"))#
    }#
    return(tmp)#
  }#
#
`two_party` <- #
  function(D, R) {#
    replaceNA(as.numeric(D))/(replaceNA(as.numeric(D))+replaceNA(as.numeric(R)))#
    }#
#
`find.winner` <- #
  function(inp) 0*(inp<0.5)+1*(inp>0.5)#
#
`seats` <- #
  function(inp) mean(inp>.5)#
#
`make.weights` <-  #
  function(x, d = 5) {#
    r(x/sum(x,na.rm=T), d = d)}#
#
`f.num` <- function(x, d=2) format(round(x, d=d), nsmall=d)#
#
`percent` <- #
  function(x, d = 1) {#
    paste0(f.num(x * 100, d= d), "%")#
  }#
#
`replaceNA` <- #
  function (x, value=0) {#
   x[is.na(x)] <- value #
   return(x)}#
#
`mean.w` <- # WEIGHTED MEAN#
function (x, weight=NULL, na.rm = FALSE, ...) {#
    if (is.null(weight)) weight <- rep(1, length(x))#
      w <- as.double(weight)#
    if (na.rm) {#
        i <- !is.na(x)#
        w <- w[i]#
        x <- x[i]#
    }#
    sum((x * w)[w != 0])/sum(w)}#
#
inv <- function(x) {#
  (exp(x) / ( 1 + exp(x)))#
}#
#
sv.curve <- function(s,v) {#
  reg <- lm(log(sv(s)) ~ log(sv(v)))#
  vote <- seq(0.01,0.99, by=.01)#
  seatvotes <-  reg$coefficients[2]*log(vote/(1-vote)) + reg$coefficients[1]#
  return(inv(seatvotes))#
}#
#
sv <- function(x) (x / (1 - x))#
#
`seats.print` <- function(x) paste0(sum(find.winner(x)), "R-", length(x)-sum(find.winner(x)), "D")#
#
`agg.precinct` <- function(data, var, id) {#
        cbind.data.frame(#
          REP = aggregate(as.numeric(data[,paste0(var, "R")]), by=list(id=data[,id]), FUN=sum)[,2], #
          DEM = aggregate(as.numeric(data[,paste0(var, "D")]), by=list(id=data[,id]), FUN=sum)[,2]#
        )#
      }#
#
comp.raw <- function(data) {#
  (two_party(as.numeric(data[,"T16PRESR"]), as.numeric(data[,"T16PRESD"])) +#
  two_party(as.numeric(data[,"T16SENR"]), as.numeric(data[,"T16SEND"])) +#
  two_party(as.numeric(data[,"T16ATGR"]), as.numeric(data[,"T16ATGD"])) +#
  two_party(as.numeric(data[,"T16AUDR"]), as.numeric(data[,"T16AUDD"])) +#
  two_party(as.numeric(data[,"T16TREASR"]), as.numeric(data[,"T16TREASD"]))#
  ) / 5#
  }#
#
composite <- function(data, id) {#
  data <- data[order(data[,id]),]#
  cbind.data.frame(#
        CONG = two_party(aggregate(as.numeric(data[,"T16CONGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16CONGD"]), by=list(id=data[,id]), FUN=sum)[,2]),#
        PRES = two_party(aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2]), #
        USSEN = two_party(aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        ATTGEN = two_party(aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        AUDITOR = two_party(aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        TREASURER = two_party(aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        COMPOSITE = composite.sum(data, id)#
        )#
  }#
#
composite.sum <- function(data, id) {#
  two_party(#
            (#
              aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2]#
            ),#
            (#
              aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]#
            )#
          )#
  }#
#
std <- function(x) {#
  (mean(x[!is.na(x)]) - x) / sd(x[!is.na(x)])#
  }#
#
# ci <- function(x)#
#   {#
#     r <- summary(lm(x~1))#
#       m <- coef(r)[1]#
#       se <- coef(r)[2]#
#       return(qt(0.975, df=length(x)-1) * se)#
#   }#
#
`quick.summary` <- function (x) #
  {#
  `qsum` <- function (set) cbind(mean(set[!is.na(set)]),#
                            sd(set[!is.na(set)]),#
                            var(set[!is.na(set)]),#
                            min(set[!is.na(set)]),#
                            max(set[!is.na(set)]),#
                            sum(1*!is.na(set)),#
                            sum(1*is.na(set)))#
    out <- qsum(x)#
  colnames(out) <- c("Mean","SD","Variance","Min","Max","Valid","Missing")#
    return(out)#
  }#
#
`quick.sum` <- #
  function (x) {#
  `qsum` <- #
    function (set) cbind(#
                      mean(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) - ci(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) + ci(set[!is.na(set)]),#
                      sd(set[!is.na(set)]),#
                      min(set[!is.na(set)]),#
                      quantile(set[!is.na(set)], 0.025),#
                      quantile(set[!is.na(set)], 0.975),                            #
                      max(set[!is.na(set)])#
                      )#
#
    out <- qsum(x)#
  colnames(out) <- c("Mean","CI_lower","CI_upper","SD","Min","2.5%","97.5%","Max")#
    return(out)}#
#
  comb <-combn(c(0:9,LETTERS[1:6]),2)#
  opacity <- c(paste0(comb[1,1:120], comb[2,1:120]), paste0("FF"))#
#
`unc` <- function(x) -1 * (x <= .25) + 1 * (x >= .75)#
#
`delete.unc` <-#
function(vs, uncL, uncU) {#
  #replaces uncontested vote values with "missing".#
 f1 <- function (a,b,c) ifelse (is.na(a),NA,#
                                 ifelse(a<b,NA, ifelse(a>c,NA,a)))#
  return(sapply (vs,f1,uncL,uncU)) }#
#
`replace.unc` <-#
function (vs,l,u,lr,ur, na.rm=T) { # na.rm replaces NAs with 0#
  na <- numeric()#
    if(na.rm==T) #
    {#
      na <- 0#
    } else NA#
  f1 <- function (a,b,c,d,e) if (!is.na(a)) {#
    if (a<b) a <- d else if (a>c) a <- e else a#
  } else NA#
  return(sapply (vs,f1,l,u,lr,ur))}#
`default.unc` <-#
function (vs, uncL = 0.25, uncU = 0.75, uncLR = 0.25, uncUR = 0.75) {#
  vs <- replace.unc(vs, uncL, uncU, uncLR, uncUR)#
  return(vs)}#
#
`mean.unc` <- # uncL and uncU to replace outside bounds, otherwise just NAs#
function(vs, uncL = 0.0001, uncU = 0.99999) {#
  vs <- delete.unc(vs, uncL=uncL, uncU=uncU)#
tmp <- vs[!is.na(vs)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.mean <- truncate(rnorm(length(vs[is.na(vs)]), reg, reg_sig))#
  vs[is.na(vs)] <- new.mean#
  return(vs)}#
`impute.weights` <- # for use when turnout in districts is abnormally high or low (w = # of standard deviations from median)#
function(t1, t2, w = 2) {#
  t <- t1+t2#
  uncL <- (median(t, na.rm=T) -  (w * sd(t, na.rm=T)))#
  uncU <- (median(t, na.rm=T) + (w * sd(t, na.rm=T)))#
  t1 <- delete.unc(t, uncL=uncL, uncU=uncU)#
tmp <- t1[!is.na(t1)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.reg <- r(rnorm(length(t1[is.na(t1)]), reg, reg_sig), d=0)#
  t1[is.na(t1)] <- new.reg#
  return(t1)}#
`truncate` <-#
function(r1) {#
  below <- r1<=0#
  above <- r1>=1#
  between <- !(below|above)#
  (below*0.0001)+(r1*between)+0.9999*above#
}#
sim.election <- function(votes= NULL, center=house.2016.votes, incumbency=NULL, yr=2018, sims=1000, sigma=sigma) {#
  if (is.null(sims)) sims <- 1000#
      equal.vote <- mean(votes) - mean(center)#
      sims.year <- new.list(sims)#
    for (k in 1:sims)#
      {#
    sims.year[[k]] <- #
        rnorm(length(votes), votes - equal.vote, #
          sigma)#
      }#
  return(sims.year)#
  }#
#
r <- function(r, d=2) round(r, digits=d)#
# =================================================================#
# -- GERRYMANDER MEASURES -- GERRYMANDER MEASURES -- GERRYMANDER ME#
# =================================================================#
# ••• DELINATION ••••••••••••••••••••••••••••••••••••••••••••••••••#
`declination` <- # Warrington, Gregory S. 2018. “Quantifying Gerrymandering Using the Vote Distribution.” Election Law Journal 17(1): 39–57. www.liebertpub.com (Accessed February 22, 2019).#
  function(votes) {#
    abo = votes[votes > 0.5]   # districts won by party A#
    bel = votes[votes <= 0.5]  # districts won by party B#
  # declination is undefined if one party wins all seats.#
    if (length(bel) == 0 | length(abo) == 0) {#
      return(NaN)#
    }#
  # angle for party B#
    theta = atan((1-(2*mean(bel))) / (length(bel) / length(votes)))#
  # angle for party A#
    gamma = atan((2*mean(abo)-1) / (length(abo) / length(votes)))#
  # normalize from radians to values betwen -1 and 1#
  # A little extra precision just in case :)#
    return(-1 * (2.0*(gamma-theta)/3.1415926535))}#
#
`declin2` <- # Simplified, not transformed from: Katz, Jonathan N. et al. 2018. Theoretical Foundations and Empirical Evaluations of Partisan Fairness in District-Based Democracies *. https://gking.harvard.edu/files/psym_2.pdf (Accessed March 16, 2019).#
  function(vs) {#
    abo = vs[vs > 0.5]   # districts won by party A#
    bel = vs[vs <= 0.5]  # districts won by party B#
    (-1 * (mean(abo) - 0.5) / (sum(find.winner(abo))/length(vs))) - ((0.5 - mean(bel)) / (1 - sum(find.winner(abo))/length(vs)))#
  }#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••• MEAN/MEDIAN••••••••••••••••••••••••••••••••••••••••••••••••••#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
`meanmedian` <- # Best, Robin E., Shawn J. Donahue, Jonathan Krasno, Daniel B. Magleby, et al. 2018. “Considering the Prospects for Establishing a Packing Gerrymandering Standard.” Election Law Journal: Rules, Politics, and Policy 17(1): 1–20. http://www.liebertpub.com/doi/10.1089/elj.2016.0392 (Accessed July 24, 2018).#
  function(votes) median(votes) - mean(votes)#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# •••• EFFICIENCY GAP••••••••••••••••••••••••••••••••••••••••••••••#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
`eff_gap` <- # Stephanopoulos, Nicholas, and Eric Mcghee. 2014. “Partisan Gerrymandering and the Efficiency Gap.” University of Chicago Law School 82. http://ssrn.com/abstract=2457468.. (Accessed September 10, 2018).#
  function(D, R) {#
    total <- sum(D)+sum(R)#
    dem_wasted <- sum(ifelse(D>R, D[D>R] - R[D>R], D[D<R]))#
    rep_wasted <- sum(ifelse(D<R, R[D<R] - D[D<R], R[D>R]))#
      return((dem_wasted - rep_wasted) / sum(total))}#
#
eg_TP <- function(votes) #
  {#
    y <- cbind.data.frame(votes, 1-votes)#
      return(-1 * eff_gap(y[1], y[2]))#
  }#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# •••• GERRY DISPLAY ••••••••••••••••••••••••••••••••••••••••••••••#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
  gerry <- function(x, toggle=TRUE)#
    {#
    Seats = paste0(" [", seats.print(x), "]")#
    SeatPER = percent(seats(x))#
    Votes = percent(mean(default.unc(x)))#
    Bias = r(seatsvotes(x)$bias)#
    EfficiencyGap = r(eg_TP(x))#
    MeanMedian = r(meanmedian(x))#
    Declination = r(declination(x))#
    a <- rbind.data.frame(#
      Seats, SeatPER, Votes, Bias, EfficiencyGap, MeanMedian, Declination)#
    rownames(a) <- c("Seats","Seat %","Votes","Bias","Efficiency Gap","Mean/Median","Declination")#
    colnames(a) <- "Summary"#
#
    if (toggle!=TRUE) { # TO RETURN UNFORMATTED MEASUREMENTS#
    SeatsT = NA#
    SeatPERT = r(seats(x))#
    VotesT = r(mean(default.unc(x)))#
    BiasT = r(seatsvotes(x)$bias)#
    EfficiencyGapT = r(eg_TP(x))#
    MeanMedianT = r(meanmedian(x))#
    DeclinationT = r(declination(x))#
      return(rbind.data.frame(#
      SeatsT, SeatPERT, VotesT, BiasT, EfficiencyGapT, MeanMedianT, DeclinationT))}#
    return(a)#
    }#
#
p_value <- function(x){#
  l <- quantile(x[!is.na(x)], 0.025)#
  u <- quantile(x[!is.na(x)], 0.975)#
  t <- mean(x)/sd(x)#
    return(2*pt(-abs(t),df=length(x)-1))#
  }#
#
strs <- function(s) {#
  strs <- #
  ifelse(s <= 0.001, "***", #
    ifelse(0.001 < s & s <= 0.01, "**", #
      ifelse(0.01 < s & s <= 0.05, "*", "")))#
  return(paste0("$^{", strs, "}$"))#
}#
#
nintyfive <- function(x, percent=FALSE) {#
  if (percent==TRUE){ return(paste0("(", percent(r(quantile(x[!is.na(x)], 0.025),d=1)), ", ", percent(r(quantile(x[!is.na(x)], 0.975),d=1)), ")")) }#
  return(paste0("\\textit{[", r(quantile(x[!is.na(x)], 0.025)), ", ", r(quantile(x[!is.na(x)], 0.975)), "]}"))#
  }#
#
gtab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
  x.bias <- get(paste0(p, ".bias"))#
  x.eg <- get(paste0(p, ".eg"))#
  x.mm <- get(paste0(p, ".meanmedian"))#
  x.declin <- get(paste0(p, ".declination"))#
    return(c(#
      paste0(r(mean(x.bias)), strs(p_value(x.bias))), #
        nintyfive(x.bias), #
      paste0(r(mean(x.eg)), strs(p_value(x.eg))),#
        nintyfive(x.eg), #
      paste0(r(mean(x.mm)), strs(p_value(x.mm))),#
        nintyfive(x.mm),#
      paste0(r(mean(x.declin)), strs(p_value(x.declin))),#
        nintyfive(x.declin)#
      ))#
    }#
#
ptab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
    return(c(#
      paste0(r(sum(s.tmp*18)/(1000*18)*18), "R-", r(18-sum(s.tmp*18)/(1000*18)*18), "D"),#
      paste0("\\textit{[", r(quantile(s.tmp, 0.025),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.025),d=1)*18, "D, ", r(quantile(s.tmp, 0.975),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.975),d=1)*18, "D]}"),#
      paste0(median(s.tmp)*18, "R-", 18-median(s.tmp)*18, "D"),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) > 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) < 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) == 0.5) / 1000)#
      ))#
} #
#
 # •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••   #
`circle.new` <- #
  function(xorig, yorig, radius, add, ...){#
    x <- seq(-radius, radius, length.out = 1000)#
  # Euclidian distance to the origin#
  y <- sapply(x, function(z) sqrt(radius^2 - z^2))#
  if(add == TRUE){#
    line(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
          type = "l", add=T, ...)#
   } else {#
   plot(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
        type = "l",  #
    xlab="", #
    ylab="", #
    xaxt="n", #
    yaxt="n", #
    bty="n", ...)#
   }#
}#
#
`circle` <- #
function (x, y, radius, nv = 100, border = NULL, col = NA, lty = 1, #
    lwd = 1) {#
    xylim <- par("usr")#
    plotdim <- par("pin")#
    ymult <- getYmult()#
    angle.inc <- 2 * pi/nv#
    angles <- seq(0, 2 * pi - angle.inc, by = angle.inc)#
    if (length(col) < length(radius)) #
        col <- rep(col, length.out = length(radius))#
    for (circle in 1:length(radius)) {#
        xv <- cos(angles) * radius[circle] + x#
        yv <- sin(angles) * radius[circle] * ymult + y#
        polygon(xv, yv, border = border, col = col[circle], lty = lty, #
            lwd = lwd)#
    }#
    invisible(list(x = xv, y = yv))#
}#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# COMPACTNESS MEASURES#
#
POLSBYPOPPER <- function (x) r(mean((4 * 3.1415926535 * x$area) / (x$perimeter^2)), d = 3)#
REOCK <- function (x) r(mean(x$area/(x$smallestcircle^2 * 3.1415926535)), d = 3)#
poly.math <- function (x) {#
    compactness <- NULL#
  if (class(x)=="character") x <- get(x)#
      shapeFile <- x#
      mapObject <- fortify(shapeFile)#
      mapObject$id <- as.character(as.numeric(mapObject$id) + 1)#
      mapObject <- data.frame(mapObject, shapeFile@data[mapObject$id, ])#
      mapObject$piece <- as.character(mapObject$piece)#
      uniqueCDs <- sort(unique(as.numeric(mapObject$id)))#
      for(id in uniqueCDs)#
        {#
          cdShape <- mapObject[mapObject$id == id, ]#
          cdPoly <- SpatialPolygons(list(Polygons(lapply(split(cdShape[, c("long", "lat")], cdShape$piece), Polygon), ID = "b")))#
          owinObject <- try(as(cdPoly, "owin"))#
          compactness[[id]] <-  data.frame(area=area.owin(owinObject), perimeter=perimeter(owinObject), smallestcircle=boundingradius(owinObject))#
        }#
        x <- do.call("rbind", compactness)#
        cat(#
          "\n REOCK:        ", REOCK(x), "\n",#
          "POLSBY-POPPER:", POLSBYPOPPER(x), "\n \n"#
          )#
        return(x)#
  }
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)
ptab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
    return(c(#
      paste0(r(sum(s.tmp*18)/(1000*18)*18, d=1), "R-", r(18-sum(s.tmp*18)/(1000*18)*18, d=1), "D"),#
      paste0("\\textit{[", r(quantile(s.tmp, 0.025),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.025),d=1)*18, "D, ", r(quantile(s.tmp, 0.975),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.975),d=1)*18, "D]}"),#
      paste0(median(s.tmp)*18, "R-", 18-median(s.tmp)*18, "D"),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) > 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) < 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) == 0.5) / 1000)#
      ))#
}
# ▀▀█▀▀ █▀▀█ █▀▀▄ █░░ █▀▀ █▀▀#
# ░░█░░ █▄▄█ █▀▀▄ █░░ █▀▀ ▀▀█#
# ░░▀░░ ▀░░▀ ▀▀▀░ ▀▀▀ ▀▀▀ ▀▀▀#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
cat(#
    "\n#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
        CREATING TABLES . . . . . . . . . . . . . . . . . . . . .#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n")#
sink("/dev/null")#
# races <- c("cong2016", "pres16", "ussen16", "atg16", "aud16", "trea16", "comp2016")#
statewide.contests.2016 <- c("Congress", "Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite")#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# County Spliits#
county_splits <- c(41, 19, 19, 17)#
# COMPACTNESS#
geom_names <- paste0(plans, ".geom")#
#
reock.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        reock.plans <- rbind(reock.plans, REOCK(get(geom_names[i])))#
        }#
polsby.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        polsby.plans <- rbind(polsby.plans, POLSBYPOPPER(get(geom_names[i])))#
        }#
#
tab_plan_sum <- cbind.data.frame(#
    Plan = plan_names,#
    CountySplits = county_splits, #
    Reock = reock.plans, #
    PolsbyPopper = polsby.plans#
    # Pres2016 = c(seats.print(enacted.pres), seats.print(court.pres), seats.print(joint.pres), seats.print(govwolf.pres)),#
    # Comp2016 = c(seats.print(enacted.comp), seats.print(court.comp), seats.print(joint.comp), seats.print(govwolf.comp))#
    )#
tab_plan_sum#
plan_summary.caption = "County Splits and Compactness Scores of the Plans"#
plan_summary.label = "tab:summaries"#
plan_summary.footnote = "County splits include all the pieces in which a county is split, not just the total number of counties that have been split. (The latter number is the one most often reported in both court documents and in the media, but we regard the measure we report as both more precise and more informative.)"#
tab_plan_summary <- #
    stargazer(tab_plan_sum,#
        style = "apsr", #
        header = FALSE,#
        summary = FALSE,#
        model.numbers = FALSE,#
        initial.zero = TRUE,#
        digits = 3,#
        column.sep.width = "0pt",#
        rownames = FALSE,#
        multicolumn = TRUE,#
        label = plan_summary.label,#
        title = plan_summary.caption,#
        covariate.labels = ,#
        notes = plan_summary.footnote #
        )#
tab_plan_summary <- tab_plan_summary[c(-6, -7, -13, -14, -15, -16)]#
# tab_plan_summary <- append(tab_plan_summary, #
#   "  &   &  &  & \\textbf{Projected} & \\textbf{Projected}  \\\\", #
#   after = 5)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  &  \\textbf{County}  &  \\textbf{Polsby}   &   &  \\textbf{2016 using}  &  \\textbf{five state-wide} \\\\",#
#   after = 6)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  &  \\textbf{Presidential Results}  &  \\textbf{elections in 2016} \\\\",#
#   after = 7)#
#
tab_plan_summary <- append(tab_plan_summary,#
    "  &  \\textbf{County}  &  \\textbf{Polsby}   &   \\\\",#
    after = 5)#
tab_plan_summary <- append(tab_plan_summary,#
    "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  \\\\",#
    after = 6)#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# =================================================================#
# Comparing Pennsylvania Congressional Results with State-wide Elections (2016) %#
# =================================================================#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# SUMMARY OF ELECTIONS#
percent(sum(house.rep.2012)/sum(house.2012.turnout))#
percent(sum(house.rep.2014)/sum(house.2014.turnout))#
percent(sum(house.rep.2016)/sum(house.2016.turnout))#
#
congsum.caption = "U.S. House Election Summaries \\\\ \\hspace{2cm}(PA 2012-2016 Enacted Map)"#
congsum.label = "tab:congsum"#
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."#
congsum.tmp <- cbind.data.frame(#
    # PA2004 = gerry(default.unc(house.2004.votes)),#
    # PA2006 = gerry(default.unc(house.2006.votes)),#
    # PA2008 = gerry(default.unc(house.2008.votes)),#
    # PA2010 = gerry(default.unc(house.2010.votes)),#
    PA2012 = gerry(default.unc(house.2012.votes)), #
    PA2014 = gerry(default.unc(house.2014.votes)), #
    PA2016 = gerry(default.unc(house.2016.votes)),#
    # PA2018 = gerry(default.unc(house.2018.votes)),#
    PA2012_2016_AVE = rowMeans(cbind(   #
    gerry(default.unc(house.2012.votes), toggle=F),#
    gerry(default.unc(house.2014.votes), toggle=F),#
    gerry(default.unc(house.2016.votes), toggle=F)))#
  )#
congsum.tmp[2:3,4]  <- percent(congsum.tmp[2:3,4])#
congsum.tmp[4:7,4] <- r(as.numeric(congsum.tmp[4:7,4]))#
colnames(congsum.tmp) <- c(seq(2012,2016,2),"AVE")#
congsum.tmp.tex <- stargazer(congsum.tmp,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    label = congsum.label,#
    title = congsum.caption,#
    notes = congsum.footnote)#
congsum.tmp.tex <- congsum.tmp.tex[c(-6, -16, -17, -18, -19)]#
congsum.tmp.tex[8] <- "Seats &  [13R-5D] &  [13R-5D] &  [13R-5D] & [13R-5D] \\\\ "#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
dist_summary <- #
    rbind(#
      cbind.data.frame(#
        District = seq(1,18,1),#
        sapply(composite(pa.redist.dta, "enacted"), percent)),#
        c(#
          "Statewide",#
          sapply(colMeans(composite(pa.redist.dta, "enacted")), percent)#
        )#
    )#
#
district_summary.caption = "2016 District-level Summaries for 2011 Enacted Plan"#
district_summary.label = "tab:districtvotes"#
district_summary.footnote = "Uncontested races and those with only negligible competition will be imputed with 0.25 and 0.75 for the respective winners. All votes are calculated from the Republican perspective of the two-party vote. Composite does NOT include the Congressional elections. The statewide average is the unweighted mean of districts."#
#
district_summary <- stargazer(dist_summary,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    rownames = FALSE,#
    label = district_summary.label,#
    title = district_summary.caption,#
    notes = "REPLACE WITH NOTES" )#
district_summary <- district_summary[c(-6, -28, -29, -30, -31)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# =================================================================#
# Measures of Gerrymandering for the Eight Considered Plans#
# =================================================================#
gerry.table.gen <- #
    cbind.data.frame(#
        gtab("enacted"),#
        gtab("joint"),#
        gtab("govwolf"),#
        gtab("court")#
        )#
#
    colnames(gerry.table.gen) <- c(plan_names)#
    rownames(gerry.table.gen) <- c(#
        "Partisan Bias", #
        "CI1",#
        "Efficiency Gap",#
        "CI2",#
        "Mean/Median",#
        "CI3",#
        "Declination",#
        "CI4")#
#
gerry.caption = "Measures of Gerrymandering for the Four Considered Plans"#
gerry.label = "tab:gerry"#
gerry.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001. Measures are averages of 1,000 simulations for each map using the 2016 composite. Brackets numbers are the 95\\% range."#
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)#
    tab_gerry.tex <- gsub("$\\hat{\\mkern6mu}$", "^", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textasteriskcentered ", "*", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textbackslash ", "\\", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\{", "{", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\}", "}", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\$", "$", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)#
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )#
    colnames(tab_prop.gen) <- c(plan_names)#
    rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "SDS", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")#
#
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"#
prop.label = "tab:prob"#
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."#
#
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)#
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# =================================================================#
# Information about Twelve States with Constitutional Provisions similar to Pennsylvania#
# =================================================================#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# latex <- function(x) cat(paste(x, collapse="\n"), "\n")#
# states_compare.caption = "Information about Twelve States with Constitutional Provisions similar to Pennsylvania"#
# states_compare.label = "tab:states_compare"#
# states_compare.footnote = "Seats and votes are based on the 2016 five-election projection (to deal with the existence of non-contested congressional districts). Percentages are of the of district level results. This difference is why the percentages reported in columns 6 and 8 are not identical. Data from DailyKos, All about Redistricting, and Ballotpedia web sites.States with fewer than 9 districts do not have efficiency gap or median values reported because of the potential unreliability of those calculations given the small number of districts involved."#
#
# tab_states_compare <- #
#   paste("#
#       \\begin{table}#
#       \\caption{", states_compare.caption, "}#
#       \\label{", states_compare.label, "}#
#       \\begin{tabular}{#
#       >{\\centering\\arraybackslash}#
#       M{0.1\\linewidth}|#
#       M{.025\\linewidth}|#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.3\\linewidth}}#
#       \\textbf{State} & \\textbf{\\# CDs} & \\textbf{Unified Control (2011)} & \\textbf{Unified Control (2016)} & \\textbf{Seats (2016)} & \\textbf{Votes (2016)} & \\textbf{Mean District Vote Share (Dem)} & \\textbf{Median District Vote Share (Dem)} & \\textbf{Efficiency Gap} & \\textbf{Who does districting} \\\\#
#        \\hline#
#       Arizona & 9 & $\\surd$ (R) & $\\surd$ (R) & 44.4\\% & 48.1\\% & 50.4\\% & 49.4\\% & 0.08 & Independent commission \\\\#
#       Arkansas & 4 & $\\surd$ (D) & $\\surd$ (R) & 0\\% & 35.7\\% & 35.5\\% &  &  & state legislature \\\\#
#       Delaware & 1 & $\\surd$ (D) & $\\surd$ (D) & 100\\% & 55.9\\% & 56\\% &  &  & NA \\\\#
#       Illinois & 18 & $\\surd$ (D) &  & 61.1\\% & 59.0\\% & 59.6\\% & 59.8\\% & 0.08 & state legislature \\\\#
#       Indiana & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 39.9\\% & 40.1\\% & 35.9\\% & 0.09 & state legislature \\\\#
#       Kentucky & 6 &  & $\\surd$ (R) & 16.7\\% & 34.3\\% & 33.8\\% &  &  & state legislature \\\\#
#       Oklahoma & 5 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 30.7\\% & 30.7\\% &  &  & state legislature \\\\#
#       Oregon & 5 &  & $\\surd$ (D) & 80\\% & 56.2\\% & 56\\% &  &  & state legislature \\\\#
#       South Dakota & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 34.0\\% & 34\\% &  &  & NA \\\\#
#       Tennessee & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 36.4\\% & 37.4\\% & 31.3\\% & 0.03 & state legislature \\\\#
#       Washington & 10 & $\\surd$ (D) & $\\surd$ (D) & 70\\% & 58.8\\% & 56\\% & 52.3\\% & -0.05 & 5-member independent commission \\\\#
#       Wyoming & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 24.3\\% & 24.3\\% &  &  & NA ")#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# TWO-PARTY VOTE SHARES, ACTUAL (CONGRESSIONAL)#
# tab_summary_stats <- stargazer(#
#   congress.PA.2008.2018,#
#   title="Republican Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   style = "apsr", #
#   header = FALSE,#
#   column.sep.width = "-5pt",#
#   multicolumn = TRUE,#
#   label = "tab:twoparty_actuals" )#
# tab_summary_stats <- tab_summary_stats[c(-6, -11)]#
#
#  Table(tab_summary_stats,#
#   caption="Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   label="ab:twoparty_actuals",#
#   footnote="Votes are the two-party unweighted average of PA Congressional Districts by year (uncontested elections imputed).")#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# AVERAGE VOTE BY DISTRICT#
means_races <- numeric()#
  base.form <- formula(. ~ 1)#
  cong.form <- formula(default.unc(enacted.elections$CONG) ~ + pahouse.2016$INC)#
for (k in colnames(enacted.elections))#
{#
  r.tmp <- composite(pa.redist.dta, "enacted")[,k]#
    reg_races <- summary(lm(update(base.form, as.formula(default.unc(r.tmp) ~ .))))#
    means_races[k] <- coef(reg_races)[1]#
}#
#
  cat(#
    "2016 Average Vote Share by State-level Contest \n"#
    )#
    print(cbind.data.frame(Race = statewide.contests.2016, Mean = percent(means_races)))#
    cat(paste("", collapse = "\n"), "\n")#
# # Regression of Race on Congressional Results#
    reg_cong_pres <- lm(update(cong.form, as.formula(. ~ . + default.unc(enacted.elections$PRES))))#
    reg_cong_ussen <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$USSEN))))#
    reg_cong_atg <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$ATTGEN))))#
    reg_cong_aud <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$AUDITOR))))#
    reg_cong_trea <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$TREASURER))))#
    reg_cong_comp <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$COMPOSITE))))#
    reg_cong_all <- lm(update(cong.form, #
      as.formula(. ~  . +  #
        default.unc(enacted.elections$PRES) + #
        default.unc(enacted.elections$USSEN) +#
        default.unc(enacted.elections$ATTGEN) +#
        default.unc(enacted.elections$AUDITOR) +#
        default.unc(enacted.elections$TREASURER))#
    ))#
#
congress_predict.caption = "Comparing Pennsylvania Congressional Results with State-wide Elections (2016)"#
congress_predict.label = "tab:tab_congress_predict"#
congress_predict.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001 \\\\ Uncontested (or non-competitive) elections replaced with 0.25 \\& 0.75 vote shares. Regressions are unweighted, ie, all districts are assumed to have identical turnout. This is the usual way political scientist measure aggregate congressional vote \\cite{GelmanKing1994_unifiedAJPS}. \\\\"#
#
tab_congress_predict <- stargazer(#
        reg_cong_pres, #
         reg_cong_ussen, #
         reg_cong_atg, #
         reg_cong_aud, #
         reg_cong_trea, #
         reg_cong_comp,#
         reg_cong_all,#
star.cutoffs = c(0.05,0.01, 0.001),#
style = "apsr", #
header = FALSE,#
model.numbers = FALSE,#
initial.zero = TRUE,#
digits = 2,#
column.sep.width = "0pt",#
multicolumn = TRUE,#
omit.stat = c("ll", "F", "ser"),#
dep.var.labels = "Actual Congressional Results (2016)",#
covariate.labels = c("Incumbency Advantage","Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite"),#
label = congress_predict.label,#
title = congress_predict.caption,#
notes = congress_predict.footnote#
    )#
tab_congress_predict <- tab_congress_predict[c(-6, -26, -28, -29, -30, -31, -32)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
sink()#
Table(tab_plan_summary,#
    path = "Tables/_tab_summaries.tex",#
    caption = plan_summary.caption,#
    label = plan_summary.label,#
    footnote = plan_summary.footnote)#
#
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)#
#
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)#
#
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)#
#
Table(district_summary,#
    path = "Tables/_a_tab_districtvotes.tex",#
    caption = district_summary.caption,#
    label = district_summary.label,#
    footnote = district_summary.footnote#
    )#
#
Table(tab_congress_predict,#
    path = "Tables/a_tab_congress_predict.tex",#
    caption = congress_predict.caption,#
    label = congress_predict.label,#
    footnote = congress_predict.footnote)#
cat("#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#
    ")
# ▀▀█▀▀ █▀▀█ █▀▀▄ █░░ █▀▀ █▀▀#
# ░░█░░ █▄▄█ █▀▀▄ █░░ █▀▀ ▀▀█#
# ░░▀░░ ▀░░▀ ▀▀▀░ ▀▀▀ ▀▀▀ ▀▀▀#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
cat(#
    "\n#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
        CREATING TABLES . . . . . . . . . . . . . . . . . . . . .#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n")#
sink("/dev/null")#
# races <- c("cong2016", "pres16", "ussen16", "atg16", "aud16", "trea16", "comp2016")#
statewide.contests.2016 <- c("Congress", "Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite")#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# County Spliits#
county_splits <- c(41, 19, 19, 17)#
# COMPACTNESS#
geom_names <- paste0(plans, ".geom")#
#
reock.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        reock.plans <- rbind(reock.plans, REOCK(get(geom_names[i])))#
        }#
polsby.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        polsby.plans <- rbind(polsby.plans, POLSBYPOPPER(get(geom_names[i])))#
        }#
#
tab_plan_sum <- cbind.data.frame(#
    Plan = plan_names,#
    CountySplits = county_splits, #
    Reock = reock.plans, #
    PolsbyPopper = polsby.plans#
    # Pres2016 = c(seats.print(enacted.pres), seats.print(court.pres), seats.print(joint.pres), seats.print(govwolf.pres)),#
    # Comp2016 = c(seats.print(enacted.comp), seats.print(court.comp), seats.print(joint.comp), seats.print(govwolf.comp))#
    )#
tab_plan_sum#
plan_summary.caption = "County Splits and Compactness Scores of the Plans"#
plan_summary.label = "tab:summaries"#
plan_summary.footnote = "County splits include all the pieces in which a county is split, not just the total number of counties that have been split. (The latter number is the one most often reported in both court documents and in the media, but we regard the measure we report as both more precise and more informative.)"#
tab_plan_summary <- #
    stargazer(tab_plan_sum,#
        style = "apsr", #
        header = FALSE,#
        summary = FALSE,#
        model.numbers = FALSE,#
        initial.zero = TRUE,#
        digits = 3,#
        column.sep.width = "0pt",#
        rownames = FALSE,#
        multicolumn = TRUE,#
        label = plan_summary.label,#
        title = plan_summary.caption,#
        covariate.labels = ,#
        notes = plan_summary.footnote #
        )#
tab_plan_summary <- tab_plan_summary[c(-6, -7, -13, -14, -15, -16)]#
# tab_plan_summary <- append(tab_plan_summary, #
#   "  &   &  &  & \\textbf{Projected} & \\textbf{Projected}  \\\\", #
#   after = 5)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  &  \\textbf{County}  &  \\textbf{Polsby}   &   &  \\textbf{2016 using}  &  \\textbf{five state-wide} \\\\",#
#   after = 6)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  &  \\textbf{Presidential Results}  &  \\textbf{elections in 2016} \\\\",#
#   after = 7)#
#
tab_plan_summary <- append(tab_plan_summary,#
    "  &  \\textbf{County}  &  \\textbf{Polsby}   &   \\\\",#
    after = 5)#
tab_plan_summary <- append(tab_plan_summary,#
    "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  \\\\",#
    after = 6)#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# =================================================================#
# Comparing Pennsylvania Congressional Results with State-wide Elections (2016) %#
# =================================================================#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# SUMMARY OF ELECTIONS#
percent(sum(house.rep.2012)/sum(house.2012.turnout))#
percent(sum(house.rep.2014)/sum(house.2014.turnout))#
percent(sum(house.rep.2016)/sum(house.2016.turnout))#
#
congsum.caption = "U.S. House Election Summaries \\\\ \\hspace{2cm}(PA 2012-2016 Enacted Map)"#
congsum.label = "tab:congsum"#
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."#
congsum.tmp <- cbind.data.frame(#
    # PA2004 = gerry(default.unc(house.2004.votes)),#
    # PA2006 = gerry(default.unc(house.2006.votes)),#
    # PA2008 = gerry(default.unc(house.2008.votes)),#
    # PA2010 = gerry(default.unc(house.2010.votes)),#
    PA2012 = gerry(default.unc(house.2012.votes)), #
    PA2014 = gerry(default.unc(house.2014.votes)), #
    PA2016 = gerry(default.unc(house.2016.votes)),#
    # PA2018 = gerry(default.unc(house.2018.votes)),#
    PA2012_2016_AVE = rowMeans(cbind(   #
    gerry(default.unc(house.2012.votes), toggle=F),#
    gerry(default.unc(house.2014.votes), toggle=F),#
    gerry(default.unc(house.2016.votes), toggle=F)))#
  )#
congsum.tmp[2:3,4]  <- percent(congsum.tmp[2:3,4])#
congsum.tmp[4:7,4] <- r(as.numeric(congsum.tmp[4:7,4]))#
colnames(congsum.tmp) <- c(seq(2012,2016,2),"AVE")#
congsum.tmp.tex <- stargazer(congsum.tmp,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    label = congsum.label,#
    title = congsum.caption,#
    notes = congsum.footnote)#
congsum.tmp.tex <- congsum.tmp.tex[c(-6, -16, -17, -18, -19)]#
congsum.tmp.tex[8] <- "Seats &  [13R-5D] &  [13R-5D] &  [13R-5D] & [13R-5D] \\\\ "#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
dist_summary <- #
    rbind(#
      cbind.data.frame(#
        District = seq(1,18,1),#
        sapply(composite(pa.redist.dta, "enacted"), percent)),#
        c(#
          "Statewide",#
          sapply(colMeans(composite(pa.redist.dta, "enacted")), percent)#
        )#
    )#
#
district_summary.caption = "2016 District-level Summaries for 2011 Enacted Plan"#
district_summary.label = "tab:districtvotes"#
district_summary.footnote = "Uncontested races and those with only negligible competition will be imputed with 0.25 and 0.75 for the respective winners. All votes are calculated from the Republican perspective of the two-party vote. Composite does NOT include the Congressional elections. The statewide average is the unweighted mean of districts."#
#
district_summary <- stargazer(dist_summary,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    rownames = FALSE,#
    label = district_summary.label,#
    title = district_summary.caption,#
    notes = "REPLACE WITH NOTES" )#
district_summary <- district_summary[c(-6, -28, -29, -30, -31)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# =================================================================#
# Measures of Gerrymandering for the Eight Considered Plans#
# =================================================================#
gerry.table.gen <- #
    cbind.data.frame(#
        gtab("enacted"),#
        gtab("joint"),#
        gtab("govwolf"),#
        gtab("court")#
        )#
#
    colnames(gerry.table.gen) <- c(plan_names)#
    rownames(gerry.table.gen) <- c(#
        "Partisan Bias", #
        "CI1",#
        "Efficiency Gap",#
        "CI2",#
        "Mean/Median",#
        "CI3",#
        "Declination",#
        "CI4")#
#
gerry.caption = "Measures of Gerrymandering for the Four Considered Plans"#
gerry.label = "tab:gerry"#
gerry.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001. Measures are averages of 1,000 simulations for each map using the 2016 composite. Brackets numbers are the 95\\% range."#
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)#
    tab_gerry.tex <- gsub("$\\hat{\\mkern6mu}$", "^", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textasteriskcentered ", "*", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\textbackslash ", "\\", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\{", "{", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\}", "}", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("\\$", "$", tab_gerry.tex,fixed=TRUE)#
    tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)#
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )#
    colnames(tab_prop.gen) <- c(plan_names)#
    rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "SDS", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")#
#
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"#
prop.label = "tab:prob"#
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."#
#
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)#
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# =================================================================#
# Information about Twelve States with Constitutional Provisions similar to Pennsylvania#
# =================================================================#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# latex <- function(x) cat(paste(x, collapse="\n"), "\n")#
# states_compare.caption = "Information about Twelve States with Constitutional Provisions similar to Pennsylvania"#
# states_compare.label = "tab:states_compare"#
# states_compare.footnote = "Seats and votes are based on the 2016 five-election projection (to deal with the existence of non-contested congressional districts). Percentages are of the of district level results. This difference is why the percentages reported in columns 6 and 8 are not identical. Data from DailyKos, All about Redistricting, and Ballotpedia web sites.States with fewer than 9 districts do not have efficiency gap or median values reported because of the potential unreliability of those calculations given the small number of districts involved."#
#
# tab_states_compare <- #
#   paste("#
#       \\begin{table}#
#       \\caption{", states_compare.caption, "}#
#       \\label{", states_compare.label, "}#
#       \\begin{tabular}{#
#       >{\\centering\\arraybackslash}#
#       M{0.1\\linewidth}|#
#       M{.025\\linewidth}|#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.3\\linewidth}}#
#       \\textbf{State} & \\textbf{\\# CDs} & \\textbf{Unified Control (2011)} & \\textbf{Unified Control (2016)} & \\textbf{Seats (2016)} & \\textbf{Votes (2016)} & \\textbf{Mean District Vote Share (Dem)} & \\textbf{Median District Vote Share (Dem)} & \\textbf{Efficiency Gap} & \\textbf{Who does districting} \\\\#
#        \\hline#
#       Arizona & 9 & $\\surd$ (R) & $\\surd$ (R) & 44.4\\% & 48.1\\% & 50.4\\% & 49.4\\% & 0.08 & Independent commission \\\\#
#       Arkansas & 4 & $\\surd$ (D) & $\\surd$ (R) & 0\\% & 35.7\\% & 35.5\\% &  &  & state legislature \\\\#
#       Delaware & 1 & $\\surd$ (D) & $\\surd$ (D) & 100\\% & 55.9\\% & 56\\% &  &  & NA \\\\#
#       Illinois & 18 & $\\surd$ (D) &  & 61.1\\% & 59.0\\% & 59.6\\% & 59.8\\% & 0.08 & state legislature \\\\#
#       Indiana & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 39.9\\% & 40.1\\% & 35.9\\% & 0.09 & state legislature \\\\#
#       Kentucky & 6 &  & $\\surd$ (R) & 16.7\\% & 34.3\\% & 33.8\\% &  &  & state legislature \\\\#
#       Oklahoma & 5 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 30.7\\% & 30.7\\% &  &  & state legislature \\\\#
#       Oregon & 5 &  & $\\surd$ (D) & 80\\% & 56.2\\% & 56\\% &  &  & state legislature \\\\#
#       South Dakota & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 34.0\\% & 34\\% &  &  & NA \\\\#
#       Tennessee & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 36.4\\% & 37.4\\% & 31.3\\% & 0.03 & state legislature \\\\#
#       Washington & 10 & $\\surd$ (D) & $\\surd$ (D) & 70\\% & 58.8\\% & 56\\% & 52.3\\% & -0.05 & 5-member independent commission \\\\#
#       Wyoming & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 24.3\\% & 24.3\\% &  &  & NA ")#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# TWO-PARTY VOTE SHARES, ACTUAL (CONGRESSIONAL)#
# tab_summary_stats <- stargazer(#
#   congress.PA.2008.2018,#
#   title="Republican Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   style = "apsr", #
#   header = FALSE,#
#   column.sep.width = "-5pt",#
#   multicolumn = TRUE,#
#   label = "tab:twoparty_actuals" )#
# tab_summary_stats <- tab_summary_stats[c(-6, -11)]#
#
#  Table(tab_summary_stats,#
#   caption="Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   label="ab:twoparty_actuals",#
#   footnote="Votes are the two-party unweighted average of PA Congressional Districts by year (uncontested elections imputed).")#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# AVERAGE VOTE BY DISTRICT#
means_races <- numeric()#
  base.form <- formula(. ~ 1)#
  cong.form <- formula(default.unc(enacted.elections$CONG) ~ + pahouse.2016$INC)#
for (k in colnames(enacted.elections))#
{#
  r.tmp <- composite(pa.redist.dta, "enacted")[,k]#
    reg_races <- summary(lm(update(base.form, as.formula(default.unc(r.tmp) ~ .))))#
    means_races[k] <- coef(reg_races)[1]#
}#
#
  cat(#
    "2016 Average Vote Share by State-level Contest \n"#
    )#
    print(cbind.data.frame(Race = statewide.contests.2016, Mean = percent(means_races)))#
    cat(paste("", collapse = "\n"), "\n")#
# # Regression of Race on Congressional Results#
    reg_cong_pres <- lm(update(cong.form, as.formula(. ~ . + default.unc(enacted.elections$PRES))))#
    reg_cong_ussen <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$USSEN))))#
    reg_cong_atg <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$ATTGEN))))#
    reg_cong_aud <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$AUDITOR))))#
    reg_cong_trea <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$TREASURER))))#
    reg_cong_comp <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$COMPOSITE))))#
    reg_cong_all <- lm(update(cong.form, #
      as.formula(. ~  . +  #
        default.unc(enacted.elections$PRES) + #
        default.unc(enacted.elections$USSEN) +#
        default.unc(enacted.elections$ATTGEN) +#
        default.unc(enacted.elections$AUDITOR) +#
        default.unc(enacted.elections$TREASURER))#
    ))#
#
congress_predict.caption = "Comparing Pennsylvania Congressional Results with State-wide Elections (2016)"#
congress_predict.label = "tab:tab_congress_predict"#
congress_predict.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001 \\\\ Uncontested (or non-competitive) elections replaced with 0.25 \\& 0.75 vote shares. Regressions are unweighted, ie, all districts are assumed to have identical turnout. This is the usual way political scientist measure aggregate congressional vote \\cite{GelmanKing1994_unifiedAJPS}. \\\\"#
#
tab_congress_predict <- stargazer(#
        reg_cong_pres, #
         reg_cong_ussen, #
         reg_cong_atg, #
         reg_cong_aud, #
         reg_cong_trea, #
         reg_cong_comp,#
         reg_cong_all,#
star.cutoffs = c(0.05,0.01, 0.001),#
style = "apsr", #
header = FALSE,#
model.numbers = FALSE,#
initial.zero = TRUE,#
digits = 2,#
column.sep.width = "0pt",#
multicolumn = TRUE,#
omit.stat = c("ll", "F", "ser"),#
dep.var.labels = "Actual Congressional Results (2016)",#
covariate.labels = c("Incumbency Advantage","Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite"),#
label = congress_predict.label,#
title = congress_predict.caption,#
notes = congress_predict.footnote#
    )#
tab_congress_predict <- tab_congress_predict[c(-6, -26, -28, -29, -30, -31, -32)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
sink()#
Table(tab_plan_summary,#
    path = "Tables/_tab_summaries.tex",#
    caption = plan_summary.caption,#
    label = plan_summary.label,#
    footnote = plan_summary.footnote)#
#
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)#
#
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)#
#
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)#
#
Table(district_summary,#
    path = "Tables/_a_tab_districtvotes.tex",#
    caption = district_summary.caption,#
    label = district_summary.label,#
    footnote = district_summary.footnote#
    )#
#
Table(tab_congress_predict,#
    path = "Tables/a_tab_congress_predict.tex",#
    caption = congress_predict.caption,#
    label = congress_predict.label,#
    footnote = congress_predict.footnote)#
cat("#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#
    ")
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)
# tab_gerry.tex <- gsub("$\\hat{\\mkern6mu}$", "^", tab_gerry.tex,fixed=TRUE)
# tab_gerry.tex <- gsub("\\textasteriskcentered ", "*", tab_gerry.tex,fixed=TRUE)
# tab_gerry.tex <- gsub("\\textbackslash ", "\\", tab_gerry.tex,fixed=TRUE)
# tab_gerry.tex <- gsub("\\{", "{", tab_gerry.tex,fixed=TRUE)
#    tab_gerry.tex <- gsub("\\}", "}", tab_gerry.tex,fixed=TRUE)
#    tab_gerry.tex <- gsub("\\$", "$", tab_gerry.tex,fixed=TRUE)
tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )
colnames(tab_prop.gen) <- c(plan_names)
rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "CI1", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"
prop.label = "tab:prob"
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)
tab_prop <- gsub("CI[1-9]", "", tab_prop)
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)
tab_prop
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )#
    colnames(tab_prop.gen) <- c(plan_names)#
    rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "CI1", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")#
#
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"#
prop.label = "tab:prob"#
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."#
#
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)#
tab_prop.tex <- gsub("CI[1-9]", "", tab_prop.tex)#
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)
ptab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
    return(c(#
      paste0(r(sum(s.tmp*18)/(1000*18)*18, d=1), "R-", r(18-sum(s.tmp*18)/(1000*18)*18, d=1), "D"),#
      paste0("{\\small\\textit{[", r(quantile(s.tmp, 0.025),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.025),d=1)*18, "D, ", r(quantile(s.tmp, 0.975),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.975),d=1)*18, "D]}}"),#
      paste0(median(s.tmp)*18, "R-", 18-median(s.tmp)*18, "D"),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) > 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) < 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) == 0.5) / 1000)#
      ))#
}
# ▀▀█▀▀ █▀▀█ █▀▀▄ █░░ █▀▀ █▀▀#
# ░░█░░ █▄▄█ █▀▀▄ █░░ █▀▀ ▀▀█#
# ░░▀░░ ▀░░▀ ▀▀▀░ ▀▀▀ ▀▀▀ ▀▀▀#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
cat(#
    "\n#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
        CREATING TABLES . . . . . . . . . . . . . . . . . . . . .#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n")#
sink("/dev/null")#
# races <- c("cong2016", "pres16", "ussen16", "atg16", "aud16", "trea16", "comp2016")#
statewide.contests.2016 <- c("Congress", "Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite")#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# County Spliits#
county_splits <- c(41, 19, 19, 17)#
# COMPACTNESS#
geom_names <- paste0(plans, ".geom")#
#
reock.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        reock.plans <- rbind(reock.plans, REOCK(get(geom_names[i])))#
        }#
polsby.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        polsby.plans <- rbind(polsby.plans, POLSBYPOPPER(get(geom_names[i])))#
        }#
#
tab_plan_sum <- cbind.data.frame(#
    Plan = plan_names,#
    CountySplits = county_splits, #
    Reock = reock.plans, #
    PolsbyPopper = polsby.plans#
    # Pres2016 = c(seats.print(enacted.pres), seats.print(court.pres), seats.print(joint.pres), seats.print(govwolf.pres)),#
    # Comp2016 = c(seats.print(enacted.comp), seats.print(court.comp), seats.print(joint.comp), seats.print(govwolf.comp))#
    )#
tab_plan_sum#
plan_summary.caption = "County Splits and Compactness Scores of the Plans"#
plan_summary.label = "tab:summaries"#
plan_summary.footnote = "County splits include all the pieces in which a county is split, not just the total number of counties that have been split. (The latter number is the one most often reported in both court documents and in the media, but we regard the measure we report as both more precise and more informative.)"#
tab_plan_summary <- #
    stargazer(tab_plan_sum,#
        style = "apsr", #
        header = FALSE,#
        summary = FALSE,#
        model.numbers = FALSE,#
        initial.zero = TRUE,#
        digits = 3,#
        column.sep.width = "0pt",#
        rownames = FALSE,#
        multicolumn = TRUE,#
        label = plan_summary.label,#
        title = plan_summary.caption,#
        covariate.labels = ,#
        notes = plan_summary.footnote #
        )#
tab_plan_summary <- tab_plan_summary[c(-6, -7, -13, -14, -15, -16)]#
# tab_plan_summary <- append(tab_plan_summary, #
#   "  &   &  &  & \\textbf{Projected} & \\textbf{Projected}  \\\\", #
#   after = 5)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  &  \\textbf{County}  &  \\textbf{Polsby}   &   &  \\textbf{2016 using}  &  \\textbf{five state-wide} \\\\",#
#   after = 6)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  &  \\textbf{Presidential Results}  &  \\textbf{elections in 2016} \\\\",#
#   after = 7)#
#
tab_plan_summary <- append(tab_plan_summary,#
    "  &  \\textbf{County}  &  \\textbf{Polsby}   &   \\\\",#
    after = 5)#
tab_plan_summary <- append(tab_plan_summary,#
    "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  \\\\",#
    after = 6)#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# =================================================================#
# Comparing Pennsylvania Congressional Results with State-wide Elections (2016) %#
# =================================================================#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# SUMMARY OF ELECTIONS#
percent(sum(house.rep.2012)/sum(house.2012.turnout))#
percent(sum(house.rep.2014)/sum(house.2014.turnout))#
percent(sum(house.rep.2016)/sum(house.2016.turnout))#
#
congsum.caption = "U.S. House Election Summaries \\\\ \\hspace{2cm}(PA 2012-2016 Enacted Map)"#
congsum.label = "tab:congsum"#
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."#
congsum.tmp <- cbind.data.frame(#
    # PA2004 = gerry(default.unc(house.2004.votes)),#
    # PA2006 = gerry(default.unc(house.2006.votes)),#
    # PA2008 = gerry(default.unc(house.2008.votes)),#
    # PA2010 = gerry(default.unc(house.2010.votes)),#
    PA2012 = gerry(default.unc(house.2012.votes)), #
    PA2014 = gerry(default.unc(house.2014.votes)), #
    PA2016 = gerry(default.unc(house.2016.votes)),#
    # PA2018 = gerry(default.unc(house.2018.votes)),#
    PA2012_2016_AVE = rowMeans(cbind(   #
    gerry(default.unc(house.2012.votes), toggle=F),#
    gerry(default.unc(house.2014.votes), toggle=F),#
    gerry(default.unc(house.2016.votes), toggle=F)))#
  )#
congsum.tmp[2:3,4]  <- percent(congsum.tmp[2:3,4])#
congsum.tmp[4:7,4] <- r(as.numeric(congsum.tmp[4:7,4]))#
colnames(congsum.tmp) <- c(seq(2012,2016,2),"AVE")#
congsum.tmp.tex <- stargazer(congsum.tmp,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    label = congsum.label,#
    title = congsum.caption,#
    notes = congsum.footnote)#
congsum.tmp.tex <- congsum.tmp.tex[c(-6, -16, -17, -18, -19)]#
congsum.tmp.tex[8] <- "Seats &  [13R-5D] &  [13R-5D] &  [13R-5D] & [13R-5D] \\\\ "#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
dist_summary <- #
    rbind(#
      cbind.data.frame(#
        District = seq(1,18,1),#
        sapply(composite(pa.redist.dta, "enacted"), percent)),#
        c(#
          "Statewide",#
          sapply(colMeans(composite(pa.redist.dta, "enacted")), percent)#
        )#
    )#
#
district_summary.caption = "2016 District-level Summaries for 2011 Enacted Plan"#
district_summary.label = "tab:districtvotes"#
district_summary.footnote = "Uncontested races and those with only negligible competition will be imputed with 0.25 and 0.75 for the respective winners. All votes are calculated from the Republican perspective of the two-party vote. Composite does NOT include the Congressional elections. The statewide average is the unweighted mean of districts."#
#
district_summary <- stargazer(dist_summary,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    rownames = FALSE,#
    label = district_summary.label,#
    title = district_summary.caption,#
    notes = "REPLACE WITH NOTES" )#
district_summary <- district_summary[c(-6, -28, -29, -30, -31)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# =================================================================#
# Measures of Gerrymandering for the Eight Considered Plans#
# =================================================================#
gerry.table.gen <- #
    cbind.data.frame(#
        gtab("enacted"),#
        gtab("joint"),#
        gtab("govwolf"),#
        gtab("court")#
        )#
#
    colnames(gerry.table.gen) <- c(plan_names)#
    rownames(gerry.table.gen) <- c(#
        "Partisan Bias", #
        "CI1",#
        "Efficiency Gap",#
        "CI2",#
        "Mean/Median",#
        "CI3",#
        "Declination",#
        "CI4")#
#
gerry.caption = "Measures of Gerrymandering for the Four Considered Plans"#
gerry.label = "tab:gerry"#
gerry.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001. Measures are averages of 1,000 simulations for each map using the 2016 composite. Brackets numbers are the 95\\% range."#
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)#
    tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)#
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )#
    colnames(tab_prop.gen) <- c(plan_names)#
    rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "CI1", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")#
#
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"#
prop.label = "tab:prob"#
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."#
#
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)#
tab_prop.tex <- gsub("CI[1-9]", "", tab_prop.tex)#
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# =================================================================#
# Information about Twelve States with Constitutional Provisions similar to Pennsylvania#
# =================================================================#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# latex <- function(x) cat(paste(x, collapse="\n"), "\n")#
# states_compare.caption = "Information about Twelve States with Constitutional Provisions similar to Pennsylvania"#
# states_compare.label = "tab:states_compare"#
# states_compare.footnote = "Seats and votes are based on the 2016 five-election projection (to deal with the existence of non-contested congressional districts). Percentages are of the of district level results. This difference is why the percentages reported in columns 6 and 8 are not identical. Data from DailyKos, All about Redistricting, and Ballotpedia web sites.States with fewer than 9 districts do not have efficiency gap or median values reported because of the potential unreliability of those calculations given the small number of districts involved."#
#
# tab_states_compare <- #
#   paste("#
#       \\begin{table}#
#       \\caption{", states_compare.caption, "}#
#       \\label{", states_compare.label, "}#
#       \\begin{tabular}{#
#       >{\\centering\\arraybackslash}#
#       M{0.1\\linewidth}|#
#       M{.025\\linewidth}|#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.3\\linewidth}}#
#       \\textbf{State} & \\textbf{\\# CDs} & \\textbf{Unified Control (2011)} & \\textbf{Unified Control (2016)} & \\textbf{Seats (2016)} & \\textbf{Votes (2016)} & \\textbf{Mean District Vote Share (Dem)} & \\textbf{Median District Vote Share (Dem)} & \\textbf{Efficiency Gap} & \\textbf{Who does districting} \\\\#
#        \\hline#
#       Arizona & 9 & $\\surd$ (R) & $\\surd$ (R) & 44.4\\% & 48.1\\% & 50.4\\% & 49.4\\% & 0.08 & Independent commission \\\\#
#       Arkansas & 4 & $\\surd$ (D) & $\\surd$ (R) & 0\\% & 35.7\\% & 35.5\\% &  &  & state legislature \\\\#
#       Delaware & 1 & $\\surd$ (D) & $\\surd$ (D) & 100\\% & 55.9\\% & 56\\% &  &  & NA \\\\#
#       Illinois & 18 & $\\surd$ (D) &  & 61.1\\% & 59.0\\% & 59.6\\% & 59.8\\% & 0.08 & state legislature \\\\#
#       Indiana & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 39.9\\% & 40.1\\% & 35.9\\% & 0.09 & state legislature \\\\#
#       Kentucky & 6 &  & $\\surd$ (R) & 16.7\\% & 34.3\\% & 33.8\\% &  &  & state legislature \\\\#
#       Oklahoma & 5 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 30.7\\% & 30.7\\% &  &  & state legislature \\\\#
#       Oregon & 5 &  & $\\surd$ (D) & 80\\% & 56.2\\% & 56\\% &  &  & state legislature \\\\#
#       South Dakota & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 34.0\\% & 34\\% &  &  & NA \\\\#
#       Tennessee & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 36.4\\% & 37.4\\% & 31.3\\% & 0.03 & state legislature \\\\#
#       Washington & 10 & $\\surd$ (D) & $\\surd$ (D) & 70\\% & 58.8\\% & 56\\% & 52.3\\% & -0.05 & 5-member independent commission \\\\#
#       Wyoming & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 24.3\\% & 24.3\\% &  &  & NA ")#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# TWO-PARTY VOTE SHARES, ACTUAL (CONGRESSIONAL)#
# tab_summary_stats <- stargazer(#
#   congress.PA.2008.2018,#
#   title="Republican Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   style = "apsr", #
#   header = FALSE,#
#   column.sep.width = "-5pt",#
#   multicolumn = TRUE,#
#   label = "tab:twoparty_actuals" )#
# tab_summary_stats <- tab_summary_stats[c(-6, -11)]#
#
#  Table(tab_summary_stats,#
#   caption="Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   label="ab:twoparty_actuals",#
#   footnote="Votes are the two-party unweighted average of PA Congressional Districts by year (uncontested elections imputed).")#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# AVERAGE VOTE BY DISTRICT#
means_races <- numeric()#
  base.form <- formula(. ~ 1)#
  cong.form <- formula(default.unc(enacted.elections$CONG) ~ + pahouse.2016$INC)#
for (k in colnames(enacted.elections))#
{#
  r.tmp <- composite(pa.redist.dta, "enacted")[,k]#
    reg_races <- summary(lm(update(base.form, as.formula(default.unc(r.tmp) ~ .))))#
    means_races[k] <- coef(reg_races)[1]#
}#
#
  cat(#
    "2016 Average Vote Share by State-level Contest \n"#
    )#
    print(cbind.data.frame(Race = statewide.contests.2016, Mean = percent(means_races)))#
    cat(paste("", collapse = "\n"), "\n")#
# # Regression of Race on Congressional Results#
    reg_cong_pres <- lm(update(cong.form, as.formula(. ~ . + default.unc(enacted.elections$PRES))))#
    reg_cong_ussen <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$USSEN))))#
    reg_cong_atg <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$ATTGEN))))#
    reg_cong_aud <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$AUDITOR))))#
    reg_cong_trea <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$TREASURER))))#
    reg_cong_comp <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$COMPOSITE))))#
    reg_cong_all <- lm(update(cong.form, #
      as.formula(. ~  . +  #
        default.unc(enacted.elections$PRES) + #
        default.unc(enacted.elections$USSEN) +#
        default.unc(enacted.elections$ATTGEN) +#
        default.unc(enacted.elections$AUDITOR) +#
        default.unc(enacted.elections$TREASURER))#
    ))#
#
congress_predict.caption = "Comparing Pennsylvania Congressional Results with State-wide Elections (2016)"#
congress_predict.label = "tab:tab_congress_predict"#
congress_predict.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001 \\\\ Uncontested (or non-competitive) elections replaced with 0.25 \\& 0.75 vote shares. Regressions are unweighted, ie, all districts are assumed to have identical turnout. This is the usual way political scientist measure aggregate congressional vote \\cite{GelmanKing1994_unifiedAJPS}. \\\\"#
#
tab_congress_predict <- stargazer(#
        reg_cong_pres, #
         reg_cong_ussen, #
         reg_cong_atg, #
         reg_cong_aud, #
         reg_cong_trea, #
         reg_cong_comp,#
         reg_cong_all,#
star.cutoffs = c(0.05,0.01, 0.001),#
style = "apsr", #
header = FALSE,#
model.numbers = FALSE,#
initial.zero = TRUE,#
digits = 2,#
column.sep.width = "0pt",#
multicolumn = TRUE,#
omit.stat = c("ll", "F", "ser"),#
dep.var.labels = "Actual Congressional Results (2016)",#
covariate.labels = c("Incumbency Advantage","Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite"),#
label = congress_predict.label,#
title = congress_predict.caption,#
notes = congress_predict.footnote#
    )#
tab_congress_predict <- tab_congress_predict[c(-6, -26, -28, -29, -30, -31, -32)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
sink()
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)
Table(tab_plan_summary,#
    path = "Tables/_tab_summaries.tex",#
    caption = plan_summary.caption,#
    label = plan_summary.label,#
    footnote = plan_summary.footnote)#
#
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)#
#
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)#
#
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)#
#
Table(district_summary,#
    path = "Tables/_a_tab_districtvotes.tex",#
    caption = district_summary.caption,#
    label = district_summary.label,#
    footnote = district_summary.footnote#
    )#
#
Table(tab_congress_predict,#
    path = "Tables/a_tab_congress_predict.tex",#
    caption = congress_predict.caption,#
    label = congress_predict.label,#
    footnote = congress_predict.footnote)
cat(#
  "\n#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
    LOADING FUNCTIONS. . . . . . . . . . . . . . . . . . . . . . #
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n")#
`Table` <- function(x, path=NULL, caption="", label="", footnote="", landscape=FALSE, out=NULL)#
  {#
    x <- append(x,#
paste0(#
"\n% =====================================================================#
% ▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center}\\textbf{INSERT TABLE \\ref{", label, "} ABOUT HERE} \\end{center}#
% •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n#
\n% =====================================================================#
% ▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ T̟A̟B̟L̟E̟#
% ---------------------------------------------------------------------",#
ifelse(landscape==T, "\n\\begin{landscape}", "")),#
 after = 1)#
#
    x <- append(x,#
paste0("\\end{tabular}#
\\tabnotes{", footnote, "}#
\\end{table}",#
ifelse(landscape==T, "\n\\end{landscape}", ""),#
"#
% ---------------------------------------------------------------------#
% ▀▄▀▄▀▄ E͎N͎D͎ T͎A͎B͎L͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ T͎A͎B͎L͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ T͎A͎B͎L͎E͎ ▄▀▄▀▄▀#
% ===================================================================== \n \n"),#
after = length(x))#
cat(paste(latex.special.chr(x), collapse = "\n"), "\n")#
cat(paste(latex.special.chr(x), collapse = "\n"), "\n", file = paste0("Latex/", path))#
  }#
#
latex.special.chr <- function(x) {#
  x <- gsub("$\\hat{\\mkern6mu}$", "^", x,fixed=TRUE)#
  x <- gsub("\\textasteriskcentered ", "*", x,fixed=TRUE)#
  x <- gsub("\\textbackslash ", "\\", x,fixed=TRUE)#
  x <- gsub("\\{", "{", x,fixed=TRUE)#
  x <- gsub("\\}", "}", x,fixed=TRUE)#
  x <- gsub("\\$", "$", x,fixed=TRUE)#
}#
#
 `Figure` <- function(path=NULL, caption="", label="", footnote="")#
  {#
    x <- paste0("\n#
% =====================================================================#
% ▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟#
% ---------------------------------------------------------------------\n",#
ifelse(is.null(path),#
paste0("\\begin{center} \\textbf{", caption, "} \\end{center}"),#
paste0("\\input{", path, "}")),#
#
"\n \\begin{center} INSERT FIGURE \\ref{", label, "} ABOUT HERE \\end{center}#
% ---------------------------------------------------------------------#
% ▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄#
% ===================================================================== \n",#
#
"\n% =====================================================================#
% ▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟G̟U̟R̟E̟ ▄▀▄▀▄▀▀▄▀▄▀▄ F̟I̟#
% ---------------------------------------------------------------------#
% " , caption,#
"% •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
% #
\\begin{figure}#
    \\begin{center}#
    \\caption{",caption, "}#
    \\label{", label, "}#
    \\includegraphics[width=1\\textwidth]{", path, "}#
    \\end{center}#
    \\tabnotes{", footnote, "}#
\\end{figure}#
% ---------------------------------------------------------------------#
% ▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄▀▄▀▀▄▀▄▀▄ E͎N͎D͎ F͎I͎G͎U͎R͎E͎ ▄▀▄#
% =====================================================================\n \n"#
)#
  cat(paste(x, collapse = "\n"), "\n")#
  }#
#
`new.list` <-#
function(len) {#
  out <- list(NA)#
  if (len<1) stop ("Can't make a list of length ",len,".")#
  for (ii in 1:len) out[[ii]] <- NA#
  return(out)#
}#
#
`shift` <- # Seats at c vote#
function(x, w=NULL, c=0.5) {#
    if(is.null(w)) w <- rep(1,length(x))#
  tmp <- mean.w(x, w = w) - c#
    tmp.votes <- x - tmp#
    return(seats(tmp.votes))#
}#
`cube` <- function (x)  x^3/(1-3*x+3*x^2)#
#
seats.display <- function(x) {#
  tmp <- numeric()#
    for (i in 1:dim(x)[2]) {#
      rep <- seats(x[,i]) * length(x[,i])#
      dem <- length(x[,i]) - rep#
      tmp <- rbind(tmp, paste0(rep,"R-", dem, "D"))#
    }#
    return(tmp)#
  }#
#
`two_party` <- #
  function(D, R) {#
    replaceNA(as.numeric(D))/(replaceNA(as.numeric(D))+replaceNA(as.numeric(R)))#
    }#
#
`find.winner` <- #
  function(inp) 0*(inp<0.5)+1*(inp>0.5)#
#
`seats` <- #
  function(inp) mean(inp>.5)#
#
`make.weights` <-  #
  function(x, d = 5) {#
    r(x/sum(x,na.rm=T), d = d)}#
#
`f.num` <- function(x, d=2) format(round(x, d=d), nsmall=d)#
#
`percent` <- #
  function(x, d = 1) {#
    paste0(f.num(x * 100, d= d), "%")#
  }#
#
`replaceNA` <- #
  function (x, value=0) {#
   x[is.na(x)] <- value #
   return(x)}#
#
`mean.w` <- # WEIGHTED MEAN#
function (x, weight=NULL, na.rm = FALSE, ...) {#
    if (is.null(weight)) weight <- rep(1, length(x))#
      w <- as.double(weight)#
    if (na.rm) {#
        i <- !is.na(x)#
        w <- w[i]#
        x <- x[i]#
    }#
    sum((x * w)[w != 0])/sum(w)}#
#
inv <- function(x) {#
  (exp(x) / ( 1 + exp(x)))#
}#
#
sv.curve <- function(s,v) {#
  reg <- lm(log(sv(s)) ~ log(sv(v)))#
  vote <- seq(0.01,0.99, by=.01)#
  seatvotes <-  reg$coefficients[2]*log(vote/(1-vote)) + reg$coefficients[1]#
  return(inv(seatvotes))#
}#
#
sv <- function(x) (x / (1 - x))#
#
`seats.print` <- function(x) paste0(sum(find.winner(x)), "R-", length(x)-sum(find.winner(x)), "D")#
#
`agg.precinct` <- function(data, var, id) {#
        cbind.data.frame(#
          REP = aggregate(as.numeric(data[,paste0(var, "R")]), by=list(id=data[,id]), FUN=sum)[,2], #
          DEM = aggregate(as.numeric(data[,paste0(var, "D")]), by=list(id=data[,id]), FUN=sum)[,2]#
        )#
      }#
#
comp.raw <- function(data) {#
  (two_party(as.numeric(data[,"T16PRESR"]), as.numeric(data[,"T16PRESD"])) +#
  two_party(as.numeric(data[,"T16SENR"]), as.numeric(data[,"T16SEND"])) +#
  two_party(as.numeric(data[,"T16ATGR"]), as.numeric(data[,"T16ATGD"])) +#
  two_party(as.numeric(data[,"T16AUDR"]), as.numeric(data[,"T16AUDD"])) +#
  two_party(as.numeric(data[,"T16TREASR"]), as.numeric(data[,"T16TREASD"]))#
  ) / 5#
  }#
#
composite <- function(data, id) {#
  data <- data[order(data[,id]),]#
  cbind.data.frame(#
        CONG = two_party(aggregate(as.numeric(data[,"T16CONGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16CONGD"]), by=list(id=data[,id]), FUN=sum)[,2]),#
        PRES = two_party(aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2]), #
        USSEN = two_party(aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        ATTGEN = two_party(aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        AUDITOR = two_party(aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        TREASURER = two_party(aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2], aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]) ,#
        COMPOSITE = composite.sum(data, id)#
        )#
  }#
#
composite.sum <- function(data, id) {#
  two_party(#
            (#
              aggregate(as.numeric(data[,"T16PRESR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SENR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDR"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASR"]), by=list(id=data[,id]), FUN=sum)[,2]#
            ),#
            (#
              aggregate(as.numeric(data[,"T16PRESD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16SEND"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16ATGD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16AUDD"]), by=list(id=data[,id]), FUN=sum)[,2] + #
              aggregate(as.numeric(data[,"T16TREASD"]), by=list(id=data[,id]), FUN=sum)[,2]#
            )#
          )#
  }#
#
std <- function(x) {#
  (mean(x[!is.na(x)]) - x) / sd(x[!is.na(x)])#
  }#
#
# ci <- function(x)#
#   {#
#     r <- summary(lm(x~1))#
#       m <- coef(r)[1]#
#       se <- coef(r)[2]#
#       return(qt(0.975, df=length(x)-1) * se)#
#   }#
#
`quick.summary` <- function (x) #
  {#
  `qsum` <- function (set) cbind(mean(set[!is.na(set)]),#
                            sd(set[!is.na(set)]),#
                            var(set[!is.na(set)]),#
                            min(set[!is.na(set)]),#
                            max(set[!is.na(set)]),#
                            sum(1*!is.na(set)),#
                            sum(1*is.na(set)))#
    out <- qsum(x)#
  colnames(out) <- c("Mean","SD","Variance","Min","Max","Valid","Missing")#
    return(out)#
  }#
#
`quick.sum` <- #
  function (x) {#
  `qsum` <- #
    function (set) cbind(#
                      mean(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) - ci(set[!is.na(set)]),#
                      mean(set[!is.na(set)]) + ci(set[!is.na(set)]),#
                      sd(set[!is.na(set)]),#
                      min(set[!is.na(set)]),#
                      quantile(set[!is.na(set)], 0.025),#
                      quantile(set[!is.na(set)], 0.975),                            #
                      max(set[!is.na(set)])#
                      )#
#
    out <- qsum(x)#
  colnames(out) <- c("Mean","CI_lower","CI_upper","SD","Min","2.5%","97.5%","Max")#
    return(out)}#
#
  comb <-combn(c(0:9,LETTERS[1:6]),2)#
  opacity <- c(paste0(comb[1,1:120], comb[2,1:120]), paste0("FF"))#
#
`unc` <- function(x) -1 * (x <= .25) + 1 * (x >= .75)#
#
`delete.unc` <-#
function(vs, uncL, uncU) {#
  #replaces uncontested vote values with "missing".#
 f1 <- function (a,b,c) ifelse (is.na(a),NA,#
                                 ifelse(a<b,NA, ifelse(a>c,NA,a)))#
  return(sapply (vs,f1,uncL,uncU)) }#
#
`replace.unc` <-#
function (vs,l,u,lr,ur, na.rm=T) { # na.rm replaces NAs with 0#
  na <- numeric()#
    if(na.rm==T) #
    {#
      na <- 0#
    } else NA#
  f1 <- function (a,b,c,d,e) if (!is.na(a)) {#
    if (a<b) a <- d else if (a>c) a <- e else a#
  } else NA#
  return(sapply (vs,f1,l,u,lr,ur))}#
`default.unc` <-#
function (vs, uncL = 0.25, uncU = 0.75, uncLR = 0.25, uncUR = 0.75) {#
  vs <- replace.unc(vs, uncL, uncU, uncLR, uncUR)#
  return(vs)}#
#
`mean.unc` <- # uncL and uncU to replace outside bounds, otherwise just NAs#
function(vs, uncL = 0.0001, uncU = 0.99999) {#
  vs <- delete.unc(vs, uncL=uncL, uncU=uncU)#
tmp <- vs[!is.na(vs)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.mean <- truncate(rnorm(length(vs[is.na(vs)]), reg, reg_sig))#
  vs[is.na(vs)] <- new.mean#
  return(vs)}#
`impute.weights` <- # for use when turnout in districts is abnormally high or low (w = # of standard deviations from median)#
function(t1, t2, w = 2) {#
  t <- t1+t2#
  uncL <- (median(t, na.rm=T) -  (w * sd(t, na.rm=T)))#
  uncU <- (median(t, na.rm=T) + (w * sd(t, na.rm=T)))#
  t1 <- delete.unc(t, uncL=uncL, uncU=uncU)#
tmp <- t1[!is.na(t1)]#
reg <- summary(lm(tmp ~ 1))$coef[1] # mean#
reg_sig <- summary(lm(tmp ~ 1))$coef[2] # sigma#
  new.reg <- r(rnorm(length(t1[is.na(t1)]), reg, reg_sig), d=0)#
  t1[is.na(t1)] <- new.reg#
  return(t1)}#
`truncate` <-#
function(r1) {#
  below <- r1<=0#
  above <- r1>=1#
  between <- !(below|above)#
  (below*0.0001)+(r1*between)+0.9999*above#
}#
sim.election <- function(votes= NULL, center=house.2016.votes, incumbency=NULL, yr=2018, sims=1000, sigma=sigma) {#
  if (is.null(sims)) sims <- 1000#
      equal.vote <- mean(votes) - mean(center)#
      sims.year <- new.list(sims)#
    for (k in 1:sims)#
      {#
    sims.year[[k]] <- #
        rnorm(length(votes), votes - equal.vote, #
          sigma)#
      }#
  return(sims.year)#
  }#
#
r <- function(r, d=2) round(r, digits=d)#
# =================================================================#
# -- GERRYMANDER MEASURES -- GERRYMANDER MEASURES -- GERRYMANDER ME#
# =================================================================#
# ••• DELINATION ••••••••••••••••••••••••••••••••••••••••••••••••••#
`declination` <- # Warrington, Gregory S. 2018. “Quantifying Gerrymandering Using the Vote Distribution.” Election Law Journal 17(1): 39–57. www.liebertpub.com (Accessed February 22, 2019).#
  function(votes) {#
    abo = votes[votes > 0.5]   # districts won by party A#
    bel = votes[votes <= 0.5]  # districts won by party B#
  # declination is undefined if one party wins all seats.#
    if (length(bel) == 0 | length(abo) == 0) {#
      return(NaN)#
    }#
  # angle for party B#
    theta = atan((1-(2*mean(bel))) / (length(bel) / length(votes)))#
  # angle for party A#
    gamma = atan((2*mean(abo)-1) / (length(abo) / length(votes)))#
  # normalize from radians to values betwen -1 and 1#
  # A little extra precision just in case :)#
    return(-1 * (2.0*(gamma-theta)/3.1415926535))}#
#
`declin2` <- # Simplified, not transformed from: Katz, Jonathan N. et al. 2018. Theoretical Foundations and Empirical Evaluations of Partisan Fairness in District-Based Democracies *. https://gking.harvard.edu/files/psym_2.pdf (Accessed March 16, 2019).#
  function(vs) {#
    abo = vs[vs > 0.5]   # districts won by party A#
    bel = vs[vs <= 0.5]  # districts won by party B#
    (-1 * (mean(abo) - 0.5) / (sum(find.winner(abo))/length(vs))) - ((0.5 - mean(bel)) / (1 - sum(find.winner(abo))/length(vs)))#
  }#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••• MEAN/MEDIAN••••••••••••••••••••••••••••••••••••••••••••••••••#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
`meanmedian` <- # Best, Robin E., Shawn J. Donahue, Jonathan Krasno, Daniel B. Magleby, et al. 2018. “Considering the Prospects for Establishing a Packing Gerrymandering Standard.” Election Law Journal: Rules, Politics, and Policy 17(1): 1–20. http://www.liebertpub.com/doi/10.1089/elj.2016.0392 (Accessed July 24, 2018).#
  function(votes) median(votes) - mean(votes)#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# •••• EFFICIENCY GAP••••••••••••••••••••••••••••••••••••••••••••••#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
`eff_gap` <- # Stephanopoulos, Nicholas, and Eric Mcghee. 2014. “Partisan Gerrymandering and the Efficiency Gap.” University of Chicago Law School 82. http://ssrn.com/abstract=2457468.. (Accessed September 10, 2018).#
  function(D, R) {#
    total <- sum(D)+sum(R)#
    dem_wasted <- sum(ifelse(D>R, D[D>R] - R[D>R], D[D<R]))#
    rep_wasted <- sum(ifelse(D<R, R[D<R] - D[D<R], R[D>R]))#
      return((dem_wasted - rep_wasted) / sum(total))}#
#
eg_TP <- function(votes) #
  {#
    y <- cbind.data.frame(votes, 1-votes)#
      return(-1 * eff_gap(y[1], y[2]))#
  }#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# •••• GERRY DISPLAY ••••••••••••••••••••••••••••••••••••••••••••••#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
  gerry <- function(x, toggle=TRUE)#
    {#
    Seats = paste0(" [", seats.print(x), "]")#
    SeatPER = percent(seats(x))#
    Votes = percent(mean(default.unc(x)))#
    Bias = r(seatsvotes(x)$bias)#
    EfficiencyGap = r(eg_TP(x))#
    MeanMedian = r(meanmedian(x))#
    Declination = r(declination(x))#
    a <- rbind.data.frame(#
      Seats, SeatPER, Votes, Bias, EfficiencyGap, MeanMedian, Declination)#
    rownames(a) <- c("Seats","Seat %","Votes","Bias","Efficiency Gap","Mean/Median","Declination")#
    colnames(a) <- "Summary"#
#
    if (toggle!=TRUE) { # TO RETURN UNFORMATTED MEASUREMENTS#
    SeatsT = NA#
    SeatPERT = r(seats(x))#
    VotesT = r(mean(default.unc(x)))#
    BiasT = r(seatsvotes(x)$bias)#
    EfficiencyGapT = r(eg_TP(x))#
    MeanMedianT = r(meanmedian(x))#
    DeclinationT = r(declination(x))#
      return(rbind.data.frame(#
      SeatsT, SeatPERT, VotesT, BiasT, EfficiencyGapT, MeanMedianT, DeclinationT))}#
    return(a)#
    }#
#
p_value <- function(x){#
  l <- quantile(x[!is.na(x)], 0.025)#
  u <- quantile(x[!is.na(x)], 0.975)#
  t <- mean(x)/sd(x)#
    return(2*pt(-abs(t),df=length(x)-1))#
  }#
#
strs <- function(s) {#
  strs <- #
  ifelse(s <= 0.001, "***", #
    ifelse(0.001 < s & s <= 0.01, "**", #
      ifelse(0.01 < s & s <= 0.05, "*", "")))#
  return(paste0("$^{", strs, "}$"))#
}#
#
nintyfive <- function(x, percent=FALSE) {#
  if (percent==TRUE){ return(paste0("(", percent(r(quantile(x[!is.na(x)], 0.025),d=1)), ", ", percent(r(quantile(x[!is.na(x)], 0.975),d=1)), ")")) }#
  return(paste0("{\\small\\textit{[", r(quantile(x[!is.na(x)], 0.025)), ", ", r(quantile(x[!is.na(x)], 0.975)), "]}}"))#
  }#
#
gtab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
  x.bias <- get(paste0(p, ".bias"))#
  x.eg <- get(paste0(p, ".eg"))#
  x.mm <- get(paste0(p, ".meanmedian"))#
  x.declin <- get(paste0(p, ".declination"))#
    return(c(#
      paste0(r(mean(x.bias)), strs(p_value(x.bias))), #
        nintyfive(x.bias), #
      paste0(r(mean(x.eg)), strs(p_value(x.eg))),#
        nintyfive(x.eg), #
      paste0(r(mean(x.mm)), strs(p_value(x.mm))),#
        nintyfive(x.mm),#
      paste0(r(mean(x.declin)), strs(p_value(x.declin))),#
        nintyfive(x.declin)#
      ))#
    }#
#
ptab <- function(x) {#
  p <- (paste0(x, ".sims.5050"))#
  p.tmp <- get(p)#
  s.tmp <- maps.sims.seats.5050[[x]]#
  v.tmp <- maps.sims.votes.5050[[x]]#
    return(c(#
      paste0(r(sum(s.tmp*18)/(1000*18)*18, d=1), "R-", r(18-sum(s.tmp*18)/(1000*18)*18, d=1), "D"),#
      paste0("{\\small\\textit{[", r(quantile(s.tmp, 0.025),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.025),d=1)*18, "D, ", r(quantile(s.tmp, 0.975),d=1)*18, "R-", 18-r(quantile(s.tmp, 0.975),d=1)*18, "D]}}"),#
      paste0(median(s.tmp)*18, "R-", 18-median(s.tmp)*18, "D"),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) > 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) < 0.5) / 1000),#
      percent(sum(1 * do.call(rbind, lapply(p.tmp, function(x) mean(find.winner(x)))) == 0.5) / 1000)#
      ))#
} #
#
 # •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••   #
`circle.new` <- #
  function(xorig, yorig, radius, add, ...){#
    x <- seq(-radius, radius, length.out = 1000)#
  # Euclidian distance to the origin#
  y <- sapply(x, function(z) sqrt(radius^2 - z^2))#
  if(add == TRUE){#
    line(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
          type = "l", add=T, ...)#
   } else {#
   plot(xorig + c(x, rev(x)), c(yorig + y, yorig + rev(-y)),#
        type = "l",  #
    xlab="", #
    ylab="", #
    xaxt="n", #
    yaxt="n", #
    bty="n", ...)#
   }#
}#
#
`circle` <- #
function (x, y, radius, nv = 100, border = NULL, col = NA, lty = 1, #
    lwd = 1) {#
    xylim <- par("usr")#
    plotdim <- par("pin")#
    ymult <- getYmult()#
    angle.inc <- 2 * pi/nv#
    angles <- seq(0, 2 * pi - angle.inc, by = angle.inc)#
    if (length(col) < length(radius)) #
        col <- rep(col, length.out = length(radius))#
    for (circle in 1:length(radius)) {#
        xv <- cos(angles) * radius[circle] + x#
        yv <- sin(angles) * radius[circle] * ymult + y#
        polygon(xv, yv, border = border, col = col[circle], lty = lty, #
            lwd = lwd)#
    }#
    invisible(list(x = xv, y = yv))#
}#
# •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# COMPACTNESS MEASURES#
#
POLSBYPOPPER <- function (x) r(mean((4 * 3.1415926535 * x$area) / (x$perimeter^2)), d = 3)#
REOCK <- function (x) r(mean(x$area/(x$smallestcircle^2 * 3.1415926535)), d = 3)#
poly.math <- function (x) {#
    compactness <- NULL#
  if (class(x)=="character") x <- get(x)#
      shapeFile <- x#
      mapObject <- fortify(shapeFile)#
      mapObject$id <- as.character(as.numeric(mapObject$id) + 1)#
      mapObject <- data.frame(mapObject, shapeFile@data[mapObject$id, ])#
      mapObject$piece <- as.character(mapObject$piece)#
      uniqueCDs <- sort(unique(as.numeric(mapObject$id)))#
      for(id in uniqueCDs)#
        {#
          cdShape <- mapObject[mapObject$id == id, ]#
          cdPoly <- SpatialPolygons(list(Polygons(lapply(split(cdShape[, c("long", "lat")], cdShape$piece), Polygon), ID = "b")))#
          owinObject <- try(as(cdPoly, "owin"))#
          compactness[[id]] <-  data.frame(area=area.owin(owinObject), perimeter=perimeter(owinObject), smallestcircle=boundingradius(owinObject))#
        }#
        x <- do.call("rbind", compactness)#
        cat(#
          "\n REOCK:        ", REOCK(x), "\n",#
          "POLSBY-POPPER:", POLSBYPOPPER(x), "\n \n"#
          )#
        return(x)#
  }
# ▀▀█▀▀ █▀▀█ █▀▀▄ █░░ █▀▀ █▀▀#
# ░░█░░ █▄▄█ █▀▀▄ █░░ █▀▀ ▀▀█#
# ░░▀░░ ▀░░▀ ▀▀▀░ ▀▀▀ ▀▀▀ ▀▀▀#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
cat(#
    "\n#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
        CREATING TABLES . . . . . . . . . . . . . . . . . . . . .#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n")#
sink("/dev/null")#
# races <- c("cong2016", "pres16", "ussen16", "atg16", "aud16", "trea16", "comp2016")#
statewide.contests.2016 <- c("Congress", "Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite")#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# County Spliits#
county_splits <- c(41, 19, 19, 17)#
# COMPACTNESS#
geom_names <- paste0(plans, ".geom")#
#
reock.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        reock.plans <- rbind(reock.plans, REOCK(get(geom_names[i])))#
        }#
polsby.plans <- numeric()#
    for (i in 1:length(geom_names)) {#
        polsby.plans <- rbind(polsby.plans, POLSBYPOPPER(get(geom_names[i])))#
        }#
#
tab_plan_sum <- cbind.data.frame(#
    Plan = plan_names,#
    CountySplits = county_splits, #
    Reock = reock.plans, #
    PolsbyPopper = polsby.plans#
    # Pres2016 = c(seats.print(enacted.pres), seats.print(court.pres), seats.print(joint.pres), seats.print(govwolf.pres)),#
    # Comp2016 = c(seats.print(enacted.comp), seats.print(court.comp), seats.print(joint.comp), seats.print(govwolf.comp))#
    )#
tab_plan_sum#
plan_summary.caption = "County Splits and Compactness Scores of the Plans"#
plan_summary.label = "tab:summaries"#
plan_summary.footnote = "County splits include all the pieces in which a county is split, not just the total number of counties that have been split. (The latter number is the one most often reported in both court documents and in the media, but we regard the measure we report as both more precise and more informative.)"#
tab_plan_summary <- #
    stargazer(tab_plan_sum,#
        style = "apsr", #
        header = FALSE,#
        summary = FALSE,#
        model.numbers = FALSE,#
        initial.zero = TRUE,#
        digits = 3,#
        column.sep.width = "0pt",#
        rownames = FALSE,#
        multicolumn = TRUE,#
        label = plan_summary.label,#
        title = plan_summary.caption,#
        covariate.labels = ,#
        notes = plan_summary.footnote #
        )#
tab_plan_summary <- tab_plan_summary[c(-6, -7, -13, -14, -15, -16)]#
# tab_plan_summary <- append(tab_plan_summary, #
#   "  &   &  &  & \\textbf{Projected} & \\textbf{Projected}  \\\\", #
#   after = 5)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  &  \\textbf{County}  &  \\textbf{Polsby}   &   &  \\textbf{2016 using}  &  \\textbf{five state-wide} \\\\",#
#   after = 6)#
# tab_plan_summary <- append(tab_plan_summary,#
#   "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  &  \\textbf{Presidential Results}  &  \\textbf{elections in 2016} \\\\",#
#   after = 7)#
#
tab_plan_summary <- append(tab_plan_summary,#
    "  &  \\textbf{County}  &  \\textbf{Polsby}   &   \\\\",#
    after = 5)#
tab_plan_summary <- append(tab_plan_summary,#
    "  \\textbf{Plan}  &  \\textbf{Splits} &  \\textbf{Popper}  &  \\textbf{Reock}  \\\\",#
    after = 6)#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# =================================================================#
# Comparing Pennsylvania Congressional Results with State-wide Elections (2016) %#
# =================================================================#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# SUMMARY OF ELECTIONS#
percent(sum(house.rep.2012)/sum(house.2012.turnout))#
percent(sum(house.rep.2014)/sum(house.2014.turnout))#
percent(sum(house.rep.2016)/sum(house.2016.turnout))#
#
congsum.caption = "U.S. House Election Summaries \\\\ \\hspace{2cm}(PA 2012-2016 Enacted Map)"#
congsum.label = "tab:congsum"#
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."#
congsum.tmp <- cbind.data.frame(#
    # PA2004 = gerry(default.unc(house.2004.votes)),#
    # PA2006 = gerry(default.unc(house.2006.votes)),#
    # PA2008 = gerry(default.unc(house.2008.votes)),#
    # PA2010 = gerry(default.unc(house.2010.votes)),#
    PA2012 = gerry(default.unc(house.2012.votes)), #
    PA2014 = gerry(default.unc(house.2014.votes)), #
    PA2016 = gerry(default.unc(house.2016.votes)),#
    # PA2018 = gerry(default.unc(house.2018.votes)),#
    PA2012_2016_AVE = rowMeans(cbind(   #
    gerry(default.unc(house.2012.votes), toggle=F),#
    gerry(default.unc(house.2014.votes), toggle=F),#
    gerry(default.unc(house.2016.votes), toggle=F)))#
  )#
congsum.tmp[2:3,4]  <- percent(congsum.tmp[2:3,4])#
congsum.tmp[4:7,4] <- r(as.numeric(congsum.tmp[4:7,4]))#
colnames(congsum.tmp) <- c(seq(2012,2016,2),"AVE")#
congsum.tmp.tex <- stargazer(congsum.tmp,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    label = congsum.label,#
    title = congsum.caption,#
    notes = congsum.footnote)#
congsum.tmp.tex <- congsum.tmp.tex[c(-6, -16, -17, -18, -19)]#
congsum.tmp.tex[8] <- "Seats &  [13R-5D] &  [13R-5D] &  [13R-5D] & [13R-5D] \\\\ "#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
dist_summary <- #
    rbind(#
      cbind.data.frame(#
        District = seq(1,18,1),#
        sapply(composite(pa.redist.dta, "enacted"), percent)),#
        c(#
          "Statewide",#
          sapply(colMeans(composite(pa.redist.dta, "enacted")), percent)#
        )#
    )#
#
district_summary.caption = "2016 District-level Summaries for 2011 Enacted Plan"#
district_summary.label = "tab:districtvotes"#
district_summary.footnote = "Uncontested races and those with only negligible competition will be imputed with 0.25 and 0.75 for the respective winners. All votes are calculated from the Republican perspective of the two-party vote. Composite does NOT include the Congressional elections. The statewide average is the unweighted mean of districts."#
#
district_summary <- stargazer(dist_summary,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    rownames = FALSE,#
    label = district_summary.label,#
    title = district_summary.caption,#
    notes = "REPLACE WITH NOTES" )#
district_summary <- district_summary[c(-6, -28, -29, -30, -31)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# =================================================================#
# Measures of Gerrymandering for the Eight Considered Plans#
# =================================================================#
gerry.table.gen <- #
    cbind.data.frame(#
        gtab("enacted"),#
        gtab("joint"),#
        gtab("govwolf"),#
        gtab("court")#
        )#
#
    colnames(gerry.table.gen) <- c(plan_names)#
    rownames(gerry.table.gen) <- c(#
        "Partisan Bias", #
        "CI1",#
        "Efficiency Gap",#
        "CI2",#
        "Mean/Median",#
        "CI3",#
        "Declination",#
        "CI4")#
#
gerry.caption = "Measures of Gerrymandering for the Four Considered Plans"#
gerry.label = "tab:gerry"#
gerry.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001. Measures are averages of 1,000 simulations for each map using the 2016 composite. Brackets numbers are the 95\\% range."#
tab_gerry.tex <- stargazer(gerry.table.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= gerry.caption, #
    label= gerry.label,#
    notes = gerry.footnote)#
    tab_gerry.tex <- gsub("CI[1-9]", "", tab_gerry.tex)#
tab_gerry <- tab_gerry.tex[c(-6, -17, -18, -19, -20)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
tab_prop.gen <- #
    cbind.data.frame(#
        ptab("enacted"),#
        ptab("joint"),#
        ptab("govwolf"),#
        ptab("court")#
        )#
    colnames(tab_prop.gen) <- c(plan_names)#
    rownames(tab_prop.gen) <- c(#
        "Mean Seat Share",#
        "CI1", #
        "Median Seat Share",#
        "Probability Republican Majority",#
        "Probability Democratic Majority",#
        "Probability Tied Delegation")#
#
prop.caption = "Probabilistic Projections of Partisan Outcomes for Four Plans at 50\\% Vote-Share"#
prop.label = "tab:prob"#
prop.footnote = "Using a Composite of Five Statewide Elections (adjusted to a 50\\% Vote Share) but not correcting for incumbency. We report the mean seat-share from 1,000 simulations, along with a 95\\% range of the simulated outcomes."#
#
tab_prop.tex <- stargazer(tab_prop.gen,#
    style = "apsr", #
    summary=F,#
    column.sep.width = "-5pt", #
    float = T, #
    header = FALSE,#
    multicolumn = TRUE, #
    title= prop.caption, #
    label= prop.label,#
    notes = prop.footnote)#
tab_prop.tex <- gsub("CI[1-9]", "", tab_prop.tex)#
tab_prop <- tab_prop.tex[c(-6, -15, -16, -17, -18)]#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# =================================================================#
# Information about Twelve States with Constitutional Provisions similar to Pennsylvania#
# =================================================================#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# latex <- function(x) cat(paste(x, collapse="\n"), "\n")#
# states_compare.caption = "Information about Twelve States with Constitutional Provisions similar to Pennsylvania"#
# states_compare.label = "tab:states_compare"#
# states_compare.footnote = "Seats and votes are based on the 2016 five-election projection (to deal with the existence of non-contested congressional districts). Percentages are of the of district level results. This difference is why the percentages reported in columns 6 and 8 are not identical. Data from DailyKos, All about Redistricting, and Ballotpedia web sites.States with fewer than 9 districts do not have efficiency gap or median values reported because of the potential unreliability of those calculations given the small number of districts involved."#
#
# tab_states_compare <- #
#   paste("#
#       \\begin{table}#
#       \\caption{", states_compare.caption, "}#
#       \\label{", states_compare.label, "}#
#       \\begin{tabular}{#
#       >{\\centering\\arraybackslash}#
#       M{0.1\\linewidth}|#
#       M{.025\\linewidth}|#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.075\\linewidth}#
#       M{.3\\linewidth}}#
#       \\textbf{State} & \\textbf{\\# CDs} & \\textbf{Unified Control (2011)} & \\textbf{Unified Control (2016)} & \\textbf{Seats (2016)} & \\textbf{Votes (2016)} & \\textbf{Mean District Vote Share (Dem)} & \\textbf{Median District Vote Share (Dem)} & \\textbf{Efficiency Gap} & \\textbf{Who does districting} \\\\#
#        \\hline#
#       Arizona & 9 & $\\surd$ (R) & $\\surd$ (R) & 44.4\\% & 48.1\\% & 50.4\\% & 49.4\\% & 0.08 & Independent commission \\\\#
#       Arkansas & 4 & $\\surd$ (D) & $\\surd$ (R) & 0\\% & 35.7\\% & 35.5\\% &  &  & state legislature \\\\#
#       Delaware & 1 & $\\surd$ (D) & $\\surd$ (D) & 100\\% & 55.9\\% & 56\\% &  &  & NA \\\\#
#       Illinois & 18 & $\\surd$ (D) &  & 61.1\\% & 59.0\\% & 59.6\\% & 59.8\\% & 0.08 & state legislature \\\\#
#       Indiana & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 39.9\\% & 40.1\\% & 35.9\\% & 0.09 & state legislature \\\\#
#       Kentucky & 6 &  & $\\surd$ (R) & 16.7\\% & 34.3\\% & 33.8\\% &  &  & state legislature \\\\#
#       Oklahoma & 5 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 30.7\\% & 30.7\\% &  &  & state legislature \\\\#
#       Oregon & 5 &  & $\\surd$ (D) & 80\\% & 56.2\\% & 56\\% &  &  & state legislature \\\\#
#       South Dakota & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 34.0\\% & 34\\% &  &  & NA \\\\#
#       Tennessee & 9 & $\\surd$ (R) & $\\surd$ (R) & 22.2\\% & 36.4\\% & 37.4\\% & 31.3\\% & 0.03 & state legislature \\\\#
#       Washington & 10 & $\\surd$ (D) & $\\surd$ (D) & 70\\% & 58.8\\% & 56\\% & 52.3\\% & -0.05 & 5-member independent commission \\\\#
#       Wyoming & 1 & $\\surd$ (R) & $\\surd$ (R) & 0\\% & 24.3\\% & 24.3\\% &  &  & NA ")#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
# TWO-PARTY VOTE SHARES, ACTUAL (CONGRESSIONAL)#
# tab_summary_stats <- stargazer(#
#   congress.PA.2008.2018,#
#   title="Republican Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   style = "apsr", #
#   header = FALSE,#
#   column.sep.width = "-5pt",#
#   multicolumn = TRUE,#
#   label = "tab:twoparty_actuals" )#
# tab_summary_stats <- tab_summary_stats[c(-6, -11)]#
#
#  Table(tab_summary_stats,#
#   caption="Two-Party Vote/Seat Share, (PA Congressional Elections 2008-2018)",#
#   label="ab:twoparty_actuals",#
#   footnote="Votes are the two-party unweighted average of PA Congressional Districts by year (uncontested elections imputed).")#
#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
# AVERAGE VOTE BY DISTRICT#
means_races <- numeric()#
  base.form <- formula(. ~ 1)#
  cong.form <- formula(default.unc(enacted.elections$CONG) ~ + pahouse.2016$INC)#
for (k in colnames(enacted.elections))#
{#
  r.tmp <- composite(pa.redist.dta, "enacted")[,k]#
    reg_races <- summary(lm(update(base.form, as.formula(default.unc(r.tmp) ~ .))))#
    means_races[k] <- coef(reg_races)[1]#
}#
#
  cat(#
    "2016 Average Vote Share by State-level Contest \n"#
    )#
    print(cbind.data.frame(Race = statewide.contests.2016, Mean = percent(means_races)))#
    cat(paste("", collapse = "\n"), "\n")#
# # Regression of Race on Congressional Results#
    reg_cong_pres <- lm(update(cong.form, as.formula(. ~ . + default.unc(enacted.elections$PRES))))#
    reg_cong_ussen <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$USSEN))))#
    reg_cong_atg <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$ATTGEN))))#
    reg_cong_aud <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$AUDITOR))))#
    reg_cong_trea <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$TREASURER))))#
    reg_cong_comp <- lm(update(cong.form, as.formula(. ~ . +  default.unc(enacted.elections$COMPOSITE))))#
    reg_cong_all <- lm(update(cong.form, #
      as.formula(. ~  . +  #
        default.unc(enacted.elections$PRES) + #
        default.unc(enacted.elections$USSEN) +#
        default.unc(enacted.elections$ATTGEN) +#
        default.unc(enacted.elections$AUDITOR) +#
        default.unc(enacted.elections$TREASURER))#
    ))#
#
congress_predict.caption = "Comparing Pennsylvania Congressional Results with State-wide Elections (2016)"#
congress_predict.label = "tab:tab_congress_predict"#
congress_predict.footnote = "$^{*}$p $<$ 0.05; $^{**}$p $<$ .01; $^{***}$p $<$ 0.001 \\\\ Uncontested (or non-competitive) elections replaced with 0.25 \\& 0.75 vote shares. Regressions are unweighted, ie, all districts are assumed to have identical turnout. This is the usual way political scientist measure aggregate congressional vote \\cite{GelmanKing1994_unifiedAJPS}. \\\\"#
#
tab_congress_predict <- stargazer(#
        reg_cong_pres, #
         reg_cong_ussen, #
         reg_cong_atg, #
         reg_cong_aud, #
         reg_cong_trea, #
         reg_cong_comp,#
         reg_cong_all,#
star.cutoffs = c(0.05,0.01, 0.001),#
style = "apsr", #
header = FALSE,#
model.numbers = FALSE,#
initial.zero = TRUE,#
digits = 2,#
column.sep.width = "0pt",#
multicolumn = TRUE,#
omit.stat = c("ll", "F", "ser"),#
dep.var.labels = "Actual Congressional Results (2016)",#
covariate.labels = c("Incumbency Advantage","Presidential", "US Senate", "PA Attorneys General", "PA Auditor", "PA Treasurer", "Composite"),#
label = congress_predict.label,#
title = congress_predict.caption,#
notes = congress_predict.footnote#
    )#
tab_congress_predict <- tab_congress_predict[c(-6, -26, -28, -29, -30, -31, -32)]#
# ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
#
sink()#
Table(tab_plan_summary,#
    path = "Tables/_tab_summaries.tex",#
    caption = plan_summary.caption,#
    label = plan_summary.label,#
    footnote = plan_summary.footnote)#
#
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)#
#
Table(tab_gerry, #
    path = "Tables/_tab_gerry.tex", #
    caption = gerry.caption,#
    label = gerry.label, #
    footnote = gerry.footnote)#
#
Table(tab_prop,#
    path = "Tables/_tab_prob.tex",#
    caption = prop.caption,#
    label = prop.label, #
    footnote = prop.footnote,#
    landscape = TRUE)#
#
Table(district_summary,#
    path = "Tables/_a_tab_districtvotes.tex",#
    caption = district_summary.caption,#
    label = district_summary.label,#
    footnote = district_summary.footnote#
    )#
#
Table(tab_congress_predict,#
    path = "Tables/a_tab_congress_predict.tex",#
    caption = congress_predict.caption,#
    label = congress_predict.label,#
    footnote = congress_predict.footnote)#
cat("#
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••#
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#
    ")
congsum.caption = "U.S. House Election Summaries \\\\ {\\Large\\hspace{4cm}(PA 2012-2016 Enacted Map)}"
congsum.label = "tab:congsum"
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."
congsum.tmp <- cbind.data.frame(#
    # PA2004 = gerry(default.unc(house.2004.votes)),#
    # PA2006 = gerry(default.unc(house.2006.votes)),#
    # PA2008 = gerry(default.unc(house.2008.votes)),#
    # PA2010 = gerry(default.unc(house.2010.votes)),#
    PA2012 = gerry(default.unc(house.2012.votes)), #
    PA2014 = gerry(default.unc(house.2014.votes)), #
    PA2016 = gerry(default.unc(house.2016.votes)),#
    # PA2018 = gerry(default.unc(house.2018.votes)),#
    PA2012_2016_AVE = rowMeans(cbind(   #
    gerry(default.unc(house.2012.votes), toggle=F),#
    gerry(default.unc(house.2014.votes), toggle=F),#
    gerry(default.unc(house.2016.votes), toggle=F)))#
  )
congsum.tmp[2:3,4]  <- percent(congsum.tmp[2:3,4])
congsum.tmp[4:7,4] <- r(as.numeric(congsum.tmp[4:7,4]))
colnames(congsum.tmp) <- c(seq(2012,2016,2),"AVE")
congsum.tmp.tex <- stargazer(congsum.tmp,#
    style = "apsr", #
    summary = FALSE,#
    header = FALSE,#
    column.sep.width = "-5pt",#
    multicolumn = TRUE,#
    label = congsum.label,#
    title = congsum.caption,#
    notes = congsum.footnote)
congsum.tmp.tex <- congsum.tmp.tex[c(-6, -16, -17, -18, -19)]
congsum.tmp.tex[8] <- "Seats &  [13R-5D] &  [13R-5D] &  [13R-5D] & [13R-5D] \\\\ "
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)
congsum.caption = "U.S. House Election Summaries \\\\ {\\large\\hspace{4cm}(PA 2012-2016 Enacted Map)}"
congsum.label = "tab:congsum"
congsum.footnote = "Calculations based on actual congressional elections in Pennsylvania under the map found unconstitutional in 2018. Uncontested races are imputed with 0.25 and 0.75 for the respective winners. Un-adjusted Republican two-party vote totals are 49.2\\% for 2012, 55.5\\% for 2014, and 54.1\\% for 2016. All votes are calculated from the Republican perspective of the two-party vote. We've adjusted all gerrymandering measures such that negative numbers indicate bias in favor of the Democrats."
Table(congsum.tmp.tex,#
    path = "Tables/_tab_congsum.tex",#
    caption = congsum.caption,#
    label = congsum.label,#
    footnote = congsum.footnote)
